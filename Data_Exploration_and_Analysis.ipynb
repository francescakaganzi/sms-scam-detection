{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxDbDfdkdu6UesYoq/qQg0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsbq163SixRU"
      },
      "outputs": [],
      "source": [
        "# SMS Scam Detection - Data Exploration and Analysis\n",
        "# =====================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "import emoji\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Set plotting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Set up project directory structure\n",
        "project_dir = '/content/drive/MyDrive/sms-scam-detection'\n",
        "os.chdir(project_dir)\n",
        "\n",
        "directories = [\n",
        "    'data/raw',\n",
        "    'data/processed',\n",
        "    'models/baseline',\n",
        "    'models/deep_learning',\n",
        "    'models/llm',\n",
        "    'results/metrics',\n",
        "    'results/visualizations'\n",
        "]\n",
        "\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f\"Project directory: {project_dir}\")\n",
        "print(\"Directory structure created successfully!\")\n",
        "\n",
        "# Load and examine the dataset\n",
        "dataset_path = 'data/raw/spam-fraud-sms-dataset.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "print(f\"Dataset loaded with {len(df)} rows and {len(df.columns)} columns\")\n",
        "print(f\"Column names: {df.columns.tolist()}\")\n",
        "\n",
        "# Display basic dataset information\n",
        "print(\"\\nDataset Info:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Convert labels to binary format if needed\n",
        "if df['label'].dtype == object:\n",
        "    unique_labels = df['label'].unique()\n",
        "    print(f\"\\nUnique label values: {unique_labels}\")\n",
        "\n",
        "    label_map = {'ham': 0, 'spam': 1}\n",
        "    df['label'] = df['label'].map(label_map)\n",
        "    print(\"Labels converted to binary (0 for legitimate/ham, 1 for spam/scam)\")\n",
        "\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# Analyze class distribution\n",
        "print(\"\\nClass Distribution:\")\n",
        "label_counts = df['label'].value_counts(normalize=True) * 100\n",
        "print(f\"Class 0 (Legitimate): {label_counts.get(0, 0):.2f}%\")\n",
        "print(f\"Class 1 (Spam/Scam): {label_counts.get(1, 0):.2f}%\")\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.title('Distribution of Labels')\n",
        "plt.xlabel('Label (0: Legitimate, 1: Spam/Scam)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and normalize text data while preserving important patterns.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to clean\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    # Preserve important patterns with tokens\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+|[a-zA-Z0-9.-]+\\.ug\\b|[a-zA-Z0-9.-]+\\.ly\\b', 'URLTOKEN', text)\n",
        "    text = re.sub(r'\\S+@\\S+', 'EMAILTOKEN', text)\n",
        "    text = re.sub(r'\\b256(?:\\s*\\d){9}\\b', 'PHONETOKEN', text)\n",
        "    text = re.sub(r'\\b0(?:\\s*\\d){9}\\b', 'PHONETOKEN', text)\n",
        "    text = re.sub(r'\\b0(?:\\s*\\d){10,15}\\b', 'PHONETOKEN', text)\n",
        "\n",
        "    # Normalize money amounts\n",
        "    text = re.sub(r'(?:ugx|shs|shilling(?:s)?)?\\s*(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?)', r'MONEYTOKEN \\1', text)\n",
        "    text = re.sub(r'(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?)?\\s*(?:ugx|shs|shilling(?:s)?)', r'MONEYTOKEN \\1', text)\n",
        "    text = re.sub(r'(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?)?\\s*/[-=]', r'MONEYTOKEN \\1', text)\n",
        "\n",
        "    # Remove emojis and non-ASCII characters\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
        "    \"\"\"\n",
        "    Preprocess text for analysis.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to preprocess\n",
        "        remove_stopwords (bool): Whether to remove stopwords\n",
        "        lemmatize (bool): Whether to lemmatize words\n",
        "\n",
        "    Returns:\n",
        "        list: List of processed tokens\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    special_tokens = ['URLTOKEN', 'EMAILTOKEN', 'PHONETOKEN', 'MONEYTOKEN']\n",
        "    tokens = [token for token in tokens if token in special_tokens or token not in string.punctuation]\n",
        "\n",
        "    uganda_terms = ['ugx', 'shs', 'shilling', 'shillings']\n",
        "\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        stop_words = stop_words - set(uganda_terms)\n",
        "        tokens = [token for token in tokens if token.lower() not in stop_words or token in special_tokens]\n",
        "\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) if token not in special_tokens else token\n",
        "                  for token in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply text cleaning\n",
        "df['cleaned_text'] = df['message'].apply(clean_text)\n",
        "\n",
        "print(\"\\nExamples of original and cleaned text:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nOriginal: {df['message'].iloc[i]}\")\n",
        "    print(f\"Cleaned: {df['cleaned_text'].iloc[i]}\")\n",
        "\n",
        "# Calculate text statistics\n",
        "df['text_length'] = df['message'].str.len()\n",
        "df['word_count'] = df['message'].str.split().str.len()\n",
        "df['avg_word_length'] = df['message'].apply(\n",
        "    lambda x: sum(len(word) for word in x.split()) / len(x.split()) if len(x.split()) > 0 else 0\n",
        ")\n",
        "df['uppercase_ratio'] = df['message'].apply(\n",
        "    lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0\n",
        ")\n",
        "df['digit_ratio'] = df['message'].apply(\n",
        "    lambda x: sum(1 for c in x if c.isdigit()) / len(x) if len(x) > 0 else 0\n",
        ")\n",
        "df['special_char_ratio'] = df['message'].apply(\n",
        "    lambda x: sum(1 for c in x if not c.isalnum() and not c.isspace()) / len(x) if len(x) > 0 else 0\n",
        ")\n",
        "\n",
        "# Display text statistics by class\n",
        "text_stats = df.groupby('label')[\n",
        "    ['text_length', 'word_count', 'avg_word_length', 'uppercase_ratio', 'digit_ratio', 'special_char_ratio']\n",
        "].mean()\n",
        "\n",
        "text_stats.index = ['Legitimate', 'Spam/Scam']\n",
        "print(\"\\nAverage text statistics by class:\")\n",
        "print(text_stats)\n",
        "\n",
        "# Create descriptive labels for visualization\n",
        "df['label_desc'] = df['label'].map({0: 'Legitimate', 1: 'Spam/Scam'})\n",
        "\n",
        "# Visualize text length distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data=df, x='text_length', hue='label_desc', bins=50, kde=True, element='step')\n",
        "plt.title('Distribution of Message Lengths')\n",
        "plt.xlabel('Message Length (characters)')\n",
        "plt.ylabel('Count')\n",
        "plt.xlim(0, df['text_length'].quantile(0.99))\n",
        "plt.show()\n",
        "\n",
        "# Create boxplots for text statistics\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "stats = ['text_length', 'word_count', 'avg_word_length', 'uppercase_ratio', 'digit_ratio', 'special_char_ratio']\n",
        "titles = ['Message Length', 'Word Count', 'Average Word Length', 'Uppercase Ratio', 'Digit Ratio', 'Special Character Ratio']\n",
        "\n",
        "for i, (stat, title) in enumerate(zip(stats, titles)):\n",
        "    row, col = i // 3, i % 3\n",
        "    sns.boxplot(x='label', y=stat, data=df, ax=axes[row, col])\n",
        "    axes[row, col].set_title(title)\n",
        "    axes[row, col].set_xlabel('Class (0: Legitimate, 1: Spam/Scam)')\n",
        "    axes[row, col].set_xticks([0, 1])\n",
        "    axes[row, col].set_xticklabels(['Legitimate', 'Spam/Scam'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze vocabulary patterns\n",
        "legitimate_texts = df[df['label'] == 0]['cleaned_text'].tolist()\n",
        "spam_texts = df[df['label'] == 1]['cleaned_text'].tolist()\n",
        "\n",
        "legitimate_tokens = []\n",
        "for text in legitimate_texts:\n",
        "    legitimate_tokens.extend(preprocess_text(text))\n",
        "\n",
        "spam_tokens = []\n",
        "for text in spam_texts:\n",
        "    spam_tokens.extend(preprocess_text(text))\n",
        "\n",
        "# Count token frequencies\n",
        "legitimate_counter = Counter(legitimate_tokens)\n",
        "spam_counter = Counter(spam_tokens)\n",
        "\n",
        "legitimate_common = legitimate_counter.most_common(20)\n",
        "spam_common = spam_counter.most_common(20)\n",
        "\n",
        "print(\"\\nMost common words in legitimate messages:\")\n",
        "for word, count in legitimate_common:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nMost common words in spam messages:\")\n",
        "for word, count in spam_common:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Visualize most common words\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "words, counts = zip(*legitimate_common)\n",
        "sns.barplot(x=list(counts), y=list(words), ax=ax1)\n",
        "ax1.set_title('Most Common Words in Legitimate Messages')\n",
        "ax1.set_xlabel('Count')\n",
        "\n",
        "words, counts = zip(*spam_common)\n",
        "sns.barplot(x=list(counts), y=list(words), ax=ax2)\n",
        "ax2.set_title('Most Common Words in Spam Messages')\n",
        "ax2.set_xlabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Generate word clouds\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "legitimate_wordcloud = WordCloud(\n",
        "    width=800, height=400, background_color='white', max_words=100\n",
        ").generate(' '.join(legitimate_tokens))\n",
        "ax1.imshow(legitimate_wordcloud, interpolation='bilinear')\n",
        "ax1.set_title('Word Cloud for Legitimate Messages')\n",
        "ax1.axis('off')\n",
        "\n",
        "spam_wordcloud = WordCloud(\n",
        "    width=800, height=400, background_color='white', max_words=100\n",
        ").generate(' '.join(spam_tokens))\n",
        "ax2.imshow(spam_wordcloud, interpolation='bilinear')\n",
        "ax2.set_title('Word Cloud for Spam Messages')\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "def get_ngrams(texts, n=2, min_df=5):\n",
        "    \"\"\"Extract n-grams from texts.\"\"\"\n",
        "    min_df_value = 2 if n == 3 else min_df\n",
        "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english', min_df=min_df_value)\n",
        "\n",
        "    try:\n",
        "        X = vectorizer.fit_transform(texts)\n",
        "        ngrams = vectorizer.get_feature_names_out()\n",
        "        ngram_counts = X.sum(axis=0).A1\n",
        "        ngram_freq = [(ngram, count) for ngram, count in zip(ngrams, ngram_counts)]\n",
        "        return sorted(ngram_freq, key=lambda x: x[1], reverse=True)\n",
        "    except ValueError:\n",
        "        return []\n",
        "\n",
        "# Extract and visualize bigrams and trigrams\n",
        "legitimate_bigrams = get_ngrams(legitimate_texts, 2)[:15]\n",
        "spam_bigrams = get_ngrams(spam_texts, 2)[:15]\n",
        "\n",
        "legitimate_trigrams = get_ngrams(legitimate_texts, 3)[:15]\n",
        "spam_trigrams = get_ngrams(spam_texts, 3)[:15]\n",
        "\n",
        "# Visualize bigrams\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "if legitimate_bigrams:\n",
        "    words, counts = zip(*legitimate_bigrams)\n",
        "    sns.barplot(x=list(counts), y=list(words), ax=ax1)\n",
        "    ax1.set_title('Most Common Bigrams in Legitimate Messages')\n",
        "    ax1.set_xlabel('Count')\n",
        "else:\n",
        "    ax1.set_title('No Common Bigrams Found in Legitimate Messages')\n",
        "\n",
        "if spam_bigrams:\n",
        "    words, counts = zip(*spam_bigrams)\n",
        "    sns.barplot(x=list(counts), y=list(words), ax=ax2)\n",
        "    ax2.set_title('Most Common Bigrams in Spam Messages')\n",
        "    ax2.set_xlabel('Count')\n",
        "else:\n",
        "    ax2.set_title('No Common Bigrams Found in Spam Messages')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# TF-IDF analysis\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', min_df=5)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "legitimate_indices = df[df['label'] == 0].index\n",
        "spam_indices = df[df['label'] == 1].index\n",
        "\n",
        "try:\n",
        "    legitimate_tfidf = tfidf_matrix[legitimate_indices].mean(axis=0).A1\n",
        "    spam_tfidf = tfidf_matrix[spam_indices].mean(axis=0).A1\n",
        "    diff_tfidf = spam_tfidf - legitimate_tfidf\n",
        "\n",
        "    top_spam_indices = diff_tfidf.argsort()[-20:][::-1]\n",
        "    top_legitimate_indices = diff_tfidf.argsort()[:20]\n",
        "\n",
        "    top_spam_terms = [(feature_names[i], diff_tfidf[i]) for i in top_spam_indices]\n",
        "    top_legitimate_terms = [(feature_names[i], -diff_tfidf[i]) for i in top_legitimate_indices]\n",
        "\n",
        "    # Visualize most distinctive terms\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "    words, scores = zip(*top_spam_terms)\n",
        "    sns.barplot(x=list(scores), y=list(words), ax=ax1)\n",
        "    ax1.set_title('Most Distinctive Terms for Spam Messages')\n",
        "    ax1.set_xlabel('TF-IDF Difference')\n",
        "\n",
        "    words, scores = zip(*top_legitimate_terms)\n",
        "    sns.barplot(x=list(scores), y=list(words), ax=ax2)\n",
        "    ax2.set_title('Most Distinctive Terms for Legitimate Messages')\n",
        "    ax2.set_xlabel('TF-IDF Difference')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in TF-IDF analysis: {e}\")\n",
        "\n",
        "# Pattern analysis functions\n",
        "def contains_url(text):\n",
        "    \"\"\"Detect URL tokens in text.\"\"\"\n",
        "    return 'urltoken' in text.lower()\n",
        "\n",
        "def contains_phone(text):\n",
        "    \"\"\"Detect phone number tokens in text.\"\"\"\n",
        "    return 'phonetoken' in text.lower()\n",
        "\n",
        "def contains_money(text):\n",
        "    \"\"\"Detect money tokens in text.\"\"\"\n",
        "    return 'moneytoken' in text.lower()\n",
        "\n",
        "def contains_urgent_words(text):\n",
        "    \"\"\"Detect urgency-related words in text.\"\"\"\n",
        "    urgent_words = [\n",
        "        'urgent', 'now', 'today', 'immediately', 'hurry', 'quick', 'fast',\n",
        "        'limited', 'offer', 'exclusive', 'expires', 'deadline', 'alert',\n",
        "        'confirm', 'verify', 'activate', 'suspended', 'blocked', 'expire',\n",
        "        'winner', 'congratulations', 'prize', 'bonus', 'promotion'\n",
        "    ]\n",
        "    text_lower = text.lower()\n",
        "    return any(word in text_lower.split() for word in urgent_words)\n",
        "\n",
        "# Apply pattern detection\n",
        "df['has_url'] = df['cleaned_text'].apply(contains_url)\n",
        "df['has_phone'] = df['cleaned_text'].apply(contains_phone)\n",
        "df['has_money'] = df['cleaned_text'].apply(contains_money)\n",
        "df['has_urgent'] = df['cleaned_text'].apply(contains_urgent_words)\n",
        "\n",
        "# Count pattern occurrences by class\n",
        "legitimate_count = df[df['label'] == 0].shape[0]\n",
        "spam_count = df[df['label'] == 1].shape[0]\n",
        "\n",
        "if legitimate_count > 0 and spam_count > 0:\n",
        "    pattern_counts = pd.DataFrame({\n",
        "        'Legitimate': [\n",
        "            df[(df['label'] == 0) & df['has_url']].shape[0],\n",
        "            df[(df['label'] == 0) & df['has_phone']].shape[0],\n",
        "            df[(df['label'] == 0) & df['has_money']].shape[0],\n",
        "            df[(df['label'] == 0) & df['has_urgent']].shape[0]\n",
        "        ],\n",
        "        'Spam': [\n",
        "            df[(df['label'] == 1) & df['has_url']].shape[0],\n",
        "            df[(df['label'] == 1) & df['has_phone']].shape[0],\n",
        "            df[(df['label'] == 1) & df['has_money']].shape[0],\n",
        "            df[(df['label'] == 1) & df['has_urgent']].shape[0]\n",
        "        ]\n",
        "    }, index=['URLs', 'Phone Numbers', 'Money References', 'Urgency Words'])\n",
        "\n",
        "    pattern_percentages = pd.DataFrame({\n",
        "        'Legitimate (%)': pattern_counts['Legitimate'] / legitimate_count * 100,\n",
        "        'Spam (%)': pattern_counts['Spam'] / spam_count * 100\n",
        "    })\n",
        "\n",
        "    print(\"\\nPattern occurrences by class:\")\n",
        "    print(pattern_counts)\n",
        "\n",
        "    print(\"\\nPattern percentages by class:\")\n",
        "    print(pattern_percentages)\n",
        "\n",
        "    # Visualize pattern percentages\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    pattern_percentages.plot(kind='bar', rot=0)\n",
        "    plt.title('Percentage of Messages Containing Specific Patterns')\n",
        "    plt.ylabel('Percentage (%)')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Correlation analysis\n",
        "corr_features = [\n",
        "    'text_length', 'word_count', 'avg_word_length',\n",
        "    'uppercase_ratio', 'digit_ratio', 'special_char_ratio',\n",
        "    'has_url', 'has_phone', 'has_money', 'has_urgent', 'label'\n",
        "]\n",
        "\n",
        "corr_df = df[corr_features].copy()\n",
        "corr_df[['has_url', 'has_phone', 'has_money', 'has_urgent']] = \\\n",
        "    corr_df[['has_url', 'has_phone', 'has_money', 'has_urgent']].astype(int)\n",
        "\n",
        "correlation_matrix = corr_df.corr()\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(\n",
        "    correlation_matrix,\n",
        "    annot=True,\n",
        "    mask=mask,\n",
        "    cmap='coolwarm',\n",
        "    vmin=-1,\n",
        "    vmax=1,\n",
        "    fmt='.2f',\n",
        "    linewidths=0.5\n",
        ")\n",
        "plt.title('Correlation Matrix of Text Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Extract correlations with label\n",
        "label_correlations = correlation_matrix['label'].drop('label').sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=label_correlations.values, y=label_correlations.index)\n",
        "plt.title('Feature Correlations with Spam Label')\n",
        "plt.xlabel('Correlation Coefficient')\n",
        "plt.axvline(x=0, color='black', linestyle='--')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display example messages\n",
        "print(\"\\nExample Legitimate Messages:\")\n",
        "legitimate_examples = df[df['label'] == 0].sample(min(5, legitimate_count))['message'].tolist()\n",
        "for i, msg in enumerate(legitimate_examples):\n",
        "    print(f\"\\n{i+1}. {msg}\")\n",
        "\n",
        "print(\"\\n\\nExample Spam Messages:\")\n",
        "spam_examples = df[df['label'] == 1].sample(min(5, spam_count))['message'].tolist()\n",
        "for i, msg in enumerate(spam_examples):\n",
        "    print(f\"\\n{i+1}. {msg}\")\n",
        "\n",
        "# Save preprocessed data\n",
        "output_file = 'data/processed/sms_dataset_explored.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"\\nPreprocessed dataset saved to {output_file}\")\n",
        "\n",
        "print(\"\\nData exploration completed successfully!\")"
      ]
    }
  ]
}