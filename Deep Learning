{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1: Set up environment and mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up project paths\n",
        "import os\n",
        "\n",
        "# Define project directory\n",
        "project_dir = '/content/drive/MyDrive/final-sms-scam-detection'\n",
        "os.chdir(project_dir)\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "CHXOm5bH-DbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2: Install required packages\n",
        "!pip install numpy pandas matplotlib seaborn nltk scikit-learn torch torchtext tqdm optuna joblib"
      ],
      "metadata": {
        "id": "bZVQ3sNS-cFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3: Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim import Adam\n",
        "import optuna\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
        ")\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib"
      ],
      "metadata": {
        "id": "bsp_HD3f_YI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set plotting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)"
      ],
      "metadata": {
        "id": "AgJ_h3hVNYLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)"
      ],
      "metadata": {
        "id": "y00ugU5RNUOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "pMS8KEb8Nck5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "id8fNxr0NaQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4: Load data\n",
        "# Define paths\n",
        "data_dir = \"data/processed/\"\n",
        "model_dir = \"models/deep_learning/\"\n",
        "results_dir = \"results/\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(results_dir, \"metrics\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(results_dir, \"visualizations\"), exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
        "val_df = pd.read_csv(os.path.join(data_dir, \"val.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
        "\n",
        "print(f\"Loaded data: Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "6_ITZ-zT_ktY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print class distribution to understand imbalance\n",
        "print(\"\\nClass Distribution:\")\n",
        "print(\"Training Set:\")\n",
        "print(train_df['label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "print(\"\\nValidation Set:\")\n",
        "print(val_df['label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(test_df['label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Calculate imbalance ratio\n",
        "train_neg_count = (train_df['label'] == 0).sum()\n",
        "train_pos_count = (train_df['label'] == 1).sum()\n",
        "imbalance_ratio = train_neg_count / train_pos_count\n",
        "print(f\"\\nImbalance ratio (negative:positive): {imbalance_ratio:.2f}:1\")"
      ],
      "metadata": {
        "id": "Tx_4I6S0QoZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5: Define Tokenizer class\n",
        "class Tokenizer:\n",
        "    \"\"\"Simple tokenizer for text data.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=10000, max_seq_length=100):\n",
        "        \"\"\"\n",
        "        Initialize the Tokenizer.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Maximum size of vocabulary.\n",
        "            max_seq_length (int): Maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
        "        self.word_counts = {}\n",
        "\n",
        "    def fit(self, texts):\n",
        "        \"\"\"\n",
        "        Fit the tokenizer to the provided texts.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents.\n",
        "        \"\"\"\n",
        "        # Count word frequencies\n",
        "        for text in texts:\n",
        "            for word in text.split():\n",
        "                self.word_counts[word] = self.word_counts.get(word, 0) + 1\n",
        "\n",
        "        # Sort words by frequency\n",
        "        sorted_words = sorted(self.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Add most frequent words to vocabulary (up to vocab_size)\n",
        "        for i, (word, _) in enumerate(sorted_words[:self.vocab_size - 2]):  # -2 for <PAD> and <UNK>\n",
        "            idx = i + 2  # +2 for <PAD> and <UNK>\n",
        "            self.word_to_idx[word] = idx\n",
        "            self.idx_to_word[idx] = word\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.word_to_idx)}\")\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        \"\"\"\n",
        "        Convert texts to sequences of indices.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents.\n",
        "\n",
        "        Returns:\n",
        "            list: List of sequences.\n",
        "        \"\"\"\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            words = text.split()\n",
        "            sequence = [self.word_to_idx.get(word, 1) for word in words]  # 1 for <UNK>\n",
        "            sequences.append(sequence)\n",
        "        return sequences\n",
        "\n",
        "    def pad_sequences(self, sequences):\n",
        "        \"\"\"\n",
        "        Pad sequences to the maximum sequence length.\n",
        "\n",
        "        Args:\n",
        "            sequences (list): List of sequences.\n",
        "\n",
        "        Returns:\n",
        "            list: List of padded sequences.\n",
        "        \"\"\"\n",
        "        padded_sequences = []\n",
        "        for sequence in sequences:\n",
        "            # Truncate if longer than max_seq_length\n",
        "            if len(sequence) > self.max_seq_length:\n",
        "                padded_sequence = sequence[:self.max_seq_length]\n",
        "            # Pad if shorter than max_seq_length\n",
        "            else:\n",
        "                padded_sequence = sequence + [0] * (self.max_seq_length - len(sequence))\n",
        "            padded_sequences.append(padded_sequence)\n",
        "        return padded_sequences\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"\n",
        "        Save the tokenizer to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to save the tokenizer.\n",
        "        \"\"\"\n",
        "        tokenizer_data = {\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'max_seq_length': self.max_seq_length,\n",
        "            'word_to_idx': self.word_to_idx,\n",
        "            'idx_to_word': {int(k): v for k, v in self.idx_to_word.items()},\n",
        "            'word_counts': self.word_counts\n",
        "        }\n",
        "\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(tokenizer_data, f)\n",
        "\n",
        "        print(f\"Tokenizer saved to {filepath}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filepath):\n",
        "        \"\"\"\n",
        "        Load a tokenizer from a JSON file.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to load the tokenizer from.\n",
        "\n",
        "        Returns:\n",
        "            Tokenizer: Loaded tokenizer.\n",
        "        \"\"\"\n",
        "        with open(filepath, 'r') as f:\n",
        "            tokenizer_data = json.load(f)\n",
        "\n",
        "        tokenizer = cls(tokenizer_data['vocab_size'], tokenizer_data['max_seq_length'])\n",
        "        tokenizer.word_to_idx = tokenizer_data['word_to_idx']\n",
        "        tokenizer.idx_to_word = {int(k): v for k, v in tokenizer_data['idx_to_word'].items()}\n",
        "        tokenizer.word_counts = tokenizer_data['word_counts']\n",
        "\n",
        "        print(f\"Tokenizer loaded from {filepath}\")\n",
        "        return tokenizer"
      ],
      "metadata": {
        "id": "Nyf-ybX9_1Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6: Define Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"Dataset for text classification.\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, labels):\n",
        "        \"\"\"\n",
        "        Initialize the TextDataset.\n",
        "\n",
        "        Args:\n",
        "            sequences (list): List of tokenized sequences.\n",
        "            labels (numpy.ndarray): Labels.\n",
        "        \"\"\"\n",
        "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "x2xJzF0X_4ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7: Overfitting Detection and Mitigation Functions\n",
        "def detect_overfitting_dl(model, train_loader, val_loader, test_loader, device, model_name):\n",
        "    \"\"\"Detect overfitting by comparing train/val/test performance for deep learning models.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    def evaluate_loader(loader):\n",
        "        preds, labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs).squeeze()\n",
        "                pred_classes = (torch.sigmoid(outputs) > 0.5).cpu().numpy()\n",
        "                preds.extend(pred_classes)\n",
        "                labels.extend(targets.cpu().numpy())\n",
        "\n",
        "        f1 = f1_score(labels, preds)\n",
        "        mcc = matthews_corrcoef(labels, preds)\n",
        "        return f1, mcc\n",
        "\n",
        "    train_f1, train_mcc = evaluate_loader(train_loader)\n",
        "    val_f1, val_mcc = evaluate_loader(val_loader)\n",
        "    test_f1, test_mcc = evaluate_loader(test_loader)\n",
        "\n",
        "    train_test_f1_gap = train_f1 - test_f1\n",
        "    train_test_mcc_gap = train_mcc - test_mcc\n",
        "\n",
        "    if train_test_f1_gap > 0.15:\n",
        "        overfitting_level = \"SEVERE\"\n",
        "    elif train_test_f1_gap > 0.10:\n",
        "        overfitting_level = \"MODERATE\"\n",
        "    elif train_test_f1_gap > 0.05:\n",
        "        overfitting_level = \"MILD\"\n",
        "    else:\n",
        "        overfitting_level = \"MINIMAL\"\n",
        "\n",
        "    print(f\"\\n{model_name} Overfitting Analysis:\")\n",
        "    print(f\"  F1 - Train: {train_f1:.4f}, Val: {val_f1:.4f}, Test: {test_f1:.4f}\")\n",
        "    print(f\"  MCC - Train: {train_mcc:.4f}, Val: {val_mcc:.4f}, Test: {test_mcc:.4f}\")\n",
        "    print(f\"  Train-Test F1 Gap: {train_test_f1_gap:.4f}\")\n",
        "    print(f\"  Overfitting Level: {overfitting_level}\")\n",
        "\n",
        "    return {\n",
        "        'train_test_f1_gap': train_test_f1_gap,\n",
        "        'train_test_mcc_gap': train_test_mcc_gap,\n",
        "        'overfitting_level': overfitting_level,\n",
        "        'test_f1': test_f1,\n",
        "        'test_mcc': test_mcc\n",
        "    }\n",
        "\n",
        "def add_early_stopping_and_regularization(training_stats, patience=3):\n",
        "    \"\"\"Add early stopping based on validation MCC.\"\"\"\n",
        "    val_mcc_scores = training_stats['val_mcc']\n",
        "\n",
        "    if len(val_mcc_scores) < patience:\n",
        "        return False\n",
        "\n",
        "    # Check if validation MCC has not improved for 'patience' epochs\n",
        "    best_mcc = max(val_mcc_scores)\n",
        "    recent_scores = val_mcc_scores[-patience:]\n",
        "\n",
        "    if all(score <= best_mcc - 0.01 for score in recent_scores):\n",
        "        print(f\"Early stopping triggered - no improvement for {patience} epochs\")\n",
        "        return True\n",
        "\n",
        "    return False"
      ],
      "metadata": {
        "id": "NtTrHjdOhh6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8: Data Preparation\n",
        "# Check if we have a 'cleaned_text' column, if not create it\n",
        "if 'cleaned_text' not in train_df.columns:\n",
        "    print(\"Adding cleaned_text column...\")\n",
        "    def clean_text(text):\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "        # Remove email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        # Remove phone numbers\n",
        "        text = re.sub(r'\\b\\d{10,}\\b', '', text)\n",
        "        text = re.sub(r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', '', text)\n",
        "        # Remove non-ASCII characters\n",
        "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    train_df['cleaned_text'] = train_df['message'].apply(clean_text)\n",
        "    val_df['cleaned_text'] = val_df['message'].apply(clean_text)\n",
        "    test_df['cleaned_text'] = test_df['message'].apply(clean_text)\n",
        "\n",
        "# Prepare data for deep learning models\n",
        "vocab_size = 10000\n",
        "max_seq_length = 100\n",
        "embedding_dim = 100\n",
        "\n",
        "# Create and fit tokenizer\n",
        "tokenizer = Tokenizer(vocab_size=vocab_size, max_seq_length=max_seq_length)\n",
        "tokenizer.fit(train_df['cleaned_text'].tolist())\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer_path = os.path.join(model_dir, 'tokenizer.json')\n",
        "tokenizer.save(tokenizer_path)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "X_train_seq = tokenizer.pad_sequences(tokenizer.texts_to_sequences(train_df['cleaned_text'].tolist()))\n",
        "X_val_seq = tokenizer.pad_sequences(tokenizer.texts_to_sequences(val_df['cleaned_text'].tolist()))\n",
        "X_test_seq = tokenizer.pad_sequences(tokenizer.texts_to_sequences(test_df['cleaned_text'].tolist()))\n",
        "\n",
        "# Get labels\n",
        "y_train = train_df['label'].values\n",
        "y_val = val_df['label'].values\n",
        "y_test = test_df['label'].values\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TextDataset(X_train_seq, y_train)\n",
        "val_dataset = TextDataset(X_val_seq, y_val)\n",
        "test_dataset = TextDataset(X_test_seq, y_test)\n",
        "\n",
        "print(f\"Prepared datasets - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "0YdSJKI8__kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextCNN(nn.Module):\n",
        "    \"\"\"CNN-based text classifier with enhanced regularization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, max_seq_length, num_filters=128,\n",
        "                 filter_sizes=(3, 4, 5), num_classes=1, dropout=0.5, l2_reg=0.01):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "        # Add embedding dropout\n",
        "        self.embedding_dropout = nn.Dropout(dropout * 0.5)\n",
        "\n",
        "        # Convolutional layers with different filter sizes\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (filter_size, embedding_dim))\n",
        "            for filter_size in filter_sizes\n",
        "        ])\n",
        "\n",
        "        # Add batch normalization\n",
        "        self.batch_norms = nn.ModuleList([\n",
        "            nn.BatchNorm2d(num_filters) for _ in filter_sizes\n",
        "        ])\n",
        "\n",
        "        # Multiple dropout layers\n",
        "        self.dropout1 = nn.Dropout(dropout * 0.7)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Reduced fully connected layer with batch norm\n",
        "        hidden_dim = num_filters * len(filter_sizes) // 2\n",
        "        self.fc1 = nn.Linear(num_filters * len(filter_sizes), hidden_dim)\n",
        "        self.fc1_bn = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding with dropout\n",
        "        x = self.embedding(x)\n",
        "        x = self.embedding_dropout(x)\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Apply convolutions with batch norm\n",
        "        conv_outputs = []\n",
        "        for conv, bn in zip(self.convs, self.batch_norms):\n",
        "            h = F.relu(bn(conv(x)))\n",
        "            h = F.max_pool2d(h, (h.size(2), 1)).squeeze(3).squeeze(2)\n",
        "            conv_outputs.append(h)\n",
        "\n",
        "        # Concatenate and apply dropout\n",
        "        x = torch.cat(conv_outputs, 1)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # First FC layer with batch norm\n",
        "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Final layer\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def l2_penalty(self):\n",
        "        \"\"\"Calculate L2 penalty for regularization.\"\"\"\n",
        "        l2_penalty = 0\n",
        "        for param in self.parameters():\n",
        "            l2_penalty += torch.norm(param, 2) ** 2\n",
        "        return self.l2_reg * l2_penalty"
      ],
      "metadata": {
        "id": "TPjMBFUEAJ_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMClassifier(nn.Module):\n",
        "    \"\"\"BiLSTM-based text classifier with enhanced regularization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128, num_layers=2,\n",
        "                 num_classes=1, dropout=0.5, l2_reg=0.01):\n",
        "        super(BiLSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.l2_reg = l2_reg\n",
        "        self.hidden_dim = hidden_dim # Store hidden_dim as an instance variable\n",
        "\n",
        "        # Add embedding dropout\n",
        "        self.embedding_dropout = nn.Dropout(dropout * 0.3)\n",
        "\n",
        "        # Bidirectional LSTM with more conservative settings\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=min(num_layers, 2),  # Limit layers for small datasets\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Multiple dropout and regularization layers\n",
        "        self.dropout1 = nn.Dropout(dropout * 0.7)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Reduced hidden layer\n",
        "        lstm_output_dim = hidden_dim * 2\n",
        "        hidden_fc_dim = hidden_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(lstm_output_dim, hidden_fc_dim)\n",
        "        self.fc1_bn = nn.BatchNorm1d(hidden_fc_dim)\n",
        "        self.fc2 = nn.Linear(hidden_fc_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding with dropout\n",
        "        x = self.embedding(x)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        # LSTM\n",
        "        output, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # Use mean pooling instead of just final states to capture more info\n",
        "        # But also include final states\n",
        "        mean_pool = torch.mean(output, dim=1)  # (batch_size, hidden_dim * 2)\n",
        "        final_states = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "\n",
        "        # Combine mean pooling and final states\n",
        "        x = torch.cat([mean_pool, final_states], dim=1)\n",
        "\n",
        "        # Reduce dimension back to expected size\n",
        "        x = x[:, :self.hidden_dim * 2]  # Use self.hidden_dim\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # First FC layer with batch norm\n",
        "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Final layer\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def l2_penalty(self):\n",
        "        \"\"\"Calculate L2 penalty for regularization.\"\"\"\n",
        "        l2_penalty = 0\n",
        "        for param in self.parameters():\n",
        "            l2_penalty += torch.norm(param, 2) ** 2\n",
        "        return self.l2_reg * l2_penalty"
      ],
      "metadata": {
        "id": "OlEj1f2lAPD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9: Class Imbalance Handling\n",
        "\n",
        "# A. Weighted Loss Function\n",
        "def get_weighted_loss_function(train_df):\n",
        "    \"\"\"\n",
        "    Create a weighted BCE loss function based on class imbalance.\n",
        "\n",
        "    Args:\n",
        "        train_df (pandas.DataFrame): Training dataframe with 'label' column.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.BCEWithLogitsLoss: Weighted loss function.\n",
        "    \"\"\"\n",
        "    # Calculate class weights based on class distribution\n",
        "    n_samples = len(train_df)\n",
        "    n_positive = train_df['label'].sum()\n",
        "    n_negative = n_samples - n_positive\n",
        "\n",
        "    # Determine weight for positive class (usually the minority)\n",
        "    # Higher weight for the positive (minority) class\n",
        "    weight_ratio = n_negative / n_positive\n",
        "\n",
        "    print(f\"Class imbalance - Negative: {n_negative}, Positive: {n_positive}, Ratio: {weight_ratio:.2f}\")\n",
        "    print(f\"Setting positive class weight to {weight_ratio:.2f}\")\n",
        "\n",
        "    # Set class weight for the positive class in BCEWithLogitsLoss\n",
        "    pos_weight = torch.tensor([weight_ratio]).to(device)\n",
        "    return nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# B. Balanced Sampling\n",
        "def create_balanced_sampler(y_train):\n",
        "    \"\"\"\n",
        "    Create a weighted random sampler to balance class distribution in batches.\n",
        "\n",
        "    Args:\n",
        "        y_train (numpy.ndarray): Training labels.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.data.WeightedRandomSampler: Balanced sampler.\n",
        "    \"\"\"\n",
        "    # Calculate sample weights inverse to class frequency\n",
        "    class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
        "    weight = 1. / class_sample_count\n",
        "    samples_weight = np.array([weight[t] for t in y_train])\n",
        "    samples_weight = torch.from_numpy(samples_weight).float()\n",
        "\n",
        "    # Create a weighted random sampler\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    return sampler\n",
        "\n",
        "print(\"\\n===== Class Imbalance Handling Options =====\")\n",
        "# Create weighted loss function\n",
        "weighted_criterion = get_weighted_loss_function(train_df)\n",
        "\n",
        "# Create balanced sampler\n",
        "balanced_sampler = create_balanced_sampler(y_train)"
      ],
      "metadata": {
        "id": "L2E8N7oZAVSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10: Hyperparameter Optimization with Optuna\n",
        "def objective_cnn(trial, max_epochs=8):  # Reduced epochs\n",
        "    \"\"\"Objective function for Optuna (CNN model) with overfitting prevention.\"\"\"\n",
        "    # More conservative hyperparameter ranges\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
        "    embedding_dim = trial.suggest_categorical('embedding_dim', [50, 100])\n",
        "    num_filters = trial.suggest_categorical('num_filters', [64, 128])\n",
        "    dropout = trial.suggest_float('dropout', 0.3, 0.7)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True)\n",
        "    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-2, log=True)\n",
        "    use_balanced_sampling = trial.suggest_categorical('use_balanced_sampling', [True, False])\n",
        "\n",
        "    # Create data loaders\n",
        "    if use_balanced_sampling:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=balanced_sampler)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model with regularization\n",
        "    model = TextCNN(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        max_seq_length=max_seq_length,\n",
        "        num_filters=num_filters,\n",
        "        dropout=dropout,\n",
        "        l2_reg=l2_reg\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = weighted_criterion\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    best_val_mcc = -1\n",
        "    patience_counter = 0\n",
        "    patience = 3\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "\n",
        "            # Add L2 penalty to loss\n",
        "            loss = criterion(outputs, labels) + model.l2_penalty()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs).squeeze()\n",
        "                val_preds.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_mcc = matthews_corrcoef(val_labels, val_preds)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_mcc > best_val_mcc:\n",
        "            best_val_mcc = val_mcc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "        trial.report(val_mcc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return best_val_mcc\n",
        "\n",
        "def objective_bilstm(trial, max_epochs=8):  # Reduced epochs\n",
        "    \"\"\"Objective function for Optuna (BiLSTM model) with overfitting prevention.\"\"\"\n",
        "    # More conservative hyperparameter ranges\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
        "    embedding_dim = trial.suggest_categorical('embedding_dim', [50, 100])\n",
        "    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128]) # Define hidden_dim here\n",
        "    num_layers = trial.suggest_int('num_layers', 1, 2)  # Reduced max layers\n",
        "    dropout = trial.suggest_float('dropout', 0.3, 0.7)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True)\n",
        "    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-2, log=True)\n",
        "    use_balanced_sampling = trial.suggest_categorical('use_balanced_sampling', [True, False])\n",
        "\n",
        "    # Create data loaders\n",
        "    if use_balanced_sampling:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=balanced_sampler)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model with regularization\n",
        "    model = BiLSTMClassifier(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        hidden_dim=hidden_dim, # Pass hidden_dim to the model\n",
        "        num_layers=num_layers,\n",
        "        num_classes=1,\n",
        "        dropout=dropout,\n",
        "        l2_reg=l2_reg\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = weighted_criterion\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    best_val_mcc = -1\n",
        "    patience_counter = 0\n",
        "    patience = 3\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "\n",
        "            # Add L2 penalty to loss\n",
        "            loss = criterion(outputs, labels) + model.l2_penalty()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs).squeeze()\n",
        "                val_preds.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_mcc = matthews_corrcoef(val_labels, val_preds)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_mcc > best_val_mcc:\n",
        "            best_val_mcc = val_mcc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "        trial.report(val_mcc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return best_val_mcc\n",
        "\n",
        "# Optimize CNN\n",
        "print(\"Optimizing CNN hyperparameters with Optuna...\")\n",
        "study_cnn = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
        "study_cnn.optimize(lambda trial: objective_cnn(trial), n_trials=10)\n",
        "\n",
        "print(\"Best CNN parameters:\", study_cnn.best_params)\n",
        "print(\"Best CNN validation MCC score:\", study_cnn.best_value)\n",
        "\n",
        "# Optimize BiLSTM\n",
        "print(\"Optimizing BiLSTM hyperparameters with Optuna...\")\n",
        "study_bilstm = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
        "study_bilstm.optimize(lambda trial: objective_bilstm(trial), n_trials=10)\n",
        "\n",
        "print(\"Best BiLSTM parameters:\", study_bilstm.best_params)\n",
        "print(\"Best BiLSTM validation MCC score:\", study_bilstm.best_value)\n",
        "\n",
        "# Save the studies\n",
        "optuna_dir = os.path.join(results_dir, 'optuna_results')\n",
        "os.makedirs(optuna_dir, exist_ok=True)\n",
        "joblib.dump(study_cnn, os.path.join(optuna_dir, 'cnn_study.pkl'))\n",
        "joblib.dump(study_bilstm, os.path.join(optuna_dir, 'bilstm_study.pkl'))"
      ],
      "metadata": {
        "id": "P3YpZf0GAewi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11: Training and Evaluation Functions\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_name):\n",
        "    \"\"\"Train the model with early stopping and return training statistics.\"\"\"\n",
        "    print(f\"Training {model_name} on {device} with early stopping...\")\n",
        "\n",
        "    # Initialize training statistics\n",
        "    training_stats = {\n",
        "        'train_loss': [], 'train_acc': [], 'train_f1': [], 'train_mcc': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_mcc': [],\n",
        "        'val_prec': [], 'val_rec': []\n",
        "    }\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_mcc = -1\n",
        "    patience_counter = 0\n",
        "    patience = 3\n",
        "    best_model_state = None\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "\n",
        "            # Add L2 penalty if model has it\n",
        "            loss = criterion(outputs, labels)\n",
        "            if hasattr(model, 'l2_penalty'):\n",
        "                loss += model.l2_penalty()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            train_preds.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate training metrics\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "        train_f1 = f1_score(train_labels, train_preds)\n",
        "        train_mcc = matthews_corrcoef(train_labels, train_preds)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs).squeeze()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                if hasattr(model, 'l2_penalty'):\n",
        "                    loss += model.l2_penalty()\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_preds.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "        val_f1 = f1_score(val_labels, val_preds)\n",
        "        val_mcc = matthews_corrcoef(val_labels, val_preds)\n",
        "        val_prec = precision_score(val_labels, val_preds)\n",
        "        val_rec = recall_score(val_labels, val_preds)\n",
        "\n",
        "        # Update training statistics\n",
        "        training_stats['train_loss'].append(train_loss)\n",
        "        training_stats['train_acc'].append(train_acc)\n",
        "        training_stats['train_f1'].append(train_f1)\n",
        "        training_stats['train_mcc'].append(train_mcc)\n",
        "        training_stats['val_loss'].append(val_loss)\n",
        "        training_stats['val_acc'].append(val_acc)\n",
        "        training_stats['val_f1'].append(val_f1)\n",
        "        training_stats['val_mcc'].append(val_mcc)\n",
        "        training_stats['val_prec'].append(val_prec)\n",
        "        training_stats['val_rec'].append(val_rec)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_mcc > best_val_mcc:\n",
        "            best_val_mcc = val_mcc\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(f\"New best validation MCC: {best_val_mcc:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Log epoch statistics\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Train MCC: {train_mcc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val MCC: {val_mcc:.4f}\")\n",
        "        print(f\"Val Precision: {val_prec:.4f}, Val Recall: {val_rec:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    # Load best model state\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"Loaded best model with validation MCC: {best_val_mcc:.4f}\")\n",
        "\n",
        "    return model, training_stats"
      ],
      "metadata": {
        "id": "cLfSVuC8OK5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
        "    print(\"Evaluating model on test set...\")\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize evaluation statistics\n",
        "    test_loss = 0.0\n",
        "    test_preds = []\n",
        "    test_probs = []\n",
        "    test_labels = []\n",
        "\n",
        "    # Iterate over test batches\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            # Move inputs and labels to device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update statistics\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Store predictions, probabilities, and labels\n",
        "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "            preds = (probs > 0.5).astype(int)\n",
        "\n",
        "            test_probs.extend(probs)\n",
        "            test_preds.extend(preds)\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average test loss\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    test_acc = accuracy_score(test_labels, test_preds)\n",
        "    test_prec = precision_score(test_labels, test_preds)\n",
        "    test_rec = recall_score(test_labels, test_preds)\n",
        "    test_f1 = f1_score(test_labels, test_preds)\n",
        "    test_mcc = matthews_corrcoef(test_labels, test_preds)\n",
        "    test_roc_auc = roc_auc_score(test_labels, test_probs)\n",
        "    test_pr_auc = average_precision_score(test_labels, test_probs)\n",
        "    test_cm = confusion_matrix(test_labels, test_preds)\n",
        "    test_report = classification_report(test_labels, test_preds, output_dict=True)\n",
        "\n",
        "    # Log evaluation results\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Precision: {test_prec:.4f}\")\n",
        "    print(f\"Test Recall: {test_rec:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "    print(f\"Test MCC: {test_mcc:.4f}\")\n",
        "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
        "    print(f\"Test PR AUC: {test_pr_auc:.4f}\")\n",
        "    print(f\"Test Confusion Matrix:\\n{test_cm}\")\n",
        "    print(f\"Test Classification Report:\\n{classification_report(test_labels, test_preds)}\")\n",
        "\n",
        "    # Return evaluation results\n",
        "    return {\n",
        "        'test_loss': test_loss,\n",
        "        'test_acc': test_acc,\n",
        "        'test_prec': test_prec,\n",
        "        'test_rec': test_rec,\n",
        "        'test_f1': test_f1,\n",
        "        'test_mcc': test_mcc,\n",
        "        'test_roc_auc': test_roc_auc,\n",
        "        'test_pr_auc': test_pr_auc,\n",
        "        'test_cm': test_cm,\n",
        "        'test_report': test_report,\n",
        "        'test_probs': test_probs,\n",
        "        'test_preds': test_preds,\n",
        "        'test_labels': test_labels\n",
        "    }"
      ],
      "metadata": {
        "id": "f0sf2i3cOWEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_training(training_stats, model_name):\n",
        "    \"\"\"Visualize training progress.\"\"\"\n",
        "    # Create figure for loss, accuracy and F1 score\n",
        "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 6))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(training_stats['train_loss'], label='Train Loss')\n",
        "    ax1.plot(training_stats['val_loss'], label='Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax2.plot(training_stats['train_acc'], label='Train Accuracy')\n",
        "    ax2.plot(training_stats['val_acc'], label='Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Score')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot F1 score\n",
        "    ax3.plot(training_stats['train_f1'], label='Train F1')\n",
        "    ax3.plot(training_stats['val_f1'], label='Validation F1')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_ylabel('Score')\n",
        "    ax3.set_title('Training and Validation F1 Score')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "    # Plot MCC\n",
        "    ax4.plot(training_stats['train_mcc'], label='Train MCC')\n",
        "    ax4.plot(training_stats['val_mcc'], label='Validation MCC')\n",
        "    ax4.set_xlabel('Epoch')\n",
        "    ax4.set_ylabel('Score')\n",
        "    ax4.set_title('Training and Validation MCC')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True)\n",
        "\n",
        "    # Save the visualization\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'visualizations', f\"{model_name}_training_progress.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Plot precision and recall\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_stats['val_prec'], label='Validation Precision')\n",
        "    plt.plot(training_stats['val_rec'], label='Validation Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Validation Precision and Recall')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'visualizations', f\"{model_name}_precision_recall.png\"))\n",
        "    plt.show()\n",
        "\n",
        "def visualize_evaluation(eval_results, model_name):\n",
        "    \"\"\"Visualize evaluation results.\"\"\"\n",
        "    # Confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        eval_results['test_cm'],\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=['Legitimate', 'Scam'],\n",
        "        yticklabels=['Legitimate', 'Scam']\n",
        "    )\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'visualizations', f\"{model_name}_confusion_matrix.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # ROC curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    fpr, tpr, _ = roc_curve(eval_results['test_labels'], eval_results['test_probs'])\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {eval_results[\"test_roc_auc\"]:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{model_name} - ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'visualizations', f\"{model_name}_roc_curve.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Precision-Recall curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    precision, recall, _ = precision_recall_curve(eval_results['test_labels'], eval_results['test_probs'])\n",
        "    plt.step(recall, precision, where='post', lw=2, label=f'PR curve (AP = {eval_results[\"test_pr_auc\"]:.4f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'{model_name} - Precision-Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'visualizations', f\"{model_name}_pr_curve.png\"))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "fAYPKdW0OcB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12: Train and Evaluate CNN with Best Parameters\n",
        "# Get best CNN parameters\n",
        "best_cnn_params = study_cnn.best_params\n",
        "print(\"Best CNN parameters:\", best_cnn_params)\n",
        "\n",
        "# Create data loaders based on best parameters\n",
        "batch_size = best_cnn_params['batch_size']\n",
        "# Use balanced sampling if it was found to be optimal\n",
        "if best_cnn_params.get('use_balanced_sampling', False):\n",
        "    print(\"Using balanced sampling for CNN training...\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=balanced_sampler)\n",
        "else:\n",
        "    print(\"Using normal sampling for CNN training...\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize CNN model with best parameters\n",
        "cnn_model = TextCNN(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=best_cnn_params['embedding_dim'],\n",
        "    max_seq_length=max_seq_length,\n",
        "    num_filters=best_cnn_params['num_filters'],\n",
        "    filter_sizes=(3, 4, 5),\n",
        "    num_classes=1,\n",
        "    dropout=best_cnn_params['dropout']\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(cnn_model)\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in cnn_model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "# Always use weighted loss for class imbalance\n",
        "criterion = weighted_criterion\n",
        "optimizer = Adam(cnn_model.parameters(), lr=best_cnn_params['learning_rate'])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "start_time = time.time()\n",
        "cnn_model, cnn_training_stats = train_model(\n",
        "    cnn_model, train_loader, val_loader, criterion, optimizer, num_epochs, device, 'cnn'\n",
        ")\n",
        "cnn_training_time = time.time() - start_time\n",
        "print(f\"CNN training completed in {cnn_training_time:.2f} seconds\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(cnn_model.state_dict(), os.path.join(model_dir, 'cnn_model.pt'))\n",
        "print(f\"CNN model saved to {os.path.join(model_dir, 'cnn_model.pt')}\")\n",
        "\n",
        "# Visualize training progress\n",
        "visualize_training(cnn_training_stats, 'cnn')\n",
        "\n",
        "# Evaluate the model\n",
        "cnn_eval_results = evaluate_model(cnn_model, test_loader, criterion, device)\n",
        "\n",
        "# Add training time to evaluation results\n",
        "cnn_eval_results['training_time'] = cnn_training_time\n",
        "\n",
        "# Visualize evaluation results\n",
        "visualize_evaluation(cnn_eval_results, 'cnn')\n",
        "\n",
        "# Save CNN results\n",
        "cnn_results_df = pd.DataFrame({\n",
        "    'Model': ['TextCNN'],\n",
        "    'Training Time (s)': [cnn_eval_results['training_time']],\n",
        "    'Train Accuracy': [cnn_training_stats['train_acc'][-1]],\n",
        "    'Val Accuracy': [cnn_training_stats['val_acc'][-1]],\n",
        "    'Test Accuracy': [cnn_eval_results['test_acc']],\n",
        "    'Train F1': [cnn_training_stats['train_f1'][-1]],\n",
        "    'Val F1': [cnn_training_stats['val_f1'][-1]],\n",
        "    'Test F1': [cnn_eval_results['test_f1']],\n",
        "    'Train MCC': [cnn_training_stats['train_mcc'][-1]],\n",
        "    'Val MCC': [cnn_training_stats['val_mcc'][-1]],\n",
        "    'Test MCC': [cnn_eval_results['test_mcc']],\n",
        "    'Test Precision': [cnn_eval_results['test_prec']],\n",
        "    'Test Recall': [cnn_eval_results['test_rec']],\n",
        "    'Test ROC AUC': [cnn_eval_results['test_roc_auc']],\n",
        "    'Test PR AUC': [cnn_eval_results['test_pr_auc']]\n",
        "})\n",
        "\n",
        "cnn_results_df.to_csv(os.path.join(results_dir, 'metrics', 'cnn_eval_results.csv'), index=False)"
      ],
      "metadata": {
        "id": "zF7KCwR_Omqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13: Train and Evaluate BiLSTM with Best Parameters\n",
        "# Get best BiLSTM parameters\n",
        "best_bilstm_params = study_bilstm.best_params\n",
        "print(\"Best BiLSTM parameters:\", best_bilstm_params)\n",
        "\n",
        "# Create data loaders based on best parameters\n",
        "batch_size = best_bilstm_params['batch_size']\n",
        "# Use balanced sampling if it was found to be optimal\n",
        "if best_bilstm_params.get('use_balanced_sampling', False):\n",
        "    print(\"Using balanced sampling for BiLSTM training...\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=balanced_sampler)\n",
        "else:\n",
        "    print(\"Using normal sampling for BiLSTM training...\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize BiLSTM model with best parameters\n",
        "bilstm_model = BiLSTMClassifier( # Changed from BiLSTMEnhanced to BiLSTMClassifier\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=best_bilstm_params['embedding_dim'],\n",
        "    hidden_dim=best_bilstm_params['hidden_dim'],\n",
        "    num_layers=best_bilstm_params['num_layers'],\n",
        "    num_classes=1,\n",
        "    dropout=best_bilstm_params['dropout'],\n",
        "    l2_reg=best_bilstm_params['l2_reg'] # Add l2_reg parameter\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(bilstm_model)\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in bilstm_model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "# Always use weighted loss for class imbalance\n",
        "criterion = weighted_criterion\n",
        "optimizer = Adam(bilstm_model.parameters(), lr=best_bilstm_params['learning_rate'], weight_decay=best_bilstm_params['l2_reg']) # Add weight_decay\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "start_time = time.time()\n",
        "bilstm_model, bilstm_training_stats = train_model(\n",
        "    bilstm_model, train_loader, val_loader, criterion, optimizer, num_epochs, device, 'bilstm'\n",
        ")\n",
        "bilstm_training_time = time.time() - start_time\n",
        "print(f\"BiLSTM training completed in {bilstm_training_time:.2f} seconds\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(bilstm_model.state_dict(), os.path.join(model_dir, 'bilstm_model.pt'))\n",
        "print(f\"BiLSTM model saved to {os.path.join(model_dir, 'bilstm_model.pt')}\")\n",
        "\n",
        "# Visualize training progress\n",
        "visualize_training(bilstm_training_stats, 'bilstm')\n",
        "\n",
        "# Evaluate the model\n",
        "bilstm_eval_results = evaluate_model(bilstm_model, test_loader, criterion, device)\n",
        "\n",
        "# Add training time to evaluation results\n",
        "bilstm_eval_results['training_time'] = bilstm_training_time\n",
        "\n",
        "# Visualize evaluation results\n",
        "visualize_evaluation(bilstm_eval_results, 'bilstm')\n",
        "\n",
        "# Save BiLSTM results\n",
        "bilstm_results_df = pd.DataFrame({\n",
        "    'Model': ['BiLSTM'],\n",
        "    'Training Time (s)': [bilstm_eval_results['training_time']],\n",
        "    'Train Accuracy': [bilstm_training_stats['train_acc'][-1]],\n",
        "    'Val Accuracy': [bilstm_training_stats['val_acc'][-1]],\n",
        "    'Test Accuracy': [bilstm_eval_results['test_acc']],\n",
        "    'Train F1': [bilstm_training_stats['train_f1'][-1]],\n",
        "    'Val F1': [bilstm_training_stats['val_f1'][-1]],\n",
        "    'Test F1': [bilstm_eval_results['test_f1']],\n",
        "    'Train MCC': [bilstm_training_stats['train_mcc'][-1]],\n",
        "    'Val MCC': [bilstm_training_stats['val_mcc'][-1]],\n",
        "    'Test MCC': [bilstm_eval_results['test_mcc']],\n",
        "    'Test Precision': [bilstm_eval_results['test_prec']],\n",
        "    'Test Recall': [bilstm_eval_results['test_rec']],\n",
        "    'Test ROC AUC': [bilstm_eval_results['test_roc_auc']],\n",
        "    'Test PR AUC': [bilstm_eval_results['test_pr_auc']]\n",
        "})\n",
        "\n",
        "bilstm_results_df.to_csv(os.path.join(results_dir, 'metrics', 'bilstm_eval_results.csv'), index=False)"
      ],
      "metadata": {
        "id": "VUv4PNPYPEjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14: Combine with baseline model results and compare\n",
        "# Load baseline ML results if available\n",
        "baseline_ml_results_path = os.path.join(results_dir, 'metrics', 'baseline_ml_results_regularized.csv')\n",
        "if os.path.exists(baseline_ml_results_path):\n",
        "    baseline_df = pd.read_csv(baseline_ml_results_path)\n",
        "    print(\"\\nBaseline ML results loaded.\")\n",
        "else:\n",
        "    baseline_df = pd.DataFrame(columns=['Model', 'Test F1', 'Test MCC', 'Test ROC AUC', 'Test PR AUC'])\n",
        "    print(\"\\nNo baseline ML results found.\")\n",
        "\n",
        "# Combine all results\n",
        "dl_results = pd.concat([cnn_results_df, bilstm_results_df])\n",
        "all_models_df = pd.concat([baseline_df, dl_results])\n",
        "\n",
        "# Save combined results\n",
        "all_models_df.to_csv(os.path.join(results_dir, 'metrics', 'all_models_comparison.csv'), index=False)\n",
        "\n",
        "# Sort by MCC\n",
        "print(\"\\nAll Models Comparison:\")\n",
        "print(all_models_df[['Model', 'Test MCC', 'Test F1', 'Test ROC AUC', 'Test PR AUC', 'Training Time (s)']].sort_values('Test MCC', ascending=False))"
      ],
      "metadata": {
        "id": "h2V6ng4KC6OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For CNN (add after cnn_model training):\n",
        "print(\"\\n=== CNN Overfitting Analysis ===\")\n",
        "cnn_overfitting = detect_overfitting_dl(\n",
        "    cnn_model, train_loader, val_loader, test_loader, device, \"CNN\"\n",
        ")\n",
        "\n",
        "# For BiLSTM (add after bilstm_model training):\n",
        "print(\"\\n=== BiLSTM Overfitting Analysis ===\")\n",
        "bilstm_overfitting = detect_overfitting_dl(\n",
        "    bilstm_model, train_loader, val_loader, test_loader, device, \"BiLSTM\"\n",
        ")"
      ],
      "metadata": {
        "id": "KIF-Rcp1mFxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15: Visualize comparison of deep learning with baseline models\n",
        "# Create comparison visualization\n",
        "metrics = ['Test F1', 'Test MCC', 'Test ROC AUC', 'Test PR AUC']\n",
        "melted_df = pd.melt(all_models_df, id_vars=['Model'], value_vars=metrics, var_name='Metric', value_name='Score')\n",
        "\n",
        "# Plot model comparison\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(x='Model', y='Score', hue='Metric', data=melted_df)\n",
        "plt.title('Performance Comparison Across All Models')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(results_dir, 'visualizations', 'all_models_performance_comparison.png'))\n",
        "plt.show()\n",
        "\n",
        "# Plot training time comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Model', y='Training Time (s)', data=all_models_df)\n",
        "plt.title('Training Time Comparison')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(results_dir, 'visualizations', 'all_models_training_time.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YT9ZvgpxPyFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
