{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGL+JWaRG7ERFJqB3Z3gCy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-PhQu3Vl5mF"
      },
      "outputs": [],
      "source": [
        "# SMS Scam Detection - Deep Learning Models\n",
        "# ==========================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim import Adam\n",
        "import optuna\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
        ")\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Set plotting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set up project paths\n",
        "project_dir = '/content/drive/MyDrive/sms-scam-detection'\n",
        "os.chdir(project_dir)\n",
        "\n",
        "data_dir = \"data/processed/\"\n",
        "model_dir = \"models/deep_learning/\"\n",
        "results_dir = \"results/\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(results_dir, \"metrics\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(results_dir, \"visualizations\"), exist_ok=True)\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"Simple tokenizer for text data.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=10000, max_seq_length=100):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
        "        self.word_counts = {}\n",
        "\n",
        "    def fit(self, texts):\n",
        "        \"\"\"Fit the tokenizer to the provided texts.\"\"\"\n",
        "        for text in texts:\n",
        "            for word in text.split():\n",
        "                self.word_counts[word] = self.word_counts.get(word, 0) + 1\n",
        "\n",
        "        sorted_words = sorted(self.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for i, (word, _) in enumerate(sorted_words[:self.vocab_size - 2]):\n",
        "            idx = i + 2\n",
        "            self.word_to_idx[word] = idx\n",
        "            self.idx_to_word[idx] = word\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.word_to_idx)}\")\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        \"\"\"Convert texts to sequences of indices.\"\"\"\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            words = text.split()\n",
        "            sequence = [self.word_to_idx.get(word, 1) for word in words]\n",
        "            sequences.append(sequence)\n",
        "        return sequences\n",
        "\n",
        "    def pad_sequences(self, sequences):\n",
        "        \"\"\"Pad sequences to the maximum sequence length.\"\"\"\n",
        "        padded_sequences = []\n",
        "        for sequence in sequences:\n",
        "            if len(sequence) > self.max_seq_length:\n",
        "                padded_sequence = sequence[:self.max_seq_length]\n",
        "            else:\n",
        "                padded_sequence = sequence + [0] * (self.max_seq_length - len(sequence))\n",
        "            padded_sequences.append(padded_sequence)\n",
        "        return padded_sequences\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save the tokenizer to a JSON file.\"\"\"\n",
        "        tokenizer_data = {\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'max_seq_length': self.max_seq_length,\n",
        "            'word_to_idx': self.word_to_idx,\n",
        "            'idx_to_word': {int(k): v for k, v in self.idx_to_word.items()},\n",
        "            'word_counts': self.word_counts\n",
        "        }\n",
        "\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(tokenizer_data, f)\n",
        "\n",
        "        print(f\"Tokenizer saved to {filepath}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filepath):\n",
        "        \"\"\"Load a tokenizer from a JSON file.\"\"\"\n",
        "        with open(filepath, 'r') as f:\n",
        "            tokenizer_data = json.load(f)\n",
        "\n",
        "        tokenizer = cls(tokenizer_data['vocab_size'], tokenizer_data['max_seq_length'])\n",
        "        tokenizer.word_to_idx = tokenizer_data['word_to_idx']\n",
        "        tokenizer.idx_to_word = {int(k): v for k, v in tokenizer_data['idx_to_word'].items()}\n",
        "        tokenizer.word_counts = tokenizer_data['word_counts']\n",
        "\n",
        "        print(f\"Tokenizer loaded from {filepath}\")\n",
        "        return tokenizer\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"Dataset for text classification.\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    \"\"\"CNN-based text classifier with regularization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, max_seq_length, num_filters=128,\n",
        "                 filter_sizes=(3, 4, 5), num_classes=1, dropout=0.5, l2_reg=0.01):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(dropout * 0.5)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (filter_size, embedding_dim))\n",
        "            for filter_size in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.batch_norms = nn.ModuleList([\n",
        "            nn.BatchNorm2d(num_filters) for _ in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout * 0.7)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        hidden_dim = num_filters * len(filter_sizes) // 2\n",
        "        self.fc1 = nn.Linear(num_filters * len(filter_sizes), hidden_dim)\n",
        "        self.fc1_bn = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.embedding_dropout(x)\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        conv_outputs = []\n",
        "        for conv, bn in zip(self.convs, self.batch_norms):\n",
        "            h = F.relu(bn(conv(x)))\n",
        "            h = F.max_pool2d(h, (h.size(2), 1)).squeeze(3).squeeze(2)\n",
        "            conv_outputs.append(h)\n",
        "\n",
        "        x = torch.cat(conv_outputs, 1)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def l2_penalty(self):\n",
        "        \"\"\"Calculate L2 penalty for regularization.\"\"\"\n",
        "        l2_penalty = 0\n",
        "        for param in self.parameters():\n",
        "            l2_penalty += torch.norm(param, 2) ** 2\n",
        "        return self.l2_reg * l2_penalty\n",
        "\n",
        "class BiLSTMClassifier(nn.Module):\n",
        "    \"\"\"BiLSTM-based text classifier with regularization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128, num_layers=2,\n",
        "                 num_classes=1, dropout=0.5, l2_reg=0.01):\n",
        "        super(BiLSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.l2_reg = l2_reg\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(dropout * 0.3)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=min(num_layers, 2),\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout * 0.7)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        lstm_output_dim = hidden_dim * 2\n",
        "        hidden_fc_dim = hidden_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(lstm_output_dim, hidden_fc_dim)\n",
        "        self.fc1_bn = nn.BatchNorm1d(hidden_fc_dim)\n",
        "        self.fc2 = nn.Linear(hidden_fc_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        mean_pool = torch.mean(output, dim=1)\n",
        "        final_states = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "\n",
        "        x = torch.cat([mean_pool, final_states], dim=1)\n",
        "        x = x[:, :self.hidden_dim * 2]\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def l2_penalty(self):\n",
        "        \"\"\"Calculate L2 penalty for regularization.\"\"\"\n",
        "        l2_penalty = 0\n",
        "        for param in self.parameters():\n",
        "            l2_penalty += torch.norm(param, 2) ** 2\n",
        "        return self.l2_reg * l2_penalty\n",
        "\n",
        "def get_weighted_loss_function(train_df):\n",
        "    \"\"\"Create a weighted BCE loss function based on class imbalance.\"\"\"\n",
        "    n_samples = len(train_df)\n",
        "    n_positive = train_df['label'].sum()\n",
        "    n_negative = n_samples - n_positive\n",
        "\n",
        "    weight_ratio = n_negative / n_positive\n",
        "\n",
        "    print(f\"Class imbalance - Negative: {n_negative}, Positive: {n_positive}, Ratio: {weight_ratio:.2f}\")\n",
        "    print(f\"Setting positive class weight to {weight_ratio:.2f}\")\n",
        "\n",
        "    pos_weight = torch.tensor([weight_ratio]).to(device)\n",
        "    return nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "def create_balanced_sampler(y_train):\n",
        "    \"\"\"Create a weighted random sampler to balance class distribution in batches.\"\"\"\n",
        "    class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
        "    weight = 1. / class_sample_count\n",
        "    samples_weight = np.array([weight[t] for t in y_train])\n",
        "    samples_weight = torch.from_numpy(samples_weight).float()\n",
        "\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    return sampler\n",
        "\n",
        "def objective_cnn(trial, train_dataset, val_dataset, vocab_size, max_seq_length, weighted_criterion, balanced_sampler, max_epochs=8):\n",
        "    \"\"\"Objective function for Optuna CNN optimization.\"\"\"\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
        "    embedding_dim = trial.suggest_categorical('embedding_dim', [50, 100])\n",
        "    num_filters = trial.suggest_categorical('num_filters', [64, 128])\n",
        "    dropout = trial.suggest_float('dropout', 0.3, 0.7)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True)\n",
        "    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-2, log=True)\n",
        "    use_balanced_sampling = trial.suggest_categorical('use_balanced_sampling', [True, False])\n",
        "\n",
        "    if use_balanced_sampling:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=balanced_sampler)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = TextCNN(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        max_seq_length=max_seq_length,\n",
        "        num_filters=num_filters,\n",
        "        dropout=dropout,\n",
        "        l2_reg=l2_reg\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n",
        "\n",
        "    best_val_mcc = -1\n",
        "    patience_counter = 0\n",
        "    patience = 3\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "\n",
        "            loss = weighted_criterion(outputs, labels) + model.l2_penalty()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs).squeeze()\n",
        "                val_preds.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_mcc = matthews_corrcoef(val_labels, val_preds)\n",
        "\n",
        "        if val_mcc > best_val_mcc:\n",
        "            best_val_mcc = val_mcc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "        trial.report(val_mcc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return best_val_mcc\n",
        "\n",
        "def objective_bilstm(trial, train_dataset, val_dataset, vocab_size, weighted_criterion, balanced_sampler, max_epochs=8):\n",
        "    \"\"\"Objective function for Optuna BiLSTM optimization.\"\"\"\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
        "    embedding_dim = trial.suggest_categorical('embedding_dim', [50, 100])\n",
        "    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128])\n",
        "    num_layers = trial.suggest_int('num_layers', 1, 2)\n",
        "    dropout = trial.suggest_float('dropout', 0.3, 0.7)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True)\n",
        "    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-2, log=True)\n",
        "    use_balanced_sampling = trial.suggest_categorical('use_balanced_sampling', [True, False])\n",
        "\n",
        "    if use_balanced_sampling:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=balanced_sampler)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = BiLSTMClassifier(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_classes=1,\n",
        "        dropout=dropout,\n",
        "        l2_reg=l2_reg\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n",
        "\n",
        "    best_val_mcc = -1\n",
        "    patience_counter = 0\n",
        "    patience = 3\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "\n",
        "            loss = weighted_criterion(outputs, labels) + model.l2_penalty()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs).squeeze()\n",
        "                val_preds.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_mcc = matthews_corrcoef(val_labels, val_preds)\n",
        "\n",
        "        if val_mcc > best_val_mcc:\n",
        "            best_val_mcc = val_mcc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "        trial.report(val_mcc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return best_val_mcc\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_name):\n",
        "    \"\"\"Train the model with early stopping and return training statistics.\"\"\"\n",
        "    print(f\"Training {model_name} on {device} with early stopping...\")\n",
        "\n",
        "    training_stats = {\n",
        "        'train_loss': [], 'train_acc': [], 'train_f1': [], 'train_mcc': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_mcc': [],\n",
        "        'val_prec': [], 'val_rec': []\n",
        "    }\n",
        "\n",
        "    best_val_mcc = -1\n",
        "    patience_counter = 0\n",
        "    patience = 3\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            if hasattr(model, 'l2_penalty'):\n",
        "                loss += model.l2_penalty()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            train_preds.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "        train_f1 = f1_score(train_labels, train_preds)\n",
        "        train_mcc = matthews_corrcoef(train_labels, train_preds)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs).squeeze()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                if hasattr(model, 'l2_penalty'):\n",
        "                    loss += model.l2_penalty()\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_preds.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "        val_f1 = f1_score(val_labels, val_preds)\n",
        "        val_mcc = matthews_corrcoef(val_labels, val_preds)\n",
        "        val_prec = precision_score(val_labels, val_preds)\n",
        "        val_rec = recall_score(val_labels, val_preds)\n",
        "\n",
        "        # Update training statistics\n",
        "        training_stats['train_loss'].append(train_loss)\n",
        "        training_stats['train_acc'].append(train_acc)\n",
        "        training_stats['train_f1'].append(train_f1)\n",
        "        training_stats['train_mcc'].append(train_mcc)\n",
        "        training_stats['val_loss'].append(val_loss)\n",
        "        training_stats['val_acc'].append(val_acc)\n",
        "        training_stats['val_f1'].append(val_f1)\n",
        "        training_stats['val_mcc'].append(val_mcc)\n",
        "        training_stats['val_prec'].append(val_prec)\n",
        "        training_stats['val_rec'].append(val_rec)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_mcc > best_val_mcc:\n",
        "            best_val_mcc = val_mcc\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(f\"New best validation MCC: {best_val_mcc:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Train MCC: {train_mcc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val MCC: {val_mcc:.4f}\")\n",
        "        print(f\"Val Precision: {val_prec:.4f}, Val Recall: {val_rec:.4f}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"Loaded best model with validation MCC: {best_val_mcc:.4f}\")\n",
        "\n",
        "    return model, training_stats\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
        "    print(\"Evaluating model on test set...\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_preds = []\n",
        "    test_probs = []\n",
        "    test_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "            preds = (probs > 0.5).astype(int)\n",
        "\n",
        "            test_probs.extend(probs)\n",
        "            test_preds.extend(preds)\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "    test_acc = accuracy_score(test_labels, test_preds)\n",
        "    test_prec = precision_score(test_labels, test_preds)\n",
        "    test_rec = recall_score(test_labels, test_preds)\n",
        "    test_f1 = f1_score(test_labels, test_preds)\n",
        "    test_mcc = matthews_corrcoef(test_labels, test_preds)\n",
        "    test_roc_auc = roc_auc_score(test_labels, test_probs)\n",
        "    test_pr_auc = average_precision_score(test_labels, test_probs)\n",
        "    test_cm = confusion_matrix(test_labels, test_preds)\n",
        "    test_report = classification_report(test_labels, test_preds, output_dict=True)\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Precision: {test_prec:.4f}\")\n",
        "    print(f\"Test Recall: {test_rec:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "    print(f\"Test MCC: {test_mcc:.4f}\")\n",
        "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
        "    print(f\"Test PR AUC: {test_pr_auc:.4f}\")\n",
        "    print(f\"Test Confusion Matrix:\\n{test_cm}\")\n",
        "    print(f\"Test Classification Report:\\n{classification_report(test_labels, test_preds)}\")\n",
        "\n",
        "    return {\n",
        "        'test_loss': test_loss,\n",
        "        'test_acc': test_acc,\n",
        "        'test_prec': test_prec,\n",
        "        'test_rec': test_rec,\n",
        "        'test_f1': test_f1,\n",
        "        'test_mcc': test_mcc,\n",
        "        'test_roc_auc': test_roc_auc,\n",
        "        'test_pr_auc': test_pr_auc,\n",
        "        'test_cm': test_cm,\n",
        "        'test_report': test_report,\n",
        "        'test_probs': test_probs,\n",
        "        'test_preds': test_preds,\n",
        "        'test_labels': test_labels\n",
        "    }\n",
        "\n",
        "def visualize_training(training_stats, model_name):\n",
        "    \"\"\"Visualize training progress.\"\"\"\n",
        "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 6))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(training_stats['train_loss'], label='Train Loss')\n",
        "    ax1.plot(training_stats['val_loss'], label='Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax2.plot(training_stats['train_acc'], label='Train Accuracy')\n",
        "    ax2.plot(training_stats['val_acc'], label='Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Score')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot F1 score\n",
        "    ax3.plot(training_stats['train_f1'], label='Train F1')\n",
        "    ax3.plot(training_stats['val_f1'], label='Validation F1')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_ylabel('Score')\n",
        "    ax3.set_title('Training and Validation F1 Score')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "    # Plot MCC\n",
        "    ax4.plot(training_stats['train_mcc'], label='Train MCC')\n",
        "    ax4.plot(training_stats['val_mcc'], label='Validation MCC')\n",
        "    ax4.set_xlabel('Epoch')\n",
        "    ax4.set_ylabel('Score')\n",
        "    ax4.set_title('Training and Validation MCC')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'visualizations', f\"{model_name}_training_progress.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Plot precision and recall\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_stats['val_prec'], label='Validation Precision')\n",
        "    plt.plot(training_stats['val_rec'], label='Validation Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Validation Precision and Recall')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'visualizations', f\"{model_name}_precision_recall.png\"))\n",
        "    plt.show()\n",
        "\n",
        "def visualize_evaluation(eval_results, model_name):\n",
        "    \"\"\"Visualize evaluation results.\"\"\"\n",
        "    # Confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        eval_results['test_cm'],\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=['Legitimate', 'Scam'],\n",
        "        yticklabels=['Legitimate', 'Scam']\n",
        "    )\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'visualizations', f\"{model_name}_confusion_matrix.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # ROC curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    fpr, tpr, _ = roc_curve(eval_results['test_labels'], eval_results['test_probs'])\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {eval_results[\"test_roc_auc\"]:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{model_name} - ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'visualizations', f\"{model_name}_roc_curve.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Precision-Recall curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    precision, recall, _ = precision_recall_curve(eval_results['test_labels'], eval_results['test_probs'])\n",
        "    plt.step(recall, precision, where='post', lw=2, label=f'PR curve (AP = {eval_results[\"test_pr_auc\"]:.4f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'{model_name} - Precision-Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'visualizations', f\"{model_name}_pr_curve.png\"))\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    # Load data\n",
        "    train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
        "    val_df = pd.read_csv(os.path.join(data_dir, \"val.csv\"))\n",
        "    test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
        "\n",
        "    print(f\"Loaded data: Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "    # Print class distribution\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    for name, df in [(\"Training\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]:\n",
        "        print(f\"{name} Set:\")\n",
        "        print(df['label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "    # Calculate imbalance ratio\n",
        "    train_neg_count = (train_df['label'] == 0).sum()\n",
        "    train_pos_count = (train_df['label'] == 1).sum()\n",
        "    imbalance_ratio = train_neg_count / train_pos_count\n",
        "    print(f\"\\nImbalance ratio (negative:positive): {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "    # Ensure cleaned_text column exists\n",
        "    if 'cleaned_text' not in train_df.columns:\n",
        "        print(\"Adding cleaned_text column...\")\n",
        "        def clean_text(text):\n",
        "            if not isinstance(text, str):\n",
        "                return \"\"\n",
        "            text = text.lower()\n",
        "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "            text = re.sub(r'\\S+@\\S+', '', text)\n",
        "            text = re.sub(r'\\b\\d{10,}\\b', '', text)\n",
        "            text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "            return text\n",
        "\n",
        "        train_df['cleaned_text'] = train_df['message'].apply(clean_text)\n",
        "        val_df['cleaned_text'] = val_df['message'].apply(clean_text)\n",
        "        test_df['cleaned_text'] = test_df['message'].apply(clean_text)\n",
        "\n",
        "    # Prepare data for deep learning models\n",
        "    vocab_size = 10000\n",
        "    max_seq_length = 100\n",
        "    embedding_dim = 100\n",
        "\n",
        "    # Create and fit tokenizer\n",
        "    tokenizer = Tokenizer(vocab_size=vocab_size, max_seq_length=max_seq_length)\n",
        "    tokenizer.fit(train_df['cleaned_text'].tolist())\n",
        "\n",
        "    # Save tokenizer\n",
        "    tokenizer_path = os.path.join(model_dir, 'tokenizer.json')\n",
        "    tokenizer.save(tokenizer_path)\n",
        "\n",
        "    # Tokenize and pad sequences\n",
        "    X_train_seq = tokenizer.pad_sequences(tokenizer.texts_to_sequences(train_df['cleaned_text'].tolist()))\n",
        "    X_val_seq = tokenizer.pad_sequences(tokenizer.texts_to_sequences(val_df['cleaned_text'].tolist()))\n",
        "    X_test_seq = tokenizer.pad_sequences(tokenizer.texts_to_sequences(test_df['cleaned_text'].tolist()))\n",
        "\n",
        "    # Get labels\n",
        "    y_train = train_df['label'].values\n",
        "    y_val = val_df['label'].values\n",
        "    y_test = test_df['label'].values\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(X_train_seq, y_train)\n",
        "    val_dataset = TextDataset(X_val_seq, y_val)\n",
        "    test_dataset = TextDataset(X_test_seq, y_test)\n",
        "\n",
        "    print(f\"Prepared datasets - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Class imbalance handling\n",
        "    print(\"\\n===== Class Imbalance Handling =====\")\n",
        "    weighted_criterion = get_weighted_loss_function(train_df)\n",
        "    balanced_sampler = create_balanced_sampler(y_train)\n",
        "\n",
        "    # Hyperparameter optimization\n",
        "    print(\"Optimizing CNN hyperparameters with Optuna...\")\n",
        "    study_cnn = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
        "    study_cnn.optimize(\n",
        "        lambda trial: objective_cnn(trial, train_dataset, val_dataset, vocab_size, max_seq_length, weighted_criterion, balanced_sampler),\n",
        "        n_trials=10\n",
        "    )\n",
        "\n",
        "    print(\"Best CNN parameters:\", study_cnn.best_params)\n",
        "    print(\"Best CNN validation MCC score:\", study_cnn.best_value)\n",
        "\n",
        "    print(\"Optimizing BiLSTM hyperparameters with Optuna...\")\n",
        "    study_bilstm = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
        "    study_bilstm.optimize(\n",
        "        lambda trial: objective_bilstm(trial, train_dataset, val_dataset, vocab_size, weighted_criterion, balanced_sampler),\n",
        "        n_trials=10\n",
        "    )\n",
        "\n",
        "    print(\"Best BiLSTM parameters:\", study_bilstm.best_params)\n",
        "    print(\"Best BiLSTM validation MCC score:\", study_bilstm.best_value)\n",
        "\n",
        "    # Save studies\n",
        "    optuna_dir = os.path.join(results_dir, 'optuna_results')\n",
        "    os.makedirs(optuna_dir, exist_ok=True)\n",
        "    joblib.dump(study_cnn, os.path.join(optuna_dir, 'cnn_study.pkl'))\n",
        "    joblib.dump(study_bilstm, os.path.join(optuna_dir, 'bilstm_study.pkl'))\n",
        "\n",
        "    # Train and evaluate CNN with best parameters\n",
        "    best_cnn_params = study_cnn.best_params\n",
        "    batch_size = best_cnn_params['batch_size']\n",
        "\n",
        "    if best_cnn_params.get('use_balanced_sampling', False):\n",
        "        print(\"Using balanced sampling for CNN training...\")\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=balanced_sampler)\n",
        "    else:\n",
        "        print(\"Using normal sampling for CNN training...\")\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    cnn_model = TextCNN(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=best_cnn_params['embedding_dim'],\n",
        "        max_seq_length=max_seq_length,\n",
        "        num_filters=best_cnn_params['num_filters'],\n",
        "        filter_sizes=(3, 4, 5),\n",
        "        num_classes=1,\n",
        "        dropout=best_cnn_params['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    print(cnn_model)\n",
        "    print(f\"Number of parameters: {sum(p.numel() for p in cnn_model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    criterion = weighted_criterion\n",
        "    optimizer = Adam(cnn_model.parameters(), lr=best_cnn_params['learning_rate'])\n",
        "\n",
        "    num_epochs = 10\n",
        "    start_time = time.time()\n",
        "    cnn_model, cnn_training_stats = train_model(\n",
        "        cnn_model, train_loader, val_loader, criterion, optimizer, num_epochs, device, 'cnn'\n",
        "    )\n",
        "    cnn_training_time = time.time() - start_time\n",
        "    print(f\"CNN training completed in {cnn_training_time:.2f} seconds\")\n",
        "\n",
        "    torch.save(cnn_model.state_dict(), os.path.join(model_dir, 'cnn_model.pt'))\n",
        "    print(f\"CNN model saved to {os.path.join(model_dir, 'cnn_model.pt')}\")\n",
        "\n",
        "    visualize_training(cnn_training_stats, 'cnn')\n",
        "    cnn_eval_results = evaluate_model(cnn_model, test_loader, criterion, device)\n",
        "    cnn_eval_results['training_time'] = cnn_training_time\n",
        "    visualize_evaluation(cnn_eval_results, 'cnn')\n",
        "\n",
        "    # Train and evaluate BiLSTM with best parameters\n",
        "    best_bilstm_params = study_bilstm.best_params\n",
        "    batch_size = best_bilstm_params['batch_size']\n",
        "\n",
        "    if best_bilstm_params.get('use_balanced_sampling', False):\n",
        "        print(\"Using balanced sampling for BiLSTM training...\")\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=balanced_sampler)\n",
        "    else:\n",
        "        print(\"Using normal sampling for BiLSTM training...\")\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    bilstm_model = BiLSTMClassifier(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=best_bilstm_params['embedding_dim'],\n",
        "        hidden_dim=best_bilstm_params['hidden_dim'],\n",
        "        num_layers=best_bilstm_params['num_layers'],\n",
        "        num_classes=1,\n",
        "        dropout=best_bilstm_params['dropout'],\n",
        "        l2_reg=best_bilstm_params['l2_reg']\n",
        "    ).to(device)\n",
        "\n",
        "    print(bilstm_model)\n",
        "    print(f\"Number of parameters: {sum(p.numel() for p in bilstm_model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    criterion = weighted_criterion\n",
        "    optimizer = Adam(bilstm_model.parameters(), lr=best_bilstm_params['learning_rate'], weight_decay=best_bilstm_params['l2_reg'])\n",
        "\n",
        "    num_epochs = 10\n",
        "    start_time = time.time()\n",
        "    bilstm_model, bilstm_training_stats = train_model(\n",
        "        bilstm_model, train_loader, val_loader, criterion, optimizer, num_epochs, device, 'bilstm'\n",
        "    )\n",
        "    bilstm_training_time = time.time() - start_time\n",
        "    print(f\"BiLSTM training completed in {bilstm_training_time:.2f} seconds\")\n",
        "\n",
        "    torch.save(bilstm_model.state_dict(), os.path.join(model_dir, 'bilstm_model.pt'))\n",
        "    print(f\"BiLSTM model saved to {os.path.join(model_dir, 'bilstm_model.pt')}\")\n",
        "\n",
        "    visualize_training(bilstm_training_stats, 'bilstm')\n",
        "    bilstm_eval_results = evaluate_model(bilstm_model, test_loader, criterion, device)\n",
        "    bilstm_eval_results['training_time'] = bilstm_training_time\n",
        "    visualize_evaluation(bilstm_eval_results, 'bilstm')\n",
        "\n",
        "    # Save results\n",
        "    cnn_results_df = pd.DataFrame({\n",
        "        'Model': ['TextCNN'],\n",
        "        'Training Time (s)': [cnn_eval_results['training_time']],\n",
        "        'Train Accuracy': [cnn_training_stats['train_acc'][-1]],\n",
        "        'Val Accuracy': [cnn_training_stats['val_acc'][-1]],\n",
        "        'Test Accuracy': [cnn_eval_results['test_acc']],\n",
        "        'Train F1': [cnn_training_stats['train_f1'][-1]],\n",
        "        'Val F1': [cnn_training_stats['val_f1'][-1]],\n",
        "        'Test F1': [cnn_eval_results['test_f1']],\n",
        "        'Train MCC': [cnn_training_stats['train_mcc'][-1]],\n",
        "        'Val MCC': [cnn_training_stats['val_mcc'][-1]],\n",
        "        'Test MCC': [cnn_eval_results['test_mcc']],\n",
        "        'Test Precision': [cnn_eval_results['test_prec']],\n",
        "        'Test Recall': [cnn_eval_results['test_rec']],\n",
        "        'Test ROC AUC': [cnn_eval_results['test_roc_auc']],\n",
        "        'Test PR AUC': [cnn_eval_results['test_pr_auc']]\n",
        "    })\n",
        "\n",
        "    bilstm_results_df = pd.DataFrame({\n",
        "        'Model': ['BiLSTM'],\n",
        "        'Training Time (s)': [bilstm_eval_results['training_time']],\n",
        "        'Train Accuracy': [bilstm_training_stats['train_acc'][-1]],\n",
        "        'Val Accuracy': [bilstm_training_stats['val_acc'][-1]],\n",
        "        'Test Accuracy': [bilstm_eval_results['test_acc']],\n",
        "        'Train F1': [bilstm_training_stats['train_f1'][-1]],\n",
        "        'Val F1': [bilstm_training_stats['val_f1'][-1]],\n",
        "        'Test F1': [bilstm_eval_results['test_f1']],\n",
        "        'Train MCC': [bilstm_training_stats['train_mcc'][-1]],\n",
        "        'Val MCC': [bilstm_training_stats['val_mcc'][-1]],\n",
        "        'Test MCC': [bilstm_eval_results['test_mcc']],\n",
        "        'Test Precision': [bilstm_eval_results['test_prec']],\n",
        "        'Test Recall': [bilstm_eval_results['test_rec']],\n",
        "        'Test ROC AUC': [bilstm_eval_results['test_roc_auc']],\n",
        "        'Test PR AUC': [bilstm_eval_results['test_pr_auc']]\n",
        "    })\n",
        "\n",
        "    # Combine results\n",
        "    dl_results = pd.concat([cnn_results_df, bilstm_results_df])\n",
        "\n",
        "    # Load baseline ML results if available\n",
        "    baseline_ml_results_path = os.path.join(results_dir, 'metrics', 'baseline_ml_results.csv')\n",
        "    if os.path.exists(baseline_ml_results_path):\n",
        "        baseline_df = pd.read_csv(baseline_ml_results_path)\n",
        "        print(\"\\nBaseline ML results loaded.\")\n",
        "        all_models_df = pd.concat([baseline_df, dl_results])\n",
        "    else:\n",
        "        baseline_df = pd.DataFrame()\n",
        "        print(\"\\nNo baseline ML results found.\")\n",
        "        all_models_df = dl_results\n",
        "\n",
        "    # Save combined results\n",
        "    all_models_df.to_csv(os.path.join(results_dir, 'metrics', 'all_models_comparison.csv'), index=False)\n",
        "\n",
        "    print(\"\\nAll Models Comparison:\")\n",
        "    print(all_models_df[['Model', 'Test MCC', 'Test F1', 'Test ROC AUC', 'Test PR AUC', 'Training Time (s)']].sort_values('Test MCC', ascending=False))\n",
        "\n",
        "    # Visualize comparison\n",
        "    if len(all_models_df) > 1:\n",
        "        metrics = ['Test F1', 'Test MCC', 'Test ROC AUC', 'Test PR AUC']\n",
        "        melted_df = pd.melt(all_models_df, id_vars=['Model'], value_vars=metrics, var_name='Metric', value_name='Score')\n",
        "\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        sns.barplot(x='Model', y='Score', hue='Metric', data=melted_df)\n",
        "        plt.title('Performance Comparison Across All Models')\n",
        "        plt.ylim(0, 1)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(results_dir, 'visualizations', 'all_models_performance_comparison.png'))\n",
        "        plt.show()\n",
        "\n",
        "        # Training time comparison\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.barplot(x='Model', y='Training Time (s)', data=all_models_df)\n",
        "        plt.title('Training Time Comparison')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(results_dir, 'visualizations', 'all_models_training_time.png'))\n",
        "        plt.show()\n",
        "\n",
        "    # Test example predictions\n",
        "    example_messages = [\n",
        "        \"Congratulations! You've won a ugx100000 gift card. Click here to claim: www.example.com\",\n",
        "        \"Your account has been suspended. Please verify your identity by sending your PIN to this number.\",\n",
        "        \"Hi, just checking if we're still meeting for lunch tomorrow at 12?\",\n",
        "        \"URGENT: Your payment of ugx55000 has been processed. If this was not you, call immediately: 1-800-555-1234\",\n",
        "        \"Your package will be delivered tomorrow between 10am and 2pm. No signature required.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== Testing Models on Example Messages ===\")\n",
        "\n",
        "    for model_name, model in [('TextCNN', cnn_model), ('BiLSTM', bilstm_model)]:\n",
        "        print(f\"\\n=== {model_name} Predictions ===\")\n",
        "        model.eval()\n",
        "\n",
        "        for i, text in enumerate(example_messages):\n",
        "            # Preprocess text\n",
        "            cleaned_text = re.sub(r'https?://\\S+|www\\.\\S+', '', text.lower())\n",
        "            cleaned_text = re.sub(r'\\S+@\\S+', '', cleaned_text)\n",
        "            cleaned_text = re.sub(r'\\b\\d{10,}\\b', '', cleaned_text)\n",
        "            cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', cleaned_text)\n",
        "            cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "            # Tokenize and pad\n",
        "            sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
        "            padded_sequence = tokenizer.pad_sequences(sequence)\n",
        "\n",
        "            # Convert to tensor\n",
        "            input_tensor = torch.tensor(padded_sequence, dtype=torch.long).to(device)\n",
        "\n",
        "            # Make prediction\n",
        "            with torch.no_grad():\n",
        "                output = model(input_tensor).squeeze()\n",
        "                prob = torch.sigmoid(output).item()\n",
        "                prediction = \"Scam\" if prob > 0.5 else \"Legitimate\"\n",
        "\n",
        "            print(f\"\\nExample {i+1}: {text}\")\n",
        "            print(f\"Prediction: {prediction} (Confidence: {prob:.4f})\")\n",
        "\n",
        "    print(\"\\nDeep learning models training and evaluation completed successfully!\")\n",
        "    print(f\"Results saved to: {results_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}