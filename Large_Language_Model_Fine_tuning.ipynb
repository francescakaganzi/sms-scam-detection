{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPhSW4lMGEjSE8Kva80ApS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsdkuhfhqs7e"
      },
      "outputs": [],
      "source": [
        "# SMS Scam Detection - Large Language Model Fine-tuning\n",
        "# =====================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    get_peft_model,\n",
        ")\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
        ")\n",
        "import evaluate\n",
        "import warnings\n",
        "import optuna\n",
        "from pathlib import Path\n",
        "\n",
        "# Set plotting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING for better error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Ignore specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n",
        "\n",
        "# Set up project paths\n",
        "project_dir = '/content/drive/MyDrive/sms-scam-detection'\n",
        "os.chdir(project_dir)\n",
        "\n",
        "data_dir = \"data/processed/\"\n",
        "model_dir = \"models/llm/\"\n",
        "results_dir = \"results/\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(results_dir, \"metrics\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(results_dir, \"visualizations\"), exist_ok=True)\n",
        "\n",
        "# Setup label mapping\n",
        "id2label = {0: \"Legitimate\", 1: \"Scam\"}\n",
        "label2id = {\"Legitimate\": 0, \"Scam\": 1}\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load and prepare the dataset.\"\"\"\n",
        "    train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
        "    val_df = pd.read_csv(os.path.join(data_dir, \"val.csv\"))\n",
        "    test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
        "\n",
        "    print(f\"Loaded data: Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "    # Print class distribution\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    for name, df in [(\"Training\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]:\n",
        "        print(f\"{name} Set:\")\n",
        "        print(df['label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "    # Calculate imbalance ratio\n",
        "    train_neg_count = (train_df['label'] == 0).sum()\n",
        "    train_pos_count = (train_df['label'] == 1).sum()\n",
        "    imbalance_ratio = train_neg_count / train_pos_count if train_pos_count > 0 else float('inf')\n",
        "    print(f\"\\nImbalance ratio (legitimate:scam): {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "    # Convert to Hugging Face datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    dataset_dict = DatasetDict({\n",
        "        \"train\": train_dataset,\n",
        "        \"valid\": val_dataset,\n",
        "        \"test\": test_dataset\n",
        "    })\n",
        "\n",
        "    print(\"\\nDataset structure:\")\n",
        "    print(dataset_dict)\n",
        "\n",
        "    return dataset_dict\n",
        "\n",
        "def preprocess_function(examples, tokenizer, max_length=128):\n",
        "    \"\"\"Tokenize and preprocess text examples.\"\"\"\n",
        "    text_column = \"cleaned_text\" if \"cleaned_text\" in examples else \"message\"\n",
        "    return tokenizer(\n",
        "        examples[text_column],\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "def tokenize_data(dataset_dict, tokenizer, max_length=128):\n",
        "    \"\"\"Tokenize all splits in a dataset dictionary.\"\"\"\n",
        "    return dataset_dict.map(\n",
        "        lambda examples: preprocess_function(examples, tokenizer, max_length),\n",
        "        batched=True\n",
        "    )\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics for model predictions.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "    positive_class_probs = probabilities[:, 1]\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predicted_classes)\n",
        "    precision = precision_score(labels, predicted_classes)\n",
        "    recall = recall_score(labels, predicted_classes)\n",
        "    f1 = f1_score(labels, predicted_classes)\n",
        "    mcc = matthews_corrcoef(labels, predicted_classes)\n",
        "    roc_auc = roc_auc_score(labels, positive_class_probs)\n",
        "    pr_auc = average_precision_score(labels, positive_class_probs)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"mcc\": mcc,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"pr_auc\": pr_auc\n",
        "    }\n",
        "\n",
        "def create_data_collator(tokenizer):\n",
        "    \"\"\"Create a data collator for batching examples.\"\"\"\n",
        "    return DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def visualize_training_metrics(metrics, model_name):\n",
        "    \"\"\"Visualize training and evaluation metrics.\"\"\"\n",
        "    results_dir = Path(f\"results/visualizations/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_loss = [x['loss'] for x in metrics if 'loss' in x]\n",
        "    eval_loss = [x['eval_loss'] for x in metrics if 'eval_loss' in x]\n",
        "\n",
        "    if train_loss and eval_loss:\n",
        "        min_length = min(len(train_loss), len(eval_loss))\n",
        "        epochs = range(1, min_length + 1)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(epochs, train_loss[:min_length], label='Training Loss', marker='o')\n",
        "        plt.plot(epochs, eval_loss[:min_length], label='Validation Loss', marker='o')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss vs Validation Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(results_dir / \"loss_curve.png\")\n",
        "        plt.show()\n",
        "\n",
        "    # Extract evaluation metrics\n",
        "    if metrics and 'eval_accuracy' in metrics[0]:\n",
        "        eval_accuracy = [x.get('eval_accuracy', None) for x in metrics if 'eval_loss' in x]\n",
        "        eval_f1 = [x.get('eval_f1', None) for x in metrics if 'eval_loss' in x]\n",
        "        eval_mcc = [x.get('eval_mcc', None) for x in metrics if 'eval_loss' in x]\n",
        "\n",
        "        eval_accuracy = [x for x in eval_accuracy if x is not None]\n",
        "        eval_f1 = [x for x in eval_f1 if x is not None]\n",
        "        eval_mcc = [x for x in eval_mcc if x is not None]\n",
        "\n",
        "        if eval_accuracy:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(range(1, len(eval_accuracy) + 1), eval_accuracy, label='Accuracy', marker='o', color='blue')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Score')\n",
        "            plt.title('Validation Accuracy')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(results_dir / \"accuracy_curve.png\")\n",
        "            plt.show()\n",
        "\n",
        "        if eval_f1:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(range(1, len(eval_f1) + 1), eval_f1, label='F1 Score', marker='o', color='green')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Score')\n",
        "            plt.title('Validation F1 Score')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(results_dir / \"f1_curve.png\")\n",
        "            plt.show()\n",
        "\n",
        "        if eval_mcc:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(range(1, len(eval_mcc) + 1), eval_mcc, label='MCC', marker='o', color='purple')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Score')\n",
        "            plt.title('Validation MCC')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(results_dir / \"mcc_curve.png\")\n",
        "            plt.show()\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, model_name):\n",
        "    \"\"\"Plot confusion matrix.\"\"\"\n",
        "    results_dir = Path(f\"results/visualizations/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(results_dir / \"confusion_matrix.png\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curve(fpr, tpr, roc_auc, model_name):\n",
        "    \"\"\"Plot ROC curve.\"\"\"\n",
        "    results_dir = Path(f\"results/visualizations/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(results_dir / \"roc_curve.png\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_pr_curve(precision, recall, pr_auc, model_name):\n",
        "    \"\"\"Plot precision-recall curve.\"\"\"\n",
        "    results_dir = Path(f\"results/visualizations/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.step(recall, precision, color='green', lw=2, where='post',\n",
        "             label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
        "    plt.fill_between(recall, precision, alpha=0.2, color='green', step='post')\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(results_dir / \"pr_curve.png\")\n",
        "    plt.show()\n",
        "\n",
        "def train_model(model, tokenizer, tokenized_data, training_args, model_name):\n",
        "    \"\"\"Train a model and evaluate it on validation data.\"\"\"\n",
        "    print(f\"\\n=== Training {model_name} ===\")\n",
        "\n",
        "    results_dir = Path(f\"results/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_data[\"train\"],\n",
        "        eval_dataset=tokenized_data[\"valid\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=create_data_collator(tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_result = trainer.train()\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    training_time_minutes = round(training_time / 60, 2)\n",
        "\n",
        "    print(f\"\\nTraining completed in {training_time_minutes} minutes\")\n",
        "    print(f\"Training Loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"\\nValidation Results:\")\n",
        "    for key, value in eval_results.items():\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "    model_save_path = results_dir / \"final_model\"\n",
        "    trainer.save_model(str(model_save_path))\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    visualize_training_metrics(trainer.state.log_history, model_name)\n",
        "\n",
        "    metrics_file = results_dir / \"training_metrics.json\"\n",
        "    with open(metrics_file, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"training_time_seconds\": training_time,\n",
        "            \"training_time_minutes\": training_time_minutes,\n",
        "            \"final_train_loss\": train_result.training_loss,\n",
        "            \"eval_results\": eval_results\n",
        "        }, f, indent=4)\n",
        "\n",
        "    return trainer\n",
        "\n",
        "def evaluate_model(trainer, tokenized_test_data, model_name):\n",
        "    \"\"\"Evaluate a trained model on test data.\"\"\"\n",
        "    print(f\"\\n=== Evaluating {model_name} on Test Set ===\")\n",
        "\n",
        "    test_results = trainer.predict(tokenized_test_data)\n",
        "\n",
        "    predictions = test_results.predictions\n",
        "    labels = test_results.label_ids\n",
        "\n",
        "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "    positive_class_probs = probabilities[:, 1]\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predicted_classes)\n",
        "    precision = precision_score(labels, predicted_classes)\n",
        "    recall = recall_score(labels, predicted_classes)\n",
        "    f1 = f1_score(labels, predicted_classes)\n",
        "    mcc = matthews_corrcoef(labels, predicted_classes)\n",
        "    roc_auc = roc_auc_score(labels, positive_class_probs)\n",
        "    pr_auc = average_precision_score(labels, positive_class_probs)\n",
        "    cm = confusion_matrix(labels, predicted_classes)\n",
        "\n",
        "    print(\"\\nTest Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    class_names = list(id2label.values())\n",
        "    report = classification_report(labels, predicted_classes, target_names=class_names)\n",
        "    print(report)\n",
        "\n",
        "    results_dir = Path(f\"results/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Model': [model_name],\n",
        "        'Accuracy': [accuracy],\n",
        "        'Precision': [precision],\n",
        "        'Recall': [recall],\n",
        "        'F1': [f1],\n",
        "        'MCC': [mcc],\n",
        "        'ROC_AUC': [roc_auc],\n",
        "        'PR_AUC': [pr_auc]\n",
        "    })\n",
        "    metrics_df.to_csv(results_dir / \"test_metrics.csv\", index=False)\n",
        "\n",
        "    with open(results_dir / \"classification_report.txt\", \"w\") as f:\n",
        "        f.write(report)\n",
        "\n",
        "    plot_confusion_matrix(cm, class_names, model_name)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(labels, positive_class_probs)\n",
        "    plot_roc_curve(fpr, tpr, roc_auc, model_name)\n",
        "\n",
        "    precision_points, recall_points, _ = precision_recall_curve(labels, positive_class_probs)\n",
        "    plot_pr_curve(precision_points, recall_points, pr_auc, model_name)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"mcc\": mcc,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"pr_auc\": pr_auc,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"predicted_classes\": predicted_classes,\n",
        "        \"probabilities\": probabilities\n",
        "    }\n",
        "\n",
        "def predict_examples(model, tokenizer, examples, model_name):\n",
        "    \"\"\"Predict on a list of example messages.\"\"\"\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    for text in examples:\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "        prob_score = probs[0][predicted_class].item()\n",
        "\n",
        "        results.append({\n",
        "            \"text\": text,\n",
        "            \"predicted_class\": id2label[predicted_class],\n",
        "            \"confidence\": prob_score\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    print(f\"\\n=== Example Predictions with {model_name} ===\")\n",
        "    for i, row in results_df.iterrows():\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Text: {row['text']}\")\n",
        "        print(f\"Prediction: {row['predicted_class']}\")\n",
        "        print(f\"Confidence: {row['confidence']:.4f}\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    # Load data\n",
        "    dataset_dict = load_data()\n",
        "\n",
        "    # Define model families\n",
        "    model_families = {\n",
        "        \"roberta\": {\n",
        "            \"model_name\": \"roberta-base\",\n",
        "            \"tokenizer\": AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "        },\n",
        "        \"distilbert\": {\n",
        "            \"model_name\": \"distilbert-base-uncased\",\n",
        "            \"tokenizer\": AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Tokenize datasets for each model family\n",
        "    tokenized_datasets = {}\n",
        "    for family, config in model_families.items():\n",
        "        print(f\"\\nTokenizing datasets for {family}...\")\n",
        "        tokenized_datasets[family] = tokenize_data(dataset_dict, config[\"tokenizer\"], max_length=128)\n",
        "\n",
        "    # Define fine-tuning techniques\n",
        "    fine_tuning_techniques = [\n",
        "        \"roberta_frozen\",\n",
        "        \"roberta_full\",\n",
        "        \"roberta_lora\",\n",
        "        \"distilbert_frozen\",\n",
        "        \"distilbert_full\",\n",
        "        \"distilbert_lora\"\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    # Process each fine-tuning technique\n",
        "    for technique in fine_tuning_techniques:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing {technique}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        family, approach = technique.split(\"_\")\n",
        "\n",
        "        model_name = model_families[family][\"model_name\"]\n",
        "        tokenizer = model_families[family][\"tokenizer\"]\n",
        "        tokenized_data = tokenized_datasets[family]\n",
        "\n",
        "        # Load base model\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2,\n",
        "            id2label=id2label,\n",
        "            label2id=label2id\n",
        "        ).to(device)\n",
        "\n",
        "        # Apply fine-tuning technique\n",
        "        if approach == \"frozen\":\n",
        "            print(\"Freezing base model parameters...\")\n",
        "            for name, param in model.named_parameters():\n",
        "                if \"classifier\" not in name:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        elif approach == \"lora\":\n",
        "            print(\"Applying LoRA configuration...\")\n",
        "            peft_config = LoraConfig(\n",
        "                task_type=TaskType.SEQ_CLS,\n",
        "                r=16,\n",
        "                lora_alpha=32,\n",
        "                lora_dropout=0.1,\n",
        "                target_modules=[\"query\", \"value\"] if family == \"roberta\" else [\"q_lin\", \"v_lin\"]\n",
        "            )\n",
        "            model = get_peft_model(model, peft_config)\n",
        "            model.print_trainable_parameters()\n",
        "\n",
        "        # Full fine-tuning doesn't require any parameter modifications\n",
        "\n",
        "        # Define training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f\"results/{technique}/checkpoints\",\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            warmup_steps=100,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir=f\"results/{technique}/logs\",\n",
        "            logging_steps=50,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            save_total_limit=1,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"mcc\",\n",
        "            greater_is_better=True,\n",
        "            push_to_hub=False,\n",
        "            dataloader_pin_memory=False,\n",
        "            remove_unused_columns=False\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        trainer = train_model(model, tokenizer, tokenized_data, training_args, technique)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_results = evaluate_model(trainer, tokenized_data[\"test\"], technique)\n",
        "\n",
        "        # Store results\n",
        "        all_results[technique] = test_results\n",
        "\n",
        "        # Clean up GPU memory\n",
        "        del model\n",
        "        del trainer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Compile comparison results\n",
        "    comparison_results = []\n",
        "    for technique, results in all_results.items():\n",
        "        comparison_results.append({\n",
        "            \"Model\": technique,\n",
        "            \"Accuracy\": results[\"accuracy\"],\n",
        "            \"Precision\": results[\"precision\"],\n",
        "            \"Recall\": results[\"recall\"],\n",
        "            \"F1\": results[\"f1\"],\n",
        "            \"MCC\": results[\"mcc\"],\n",
        "            \"ROC_AUC\": results[\"roc_auc\"],\n",
        "            \"PR_AUC\": results[\"pr_auc\"]\n",
        "        })\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_results)\n",
        "    comparison_df = comparison_df.sort_values(\"MCC\", ascending=False)\n",
        "\n",
        "    # Save comparison results\n",
        "    comparison_df.to_csv(\"results/metrics/llm_comparison_results.csv\", index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL COMPARISON OF ALL LLM APPROACHES\")\n",
        "    print(\"=\"*60)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "\n",
        "    # Visualize comparison\n",
        "    metrics_to_plot = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"MCC\", \"ROC_AUC\", \"PR_AUC\"]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        sns.barplot(data=comparison_df, x=\"Model\", y=metric, ax=axes[i])\n",
        "        axes[i].set_title(f\"{metric} Comparison\")\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "    # Hide the last subplot since we have 7 metrics and 8 subplots\n",
        "    axes[-1].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"results/visualizations/llm_comparison_all_metrics.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Focus on key metrics\n",
        "    key_metrics = [\"F1\", \"MCC\", \"ROC_AUC\", \"PR_AUC\"]\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    melted_df = comparison_df.melt(\n",
        "        id_vars=[\"Model\"],\n",
        "        value_vars=key_metrics,\n",
        "        var_name=\"Metric\",\n",
        "        value_name=\"Score\"\n",
        "    )\n",
        "\n",
        "    sns.barplot(data=melted_df, x=\"Model\", y=\"Score\", hue=\"Metric\")\n",
        "    plt.title(\"Key Metrics Comparison Across LLM Approaches\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"results/visualizations/llm_key_metrics_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Best model analysis\n",
        "    best_model = comparison_df.iloc[0]\n",
        "    print(f\"\\nBEST PERFORMING MODEL: {best_model['Model']}\")\n",
        "    print(f\"   MCC: {best_model['MCC']:.4f}\")\n",
        "    print(f\"   F1 Score: {best_model['F1']:.4f}\")\n",
        "    print(f\"   ROC AUC: {best_model['ROC_AUC']:.4f}\")\n",
        "    print(f\"   PR AUC: {best_model['PR_AUC']:.4f}\")\n",
        "\n",
        "    # Test predictions on example messages\n",
        "    example_messages = [\n",
        "        \"Congratulations! You've won a ugx100000 gift card. Click here to claim: www.example.com\",\n",
        "        \"Your account has been suspended. Please verify your identity by sending your PIN to this number.\",\n",
        "        \"Hi, just checking if we're still meeting for lunch tomorrow at 12?\",\n",
        "        \"URGENT: Your payment of ugx55000 has been processed. If this was not you, call immediately.\",\n",
        "        \"Your package will be delivered tomorrow between 10am and 2pm. No signature required.\",\n",
        "        \"You have been selected to win ugx500000! Call now to claim your prize before it expires!\",\n",
        "        \"Meeting postponed to next Wednesday at 3pm. Please confirm if you can attend.\"\n",
        "    ]\n",
        "\n",
        "    # Load the best model for predictions\n",
        "    best_technique = best_model['Model']\n",
        "    family, approach = best_technique.split(\"_\")\n",
        "    model_name = model_families[family][\"model_name\"]\n",
        "    tokenizer = model_families[family][\"tokenizer\"]\n",
        "\n",
        "    # Load the saved best model\n",
        "    best_model_path = f\"results/{best_technique}/final_model\"\n",
        "    if approach == \"lora\":\n",
        "        # For LoRA models, we need to load the base model and apply LoRA\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2,\n",
        "            id2label=id2label,\n",
        "            label2id=label2id\n",
        "        )\n",
        "        peft_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=16,\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"query\", \"value\"] if family == \"roberta\" else [\"q_lin\", \"v_lin\"]\n",
        "        )\n",
        "        model = get_peft_model(base_model, peft_config)\n",
        "        model.load_adapter(best_model_path, \"default\")\n",
        "    else:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Make predictions\n",
        "    example_predictions = predict_examples(model, tokenizer, example_messages, best_technique)\n",
        "\n",
        "    # Save example predictions\n",
        "    example_predictions.to_csv(f\"results/{best_technique}/example_predictions.csv\", index=False)\n",
        "\n",
        "    # Load and combine with previous results if available\n",
        "    try:\n",
        "        baseline_results = pd.read_csv(\"results/metrics/baseline_ml_results.csv\")\n",
        "        dl_results = pd.read_csv(\"results/metrics/all_models_comparison.csv\")\n",
        "\n",
        "        # Combine all results\n",
        "        final_comparison = pd.concat([baseline_results, dl_results, comparison_df], ignore_index=True)\n",
        "\n",
        "        # Handle missing columns by filling with NaN\n",
        "        all_columns = set()\n",
        "        for df in [baseline_results, dl_results, comparison_df]:\n",
        "            all_columns.update(df.columns)\n",
        "\n",
        "        for col in all_columns:\n",
        "            if col not in final_comparison.columns:\n",
        "                final_comparison[col] = np.nan\n",
        "\n",
        "        # Sort by MCC\n",
        "        final_comparison = final_comparison.sort_values(\"MCC\", ascending=False)\n",
        "\n",
        "        # Save final comparison\n",
        "        final_comparison.to_csv(\"results/metrics/final_all_models_comparison.csv\", index=False)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"FINAL COMPARISON OF ALL MODELS (ML + DL + LLM)\")\n",
        "        print(\"=\"*80)\n",
        "        key_columns = [\"Model\", \"MCC\", \"F1\", \"ROC_AUC\", \"PR_AUC\"]\n",
        "        available_columns = [col for col in key_columns if col in final_comparison.columns]\n",
        "        print(final_comparison[available_columns].to_string(index=False))\n",
        "\n",
        "        # Create final visualization\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Focus on models with complete data for key metrics\n",
        "        complete_data = final_comparison.dropna(subset=[\"MCC\", \"F1\"])\n",
        "\n",
        "        if len(complete_data) > 0:\n",
        "            key_metrics = [\"F1\", \"MCC\", \"ROC_AUC\", \"PR_AUC\"]\n",
        "            available_metrics = [col for col in key_metrics if col in complete_data.columns]\n",
        "\n",
        "            melted_final = complete_data.melt(\n",
        "                id_vars=[\"Model\"],\n",
        "                value_vars=available_metrics,\n",
        "                var_name=\"Metric\",\n",
        "                value_name=\"Score\"\n",
        "            )\n",
        "\n",
        "            sns.barplot(data=melted_final, x=\"Model\", y=\"Score\", hue=\"Metric\")\n",
        "            plt.title(\"Final Performance Comparison: All Model Types\")\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.ylim(0, 1)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(\"results/visualizations/final_all_models_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            # Print top 5 models\n",
        "            print(f\"\\n TOP 5 MODELS BY MCC:\")\n",
        "            top_5 = complete_data.head(5)\n",
        "            for i, (_, row) in enumerate(top_5.iterrows(), 1):\n",
        "                print(f\"{i}. {row['Model']}: MCC = {row['MCC']:.4f}, F1 = {row['F1']:.4f}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\nNote: Could not load previous results for comparison: {e}\")\n",
        "        print(\"Showing LLM results only.\")\n",
        "\n",
        "    print(f\"\\nLLM fine-tuning completed successfully!\")\n",
        "    print(f\"Results saved to: results/\")\n",
        "    print(f\"Best model: {best_technique}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}
