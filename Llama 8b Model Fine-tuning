{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH8Rtf9pAnII"
      },
      "outputs": [],
      "source": [
        "# 1: Install Unsloth and dependencies\n",
        "print(\"Installing Unsloth and dependencies...\")\n",
        "\n",
        "# Install Unsloth (this may take a few minutes)\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes -q\n",
        "\n",
        "print(\"Installation complete!\")\n",
        "\n",
        "# Check if we have GPU access\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2: Set up project directory and imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up project paths\n",
        "import os\n",
        "project_dir = '/content/drive/MyDrive/final-sms-scam-detection'\n",
        "os.chdir(project_dir)\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Create directory for Llama results\n",
        "llama_results_dir = \"results/llama_models\"\n",
        "os.makedirs(llama_results_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "a-iYxiYWBC9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "# Unsloth and training imports\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Evaluation imports\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, classification_report\n",
        ")"
      ],
      "metadata": {
        "id": "SSSyuRc-Aws7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "lPUqQz7JBeer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3: Load data and inspect structure\n",
        "# Load data\n",
        "data_dir = \"data/processed/\"\n",
        "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
        "val_df = pd.read_csv(os.path.join(data_dir, \"val.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
        "\n",
        "print(f\"Loaded data: Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "5uR7Y5EMB_ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect data structure\n",
        "print(\"\\nData structure:\")\n",
        "print(\"Columns:\", train_df.columns.tolist())\n",
        "print(\"\\nSample messages:\")\n",
        "print(\"Legitimate:\", train_df[train_df['label'] == 0]['message'].iloc[0])\n",
        "print(\"Spam:\", train_df[train_df['label'] == 1]['message'].iloc[0])\n",
        "\n",
        "# Check class distribution\n",
        "print(\"\\nClass distribution:\")\n",
        "for split_name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
        "    class_counts = df['label'].value_counts()\n",
        "    print(f\"{split_name}: {class_counts.to_dict()}\")\n",
        "    print(f\"  Imbalance ratio: {class_counts.max() / class_counts.min():.2f}\")\n",
        "\n",
        "# Check message lengths\n",
        "print(f\"\\nMessage length statistics:\")\n",
        "train_df['msg_length'] = train_df['message'].str.len()\n",
        "print(f\"Average length: {train_df['msg_length'].mean():.1f} characters\")\n",
        "print(f\"Max length: {train_df['msg_length'].max()} characters\")\n",
        "print(f\"95th percentile: {train_df['msg_length'].quantile(0.95):.0f} characters\")"
      ],
      "metadata": {
        "id": "uMR67B8MB3Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4: Load Llama model with Unsloth\n",
        "print(\"Loading Llama 3 8B model...\")\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 512  # Good for 95th percentile of 230 chars\n",
        "dtype = None  # Auto-detect\n",
        "load_in_4bit = True  # Essential for 8B model in Colab\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",  # 4-bit quantized Llama 3 8B\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Display model info\n",
        "print(f\"Model: {model.config.name_or_path}\")\n",
        "print(f\"Max sequence length: {max_seq_length}\")\n",
        "print(f\"Model parameters: ~8B (4-bit quantized)\")\n",
        "\n",
        "# Check tokenizer\n",
        "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")\n",
        "print(f\"EOS token: {tokenizer.eos_token}\")\n",
        "\n",
        "# Test tokenization with your data\n",
        "sample_message = train_df['message'].iloc[0]\n",
        "tokens = tokenizer(sample_message, return_tensors=\"pt\")\n",
        "print(f\"\\nSample tokenization:\")\n",
        "print(f\"Message: {sample_message}\")\n",
        "print(f\"Token count: {len(tokens['input_ids'][0])}\")"
      ],
      "metadata": {
        "id": "P5JxWmprCDOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5: Add LoRA adapters for efficient fine-tuning\n",
        "print(\"Adding LoRA adapters...\")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank - adjust based on performance\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's memory optimization\n",
        "    random_state=RANDOM_SEED,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters added!\")\n",
        "\n",
        "# Count trainable parameters\n",
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params, trainable_params\n",
        "\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "print(f\"\\nParameter count:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Trainable percentage: {trainable_params/total_params*100:.2f}%\")"
      ],
      "metadata": {
        "id": "Pni0zzDYDFzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6: Define instruction formatting functions\n",
        "def format_sms_instruction(message, label=None, include_context=True):\n",
        "    \"\"\"Format SMS data as instruction-following examples for Ugandan context.\"\"\"\n",
        "\n",
        "    # Context about Ugandan SMS patterns (optional but helpful)\n",
        "    context = \"\"\n",
        "    if include_context:\n",
        "        context = \"\"\"You are an expert at detecting SMS spam in Uganda. You understand common Ugandan mobile money terms (like UGX, AirtelMoney), local phone number formats, and typical scam patterns targeting Ugandan users. \"\"\"\n",
        "\n",
        "    instruction = f\"\"\"{context}Analyze the following SMS message and classify it as either \"spam\" or \"legitimate\".\n",
        "\n",
        "SMS Message: \"{message}\"\n",
        "\n",
        "Classification:\"\"\"\n",
        "\n",
        "    if label is not None:\n",
        "        # Training format - add the correct answer\n",
        "        label_text = \"spam\" if label == 1 else \"legitimate\"\n",
        "        return {\n",
        "            \"text\": instruction + f\" {label_text}{tokenizer.eos_token}\"\n",
        "        }\n",
        "    else:\n",
        "        # Inference format - just the question\n",
        "        return instruction"
      ],
      "metadata": {
        "id": "NM29zHadEKhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(df, sample_size=None):\n",
        "    \"\"\"Convert DataFrame to instruction format dataset.\"\"\"\n",
        "    if sample_size:\n",
        "        df = df.sample(n=min(sample_size, len(df)), random_state=RANDOM_SEED)\n",
        "\n",
        "    formatted_data = []\n",
        "    for _, row in df.iterrows():\n",
        "        formatted_example = format_sms_instruction(\n",
        "            message=row['message'],\n",
        "            label=row['label'],\n",
        "            include_context=True\n",
        "        )\n",
        "        formatted_data.append(formatted_example)\n",
        "\n",
        "    return Dataset.from_list(formatted_data)\n",
        "\n",
        "# Test the formatting\n",
        "print(\"Testing instruction formatting...\")\n",
        "test_message = train_df['message'].iloc[0]\n",
        "test_label = train_df['label'].iloc[0]\n",
        "\n",
        "formatted_example = format_sms_instruction(test_message, test_label)\n",
        "print(\"Formatted training example:\")\n",
        "print(formatted_example['text'])\n",
        "print(f\"\\nLength: {len(formatted_example['text'])} characters\")\n",
        "\n",
        "# Test inference format\n",
        "inference_example = format_sms_instruction(test_message, label=None)\n",
        "print(f\"\\nInference format:\")\n",
        "print(inference_example)"
      ],
      "metadata": {
        "id": "dlfg0Bp7DpEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7: Prepare training datasets\n",
        "print(\"Preparing training datasets...\")\n",
        "\n",
        "# Start with smaller datasets for initial testing\n",
        "train_dataset = prepare_dataset(train_df, sample_size=200)  # Start small for testing\n",
        "val_dataset = prepare_dataset(val_df, sample_size=50)       # Small validation set\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(val_dataset)}\")\n",
        "\n",
        "# Check class distribution by looking at actual labels from original dataframes\n",
        "train_sample_indices = train_df.sample(n=min(200, len(train_df)), random_state=RANDOM_SEED).index\n",
        "val_sample_indices = val_df.sample(n=min(50, len(val_df)), random_state=RANDOM_SEED).index\n",
        "\n",
        "train_sample_labels = train_df.loc[train_sample_indices, 'label']\n",
        "val_sample_labels = val_df.loc[val_sample_indices, 'label']\n",
        "\n",
        "train_spam_count = sum(train_sample_labels == 1)\n",
        "train_legit_count = sum(train_sample_labels == 0)\n",
        "\n",
        "print(f\"\\nTraining set distribution:\")\n",
        "print(f\"Legitimate: {train_legit_count}\")\n",
        "print(f\"Spam: {train_spam_count}\")\n",
        "if train_spam_count > 0:\n",
        "    print(f\"Ratio: {train_legit_count/train_spam_count:.2f}:1\")\n",
        "\n",
        "# Show sample examples using proper indexing\n",
        "print(f\"\\nSample training examples:\")\n",
        "for i in range(min(2, len(train_dataset))):\n",
        "    example_text = train_dataset[i]['text'] if isinstance(train_dataset[i], dict) else str(train_dataset[i])\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(example_text[:300] + \"...\" if len(example_text) > 300 else example_text)\n",
        "\n",
        "# Check tokenization lengths with proper data access\n",
        "print(\"Checking tokenization lengths...\")\n",
        "sample_texts = []\n",
        "for i in range(min(10, len(train_dataset))):\n",
        "    if isinstance(train_dataset[i], dict):\n",
        "        sample_texts.append(train_dataset[i]['text'])\n",
        "    else:\n",
        "        sample_texts.append(str(train_dataset[i]))\n",
        "\n",
        "token_lengths = []\n",
        "for text in sample_texts:\n",
        "    try:\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "        token_lengths.append(len(tokens['input_ids'][0]))\n",
        "    except Exception as e:\n",
        "        print(f\"Tokenization error for text: {text[:50]}... Error: {e}\")\n",
        "\n",
        "if token_lengths:\n",
        "    print(f\"\\nTokenization statistics:\")\n",
        "    print(f\"Average tokens: {np.mean(token_lengths):.1f}\")\n",
        "    print(f\"Max tokens: {max(token_lengths)}\")\n",
        "    print(f\"Min tokens: {min(token_lengths)}\")\n",
        "    print(f\"Samples that fit in {max_seq_length} tokens: {sum(1 for x in token_lengths if x <= max_seq_length)}/{len(token_lengths)}\")\n",
        "else:\n",
        "    print(\"No valid tokenization results\")\n",
        "\n",
        "# Debug: Let's also check the actual dataset structure\n",
        "print(f\"\\nDebugging dataset structure:\")\n",
        "print(f\"Type of train_dataset: {type(train_dataset)}\")\n",
        "print(f\"Type of first element: {type(train_dataset[0])}\")\n",
        "if len(train_dataset) > 0:\n",
        "    first_element = train_dataset[0]\n",
        "    if isinstance(first_element, dict):\n",
        "        print(f\"Keys in first element: {first_element.keys()}\")\n",
        "    else:\n",
        "        print(f\"First element: {str(first_element)[:200]}\")"
      ],
      "metadata": {
        "id": "4BW81rhuFeeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8: Set up training configuration\n",
        "print(\"Setting up training configuration...\")\n",
        "\n",
        "# Training arguments - optimized for our dataset size and token lengths\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,      # Conservative for 8B model\n",
        "    gradient_accumulation_steps=4,       # Effective batch size = 4\n",
        "    warmup_steps=5,                     # Short warmup for small dataset\n",
        "    max_steps=100,                      # Increased since data looks good\n",
        "    learning_rate=2e-4,                 # Standard LoRA learning rate\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=10,                   # Log every 10 steps\n",
        "    optim=\"adamw_8bit\",                 # Memory efficient optimizer\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=RANDOM_SEED,\n",
        "    output_dir=f\"{llama_results_dir}/checkpoints\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20,                      # Evaluate every 20 steps\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=40,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\",                   # Disable wandb/tensorboard\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    save_total_limit=2,                 # Keep only 2 checkpoints\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Max steps: {training_args.max_steps}\")\n",
        "print(f\"Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"Evaluation every: {training_args.eval_steps} steps\")\n",
        "print(f\"Save checkpoints every: {training_args.save_steps} steps\")\n",
        "print(f\"Average tokens per example: 111.7\")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # Important for classification tasks\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")\n",
        "print(f\"Training will take approximately {training_args.max_steps * 4 / 60:.1f} minutes\")"
      ],
      "metadata": {
        "id": "rKfgaSu5GE_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9: Start training\n",
        "print(\"Starting Llama 3 8B fine-tuning for SMS spam detection...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    print(\"Training in progress...\")\n",
        "    trainer_stats = trainer.train()\n",
        "\n",
        "    # Calculate training time\n",
        "    training_time = time.time() - start_time\n",
        "    training_time_minutes = training_time / 60\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Training time: {training_time_minutes:.2f} minutes\")\n",
        "    print(f\"Final training loss: {trainer_stats.training_loss:.4f}\")\n",
        "    print(f\"Total steps completed: {trainer_stats.global_step}\")\n",
        "\n",
        "    # Save training stats\n",
        "    training_stats = {\n",
        "        \"training_time_seconds\": training_time,\n",
        "        \"training_time_minutes\": training_time_minutes,\n",
        "        \"final_training_loss\": trainer_stats.training_loss,\n",
        "        \"total_steps\": trainer_stats.global_step,\n",
        "        \"model_config\": {\n",
        "            \"model_name\": \"llama-3-8b-bnb-4bit\",\n",
        "            \"lora_r\": 16,\n",
        "            \"lora_alpha\": 16,\n",
        "            \"lora_dropout\": 0.1,\n",
        "            \"max_seq_length\": max_seq_length,\n",
        "            \"training_samples\": len(train_dataset),\n",
        "            \"validation_samples\": len(val_dataset),\n",
        "            \"effective_batch_size\": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,\n",
        "            \"learning_rate\": training_args.learning_rate\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save to file\n",
        "    with open(f\"{llama_results_dir}/training_stats.json\", \"w\") as f:\n",
        "        json.dump(training_stats, f, indent=4)\n",
        "\n",
        "    print(f\"Training stats saved to {llama_results_dir}/training_stats.json\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save_pretrained(f\"{llama_results_dir}/llama_sms_model\")\n",
        "    tokenizer.save_pretrained(f\"{llama_results_dir}/llama_sms_model\")\n",
        "    print(f\"Model saved to {llama_results_dir}/llama_sms_model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Training failed: {e}\")\n",
        "    print(\"Error details:\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Suggestions for common issues\n",
        "    print(\"\\nTroubleshooting suggestions:\")\n",
        "    print(\"1. Try reducing per_device_train_batch_size to 1\")\n",
        "    print(\"2. Try increasing gradient_accumulation_steps\")\n",
        "    print(\"3. Check GPU memory usage\")"
      ],
      "metadata": {
        "id": "MCUuZv9qG5q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10: Set up inference functions\n",
        "print(\"Setting up inference functions...\")\n",
        "\n",
        "# Enable fast inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"Fast inference mode enabled\")\n",
        "\n",
        "def parse_llama_prediction(response_text):\n",
        "    \"\"\"\n",
        "    Convert Llama's text response to binary prediction and confidence.\n",
        "\n",
        "    Returns:\n",
        "        prediction: 0 (legitimate) or 1 (spam)\n",
        "        confidence: float between 0 and 1\n",
        "        raw_text: the actual response for debugging\n",
        "    \"\"\"\n",
        "    # Extract text after \"Classification:\"\n",
        "    if \"Classification:\" in response_text:\n",
        "        prediction_text = response_text.split(\"Classification:\")[-1].strip().lower()\n",
        "    else:\n",
        "        prediction_text = response_text.lower()\n",
        "\n",
        "    # Clean the prediction text (remove special tokens and punctuation)\n",
        "    prediction_text = re.sub(r'[^\\w\\s]', '', prediction_text)\n",
        "    prediction_text = prediction_text.replace('end_of_text', '').strip()\n",
        "\n",
        "    # Determine prediction with confidence scoring\n",
        "    spam_indicators = ['spam', 'scam', 'fraudulent', 'suspicious', 'phishing']\n",
        "    legit_indicators = ['legitimate', 'ham', 'genuine', 'real', 'normal', 'legit']\n",
        "\n",
        "    # Count matches\n",
        "    spam_score = sum(1 for indicator in spam_indicators if indicator in prediction_text)\n",
        "    legit_score = sum(1 for indicator in legit_indicators if indicator in prediction_text)\n",
        "\n",
        "    if spam_score > legit_score:\n",
        "        prediction = 1  # spam\n",
        "        confidence = min(0.95, 0.6 + (spam_score - legit_score) * 0.1)\n",
        "    elif legit_score > spam_score:\n",
        "        prediction = 0  # legitimate\n",
        "        confidence = min(0.95, 0.6 + (legit_score - spam_score) * 0.1)\n",
        "    else:\n",
        "        # Check for partial matches or default behavior\n",
        "        if any(spam_word in prediction_text for spam_word in ['spam']):\n",
        "            prediction = 1\n",
        "            confidence = 0.6\n",
        "        else:\n",
        "            # Default to legitimate with moderate confidence\n",
        "            prediction = 0\n",
        "            confidence = 0.6\n",
        "\n",
        "    return prediction, confidence, prediction_text"
      ],
      "metadata": {
        "id": "lkscBD4cJkR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sms(message, max_new_tokens=15):\n",
        "    \"\"\"Predict if SMS is spam or legitimate with confidence score.\"\"\"\n",
        "    # Format the instruction\n",
        "    prompt = format_sms_instruction(message, label=None)\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            use_cache=True,\n",
        "            do_sample=False,  # Deterministic for classification\n",
        "            temperature=None,\n",
        "            top_p=None,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and extract prediction\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "    prediction, confidence, raw_text = parse_llama_prediction(response)\n",
        "\n",
        "    return prediction, confidence, raw_text, response\n",
        "\n",
        "# Test the inference function\n",
        "print(\"\\nTesting inference on sample messages...\")\n",
        "\n",
        "test_messages = [\n",
        "    \"Congratulations! You've won UGX 1,000,000! Click here to claim your prize now!\",\n",
        "    \"Hi John, are we still meeting for lunch tomorrow at 1pm?\",\n",
        "    \"Your package will be delivered tomorrow between 10am-2pm. No signature required.\"\n",
        "]\n",
        "\n",
        "for i, message in enumerate(test_messages):\n",
        "    pred, conf, raw, full_response = predict_sms(message)\n",
        "    label = \"spam\" if pred == 1 else \"legitimate\"\n",
        "\n",
        "    print(f\"\\nTest {i+1}:\")\n",
        "    print(f\"Message: {message}\")\n",
        "    print(f\"Prediction: {label} (confidence: {conf:.3f})\")\n",
        "    print(f\"Raw prediction: '{raw}'\")\n",
        "    print(f\"Full response length: {len(full_response)} chars\")"
      ],
      "metadata": {
        "id": "H8DncwKWHzrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11: Comprehensive evaluation on test set\n",
        "print(\"Starting comprehensive evaluation on test set...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def evaluate_llama_comprehensive(test_df):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation that matches other models.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "    true_labels = []\n",
        "    failed_predictions = 0\n",
        "    raw_predictions = []\n",
        "\n",
        "    print(f\"Evaluating {len(test_df)} test samples...\")\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        try:\n",
        "            pred, confidence, raw_text, full_response = predict_sms(row['message'])\n",
        "\n",
        "            # Convert confidence to probability distribution\n",
        "            if pred == 1:  # spam\n",
        "                prob_spam = confidence\n",
        "                prob_legit = 1 - confidence\n",
        "            else:  # legitimate\n",
        "                prob_legit = confidence\n",
        "                prob_spam = 1 - confidence\n",
        "\n",
        "            predictions.append(pred)\n",
        "            probabilities.append([prob_legit, prob_spam])\n",
        "            true_labels.append(row['label'])\n",
        "            raw_predictions.append(raw_text)\n",
        "\n",
        "            # Show progress every 25 samples\n",
        "            if (idx + 1) % 25 == 0:\n",
        "                print(f\"Processed {idx + 1}/{len(test_df)} samples...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to predict for sample {idx}: {e}\")\n",
        "            failed_predictions += 1\n",
        "            # Use default prediction for failed cases\n",
        "            predictions.append(0)\n",
        "            probabilities.append([0.5, 0.5])\n",
        "            true_labels.append(row['label'])\n",
        "            raw_predictions.append(\"failed\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    predictions = np.array(predictions)\n",
        "    probabilities = np.array(probabilities)\n",
        "    true_labels = np.array(true_labels)\n",
        "\n",
        "    # Calculate all metrics (same as your other models)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
        "    mcc = matthews_corrcoef(true_labels, predictions)\n",
        "\n",
        "    # ROC AUC and PR AUC using probability of positive class (spam)\n",
        "    roc_auc = roc_auc_score(true_labels, probabilities[:, 1])\n",
        "    pr_auc = average_precision_score(true_labels, probabilities[:, 1])\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"LLAMA 3 8B EVALUATION RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total predictions: {len(predictions)}\")\n",
        "    print(f\"Failed predictions: {failed_predictions}\")\n",
        "    print(f\"Success rate: {(len(predictions) - failed_predictions) / len(predictions) * 100:.1f}%\")\n",
        "\n",
        "    print(f\"\\nPerformance Metrics:\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "    print(f\"MCC:       {mcc:.4f}\")\n",
        "    print(f\"ROC AUC:   {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC:    {pr_auc:.4f}\")\n",
        "\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"[[{cm[0,0]:3d} {cm[0,1]:3d}]  (Legitimate: True/False)\")\n",
        "    print(f\" [{cm[1,0]:3d} {cm[1,1]:3d}]]  (Spam: False/True)\")\n",
        "\n",
        "    # Show some example predictions\n",
        "    print(f\"\\nSample Predictions:\")\n",
        "    for i in range(min(5, len(test_df))):\n",
        "        message = test_df.iloc[i]['message']\n",
        "        true_label = \"spam\" if true_labels[i] == 1 else \"legitimate\"\n",
        "        pred_label = \"spam\" if predictions[i] == 1 else \"legitimate\"\n",
        "        confidence = probabilities[i][predictions[i]]\n",
        "\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Message: {message[:80]}...\")\n",
        "        print(f\"True: {true_label} | Predicted: {pred_label} (conf: {confidence:.3f})\")\n",
        "        print(f\"Raw: '{raw_predictions[i]}'\")\n",
        "\n",
        "    return {\n",
        "        'Model': 'llama_3_8b_instruction',\n",
        "        'MCC': mcc,\n",
        "        'F1': f1,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'ROC_AUC': roc_auc,\n",
        "        'PR_AUC': pr_auc,\n",
        "        'Model_Type': 'LLM',\n",
        "        'Fine_Tuning_Approach': 'instruction',\n",
        "        'predictions': predictions,\n",
        "        'probabilities': probabilities,\n",
        "        'confusion_matrix': cm,\n",
        "        'failed_predictions': failed_predictions,\n",
        "        'raw_predictions': raw_predictions\n",
        "    }\n",
        "\n",
        "# Run the evaluation\n",
        "llama_results = evaluate_llama_comprehensive(test_df)\n",
        "\n",
        "# Save results\n",
        "results_summary = {\n",
        "    'model': llama_results['Model'],\n",
        "    'metrics': {\n",
        "        'mcc': float(llama_results['MCC']),\n",
        "        'f1': float(llama_results['F1']),\n",
        "        'accuracy': float(llama_results['Accuracy']),\n",
        "        'precision': float(llama_results['Precision']),\n",
        "        'recall': float(llama_results['Recall']),\n",
        "        'roc_auc': float(llama_results['ROC_AUC']),\n",
        "        'pr_auc': float(llama_results['PR_AUC'])\n",
        "    },\n",
        "    'failed_predictions': int(llama_results['failed_predictions']),\n",
        "    'total_samples': len(test_df)\n",
        "}\n",
        "\n",
        "with open(f\"{llama_results_dir}/evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(results_summary, f, indent=4)\n",
        "\n",
        "print(f\"\\nResults saved to {llama_results_dir}/evaluation_results.json\")"
      ],
      "metadata": {
        "id": "iJ4728JhJp3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12: Add Llama results to final comparison\n",
        "print(\"Adding Llama 3 8B results to model comparison...\")\n",
        "\n",
        "# Load existing comparison results\n",
        "comparison_file = \"results/final_comparison/all_models_clean_comparison.csv\"\n",
        "\n",
        "if os.path.exists(comparison_file):\n",
        "    existing_comparison = pd.read_csv(comparison_file)\n",
        "    print(f\"Loaded existing comparison with {len(existing_comparison)} models\")\n",
        "else:\n",
        "    print(\"Previous comparison file not found. Creating new comparison.\")\n",
        "    # You'll need to rerun your other model comparisons first\n",
        "    existing_comparison = pd.DataFrame()\n",
        "\n",
        "# Create Llama results row\n",
        "llama_row = pd.DataFrame([{\n",
        "    'Model': 'llama_3_8b_instruction',\n",
        "    'MCC': llama_results['MCC'],\n",
        "    'F1': llama_results['F1'],\n",
        "    'Accuracy': llama_results['Accuracy'],\n",
        "    'Precision': llama_results['Precision'],\n",
        "    'Recall': llama_results['Recall'],\n",
        "    'ROC_AUC': llama_results['ROC_AUC'],\n",
        "    'PR_AUC': llama_results['PR_AUC'],\n",
        "    'Model_Type': 'LLM',\n",
        "    'Fine_Tuning_Approach': 'instruction'\n",
        "}])\n",
        "\n",
        "print(f\"\\nLlama 3 8B Results:\")\n",
        "print(f\"MCC: {llama_results['MCC']:.4f}\")\n",
        "print(f\"F1:  {llama_results['F1']:.4f}\")\n",
        "print(f\"Accuracy: {llama_results['Accuracy']:.4f}\")\n",
        "\n",
        "if not existing_comparison.empty:\n",
        "    # Combine with existing results\n",
        "    updated_comparison = pd.concat([existing_comparison, llama_row], ignore_index=True)\n",
        "\n",
        "    # Sort by MCC\n",
        "    updated_comparison = updated_comparison.sort_values('MCC', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Save updated comparison\n",
        "    updated_comparison.to_csv(\"results/final_comparison/all_models_including_llama.csv\", index=False)\n",
        "\n",
        "    # Display top 10 models\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"UPDATED MODEL COMPARISON - TOP 10 MODELS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    display_cols = ['Model', 'Model_Type', 'MCC', 'F1', 'Accuracy', 'ROC_AUC', 'Fine_Tuning_Approach']\n",
        "    top_10 = updated_comparison.head(10)\n",
        "\n",
        "    for i, row in top_10.iterrows():\n",
        "        approach = row.get('Fine_Tuning_Approach', 'N/A')\n",
        "        if pd.isna(approach):\n",
        "            approach = 'N/A'\n",
        "        print(f\"{i+1:2d}. {row['Model']:<25} ({row['Model_Type']:<3}) MCC:{row['MCC']:.4f} F1:{row['F1']:.4f} ({approach})\")\n",
        "\n",
        "    # Performance by model type\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"AVERAGE PERFORMANCE BY MODEL TYPE\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    type_summary = updated_comparison.groupby('Model_Type')[['MCC', 'F1', 'Accuracy', 'ROC_AUC']].agg(['mean', 'max', 'count']).round(4)\n",
        "    print(type_summary)\n",
        "\n",
        "    # Llama's ranking\n",
        "    llama_rank = updated_comparison[updated_comparison['Model'] == 'llama_3_8b_instruction'].index[0] + 1\n",
        "    total_models = len(updated_comparison)\n",
        "\n",
        "    print(f\"\\nLLAMA 3 8B PERFORMANCE SUMMARY:\")\n",
        "    print(f\"   Rank: #{llama_rank} out of {total_models} models\")\n",
        "    print(f\"   MCC: {llama_results['MCC']:.4f} (Higher is better)\")\n",
        "    print(f\"   F1: {llama_results['F1']:.4f}\")\n",
        "    print(f\"   Model size: ~8B parameters\")\n",
        "    print(f\"   Training time: 4.74 minutes\")\n",
        "    print(f\"   Success rate: 100% (no failed predictions)\")\n",
        "\n",
        "else:\n",
        "    # Save just Llama results\n",
        "    llama_row.to_csv(\"results/final_comparison/llama_results.csv\", index=False)\n",
        "    print(\"Saved Llama results. Please rerun other model comparisons to see full ranking.\")\n",
        "\n",
        "print(f\"\\nResults saved to results/final_comparison/\")"
      ],
      "metadata": {
        "id": "L6NMtdadJ_xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13: Detailed analysis and insights\n",
        "print(\"DETAILED ANALYSIS OF LLAMA 3 8B PERFORMANCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Compare with your best previous models (based on your earlier results)\n",
        "previous_best = {\n",
        "    'roberta_frozen': {'MCC': 0.8750, 'F1': 0.9000},\n",
        "    'roberta_full': {'MCC': 0.8561, 'F1': 0.8852},\n",
        "    'logistic_regression': {'MCC': 0.8150, 'F1': 0.8525}\n",
        "}\n",
        "\n",
        "llama_mcc = llama_results['MCC']\n",
        "llama_f1 = llama_results['F1']\n",
        "\n",
        "print(f\"\\nComparison with Previous Best Models:\")\n",
        "print(f\"{'Model':<20} {'MCC':<8} {'F1':<8} {'vs Llama MCC':<15} {'vs Llama F1':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for model, metrics in previous_best.items():\n",
        "    mcc_diff = llama_mcc - metrics['MCC']\n",
        "    f1_diff = llama_f1 - metrics['F1']\n",
        "    mcc_status = f\"{mcc_diff:+.4f}\" if mcc_diff != 0 else \"tie\"\n",
        "    f1_status = f\"{f1_diff:+.4f}\" if f1_diff != 0 else \"tie\"\n",
        "\n",
        "    print(f\"{model:<20} {metrics['MCC']:<8.4f} {metrics['F1']:<8.4f} {mcc_status:<15} {f1_status:<15}\")\n",
        "\n",
        "print(f\"{'Llama 3 8B':<20} {llama_mcc:<8.4f} {llama_f1:<8.4f} {'baseline':<15} {'baseline':<15}\")\n",
        "\n",
        "# Error analysis\n",
        "predictions = llama_results['predictions']\n",
        "true_labels = test_df['label'].values\n",
        "raw_predictions = llama_results['raw_predictions']\n",
        "\n",
        "# Find misclassified examples\n",
        "misclassified = np.where(predictions != true_labels)[0]\n",
        "false_positives = np.where((predictions == 1) & (true_labels == 0))[0]  # Predicted spam, actually legit\n",
        "false_negatives = np.where((predictions == 0) & (true_labels == 1))[0]  # Predicted legit, actually spam\n",
        "\n",
        "print(f\"\\nError Analysis:\")\n",
        "print(f\"Total misclassified: {len(misclassified)}/{len(predictions)} ({len(misclassified)/len(predictions)*100:.1f}%)\")\n",
        "print(f\"False positives: {len(false_positives)} (legitimate messages predicted as spam)\")\n",
        "print(f\"False negatives: {len(false_negatives)} (spam messages predicted as legitimate)\")\n",
        "\n",
        "# Show some misclassified examples\n",
        "if len(false_positives) > 0:\n",
        "    print(f\"\\nFalse Positives (Legitimate predicted as Spam):\")\n",
        "    for i, idx in enumerate(false_positives[:3]):\n",
        "        message = test_df.iloc[idx]['message']\n",
        "        raw_pred = raw_predictions[idx]\n",
        "        print(f\"{i+1}. Message: {message[:100]}...\")\n",
        "        print(f\"   Raw prediction: '{raw_pred}'\")\n",
        "\n",
        "if len(false_negatives) > 0:\n",
        "    print(f\"\\nFalse Negatives (Spam predicted as Legitimate):\")\n",
        "    for i, idx in enumerate(false_negatives[:3]):\n",
        "        message = test_df.iloc[idx]['message']\n",
        "        raw_pred = raw_predictions[idx]\n",
        "        print(f\"{i+1}. Message: {message[:100]}...\")\n",
        "        print(f\"   Raw prediction: '{raw_pred}'\")"
      ],
      "metadata": {
        "id": "CGCPPXi5KpED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14: Document 29% data performance before scaling up\n",
        "print(\"DOCUMENTING LLAMA PERFORMANCE WITH LIMITED DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create detailed documentation\n",
        "limited_data_results = {\n",
        "    \"experiment_type\": \"limited_data_baseline\",\n",
        "    \"training_samples\": len(train_dataset),\n",
        "    \"total_available_samples\": len(train_df),\n",
        "    \"data_utilization_percentage\": len(train_dataset) / len(train_df) * 100,\n",
        "    \"training_steps\": 100,\n",
        "    \"training_time_minutes\": 4.74,\n",
        "    \"performance\": {\n",
        "        \"mcc\": float(llama_results['MCC']),\n",
        "        \"f1\": float(llama_results['F1']),\n",
        "        \"accuracy\": float(llama_results['Accuracy']),\n",
        "        \"precision\": float(llama_results['Precision']),\n",
        "        \"recall\": float(llama_results['Recall']),\n",
        "        \"roc_auc\": float(llama_results['ROC_AUC']),\n",
        "        \"pr_auc\": float(llama_results['PR_AUC'])\n",
        "    },\n",
        "    \"ranking\": {\n",
        "        \"position\": 7,\n",
        "        \"total_models\": 15,\n",
        "        \"percentile\": 7/15 * 100\n",
        "    },\n",
        "    \"context\": {\n",
        "        \"note\": \"Strong performance with only 29% of available training data\",\n",
        "        \"data_scarcity_relevance\": \"Demonstrates viability when labeled SMS data is limited\",\n",
        "        \"scaling_potential\": \"Expected improvement with full dataset\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save baseline documentation\n",
        "with open(f\"{llama_results_dir}/limited_data_baseline.json\", \"w\") as f:\n",
        "    json.dump(limited_data_results, f, indent=4)\n",
        "\n",
        "print(f\"Training data utilization: {len(train_dataset)}/{len(train_df)} samples ({len(train_dataset)/len(train_df)*100:.1f}%)\")\n",
        "print(f\"Performance achieved: MCC = {llama_results['MCC']:.4f}\")\n",
        "print(f\"Ranking: #{7}/15 models\")\n",
        "print(f\"Training efficiency: {llama_results['MCC']:.4f} MCC in {4.74:.1f} minutes\")\n",
        "print(f\"\\nBaseline documented in: {llama_results_dir}/limited_data_baseline.json\")"
      ],
      "metadata": {
        "id": "DM-GaOEtLRmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15: Scale up to full dataset training\n",
        "print(\"SCALING TO FULL DATASET TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Prepare full datasets\n",
        "print(\"Preparing full datasets...\")\n",
        "train_dataset_full = prepare_dataset(train_df)  # All samples\n",
        "val_dataset_full = prepare_dataset(val_df)      # All samples\n",
        "\n",
        "print(f\"Full training examples: {len(train_dataset_full)}\")\n",
        "print(f\"Full validation examples: {len(val_dataset_full)}\")\n",
        "\n",
        "# FIXED: Check class distribution using actual dataframe labels\n",
        "print(f\"Full training set distribution:\")\n",
        "print(f\"Legitimate: {sum(train_df['label'] == 0)}\")\n",
        "print(f\"Spam: {sum(train_df['label'] == 1)}\")\n",
        "print(f\"Ratio: {sum(train_df['label'] == 0) / sum(train_df['label'] == 1):.2f}:1\")\n",
        "\n",
        "# Debug the model object\n",
        "print(f\"\\nDebugging model object:\")\n",
        "print(f\"Type of model: {type(model)}\")\n",
        "print(f\"Has config: {hasattr(model, 'config')}\")\n",
        "\n",
        "# If model object is corrupted, reload it\n",
        "if not hasattr(model, 'config') or isinstance(model, str):\n",
        "    print(\"Model object corrupted, reloading...\")\n",
        "\n",
        "    # Reload the trained model\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=f\"{llama_results_dir}/llama_sms_model\",  # Load our saved model\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    # Re-add LoRA for continued training\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=16,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    print(\"Model reloaded successfully\")\n",
        "\n",
        "# Verify model is ready\n",
        "print(f\"Model type after fix: {type(model)}\")\n",
        "print(f\"Model config available: {hasattr(model, 'config')}\")\n",
        "\n",
        "# Updated training configuration for full dataset\n",
        "training_args_full = TrainingArguments(\n",
        "    per_device_train_batch_size=1,      # Keep small for memory\n",
        "    gradient_accumulation_steps=4,       # Effective batch size = 4\n",
        "    warmup_steps=10,                    # Longer warmup for more data\n",
        "    max_steps=300,                      # More steps for full dataset\n",
        "    learning_rate=2e-4,                 # Same learning rate\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=15,                   # Log every 15 steps\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=RANDOM_SEED,\n",
        "    output_dir=f\"{llama_results_dir}/checkpoints_full\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=30,                      # Evaluate every 30 steps\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=60,                      # Save every 60 steps (multiple of eval_steps)\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    save_total_limit=3,                 # Keep only 3 checkpoints\n",
        ")\n",
        "\n",
        "print(f\"\\nFull training configuration:\")\n",
        "print(f\"Training samples: {len(train_dataset_full)}\")\n",
        "print(f\"Max steps: {training_args_full.max_steps}\")\n",
        "print(f\"Estimated time: ~{training_args_full.max_steps * 7 / 60:.1f} minutes\")\n",
        "\n",
        "# Create new trainer for full dataset\n",
        "try:\n",
        "    trainer_full = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset_full,\n",
        "        eval_dataset=val_dataset_full,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=2,\n",
        "        packing=False,\n",
        "        args=training_args_full,\n",
        "    )\n",
        "\n",
        "    print(\"Full dataset trainer initialized successfully!\")\n",
        "    print(\"Ready to begin full training...\")\n",
        "    print(f\"\\nNote: Training will build upon previous fine-tuning\")\n",
        "    print(f\"Previous performance (29% data): MCC = {llama_results['MCC']:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Trainer initialization failed: {e}\")\n",
        "    print(\"This might indicate a model state issue. Let's debug further.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "l5L9mHEVNyJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16: Execute full training\n",
        "print(\"STARTING FULL DATASET TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Training with all 700 samples (vs previous 200)\")\n",
        "print(\"This will take approximately 35-40 minutes...\")\n",
        "print(f\"Building upon previous performance: MCC = {llama_results['MCC']:.4f}\")\n",
        "\n",
        "# Record start time\n",
        "start_time_full = time.time()\n",
        "\n",
        "try:\n",
        "    print(\"\\nFull training in progress...\")\n",
        "    print(\"Progress will be shown every 15 steps...\")\n",
        "\n",
        "    trainer_stats_full = trainer_full.train()\n",
        "\n",
        "    # Calculate training time\n",
        "    training_time_full = time.time() - start_time_full\n",
        "    training_time_minutes_full = training_time_full / 60\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"FULL TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Training time: {training_time_minutes_full:.2f} minutes\")\n",
        "    print(f\"Final training loss: {trainer_stats_full.training_loss:.4f}\")\n",
        "    print(f\"Total steps completed: {trainer_stats_full.global_step}\")\n",
        "\n",
        "    # Compare with limited training\n",
        "    time_ratio = training_time_minutes_full / 4.74\n",
        "    data_ratio = 700 / 200\n",
        "\n",
        "    print(f\"\\nSCALING ANALYSIS:\")\n",
        "    print(f\"Data scale: {data_ratio:.1f}x more training samples\")\n",
        "    print(f\"Time scale: {time_ratio:.1f}x longer training time\")\n",
        "    print(f\"Efficiency: {data_ratio/time_ratio:.2f}x data per unit time\")\n",
        "\n",
        "    # Save comprehensive training stats\n",
        "    training_stats_full = {\n",
        "        \"experiment_type\": \"full_dataset_training\",\n",
        "        \"training_time_seconds\": training_time_full,\n",
        "        \"training_time_minutes\": training_time_minutes_full,\n",
        "        \"final_training_loss\": trainer_stats_full.training_loss,\n",
        "        \"total_steps\": trainer_stats_full.global_step,\n",
        "        \"training_samples\": len(train_dataset_full),\n",
        "        \"validation_samples\": len(val_dataset_full),\n",
        "        \"data_utilization_percentage\": 100.0,\n",
        "        \"model_config\": {\n",
        "            \"model_name\": \"llama-3-8b-bnb-4bit\",\n",
        "            \"continues_from\": \"29_percent_baseline\",\n",
        "            \"lora_r\": 16,\n",
        "            \"lora_alpha\": 16,\n",
        "            \"lora_dropout\": 0.1,\n",
        "            \"max_seq_length\": max_seq_length,\n",
        "            \"effective_batch_size\": training_args_full.per_device_train_batch_size * training_args_full.gradient_accumulation_steps,\n",
        "            \"learning_rate\": training_args_full.learning_rate\n",
        "        },\n",
        "        \"scaling_comparison\": {\n",
        "            \"limited_data\": {\n",
        "                \"samples\": 200,\n",
        "                \"time_minutes\": 4.74,\n",
        "                \"mcc_achieved\": float(llama_results['MCC'])\n",
        "            },\n",
        "            \"full_data\": {\n",
        "                \"samples\": 700,\n",
        "                \"time_minutes\": training_time_minutes_full,\n",
        "                \"data_scaling_factor\": 700/200,\n",
        "                \"time_scaling_factor\": training_time_minutes_full/4.74\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save to file\n",
        "    with open(f\"{llama_results_dir}/full_training_stats.json\", \"w\") as f:\n",
        "        json.dump(training_stats_full, j, indent=4)\n",
        "\n",
        "    print(f\"\\nTraining stats saved to {llama_results_dir}/full_training_stats.json\")\n",
        "\n",
        "    # Save the fully trained model\n",
        "    model.save_pretrained(f\"{llama_results_dir}/llama_sms_model_full\")\n",
        "    tokenizer.save_pretrained(f\"{llama_results_dir}/llama_sms_model_full\")\n",
        "    print(f\"Full model saved to {llama_results_dir}/llama_sms_model_full\")\n",
        "\n",
        "    # Re-enable fast inference\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    print(\"Model ready for evaluation\")\n",
        "\n",
        "    print(f\"\\nNEXT: Evaluating full model performance vs {llama_results['MCC']:.4f} baseline\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Full training failed: {e}\")\n",
        "    print(\"Error details:\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "8dLbzvQKN8Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17: Evaluate full model performance\n",
        "print(\"EVALUATING FULL MODEL PERFORMANCE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Comparing full dataset results vs 29% baseline...\")\n",
        "\n",
        "# Evaluate the full model\n",
        "llama_results_full = evaluate_llama_comprehensive(test_df)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"PERFORMANCE COMPARISON: LIMITED vs FULL TRAINING\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "comparison_metrics = ['MCC', 'F1', 'Accuracy', 'Precision', 'Recall', 'ROC_AUC', 'PR_AUC']\n",
        "\n",
        "print(f\"{'Metric':<12} {'29% Data':<10} {'Full Data':<10} {'Improvement':<12}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for metric in comparison_metrics:\n",
        "    limited_val = llama_results[metric]\n",
        "    full_val = llama_results_full[metric]\n",
        "    improvement = full_val - limited_val\n",
        "    improvement_pct = (improvement / limited_val * 100) if limited_val != 0 else 0\n",
        "\n",
        "    print(f\"{metric:<12} {limited_val:<10.4f} {full_val:<10.4f} {improvement:+.4f} ({improvement_pct:+.1f}%)\")\n",
        "\n",
        "print(f\"\\nKEY IMPROVEMENTS:\")\n",
        "mcc_improvement = llama_results_full['MCC'] - llama_results['MCC']\n",
        "f1_improvement = llama_results_full['F1'] - llama_results['F1']\n",
        "\n",
        "print(f\"MCC: {llama_results['MCC']:.4f} → {llama_results_full['MCC']:.4f} ({mcc_improvement:+.4f})\")\n",
        "print(f\"F1:  {llama_results['F1']:.4f} → {llama_results_full['F1']:.4f} ({f1_improvement:+.4f})\")\n",
        "\n",
        "# Save full results\n",
        "results_full_summary = {\n",
        "    'model': 'llama_3_8b_instruction_full',\n",
        "    'training_data_percentage': 100.0,\n",
        "    'metrics': {\n",
        "        'mcc': float(llama_results_full['MCC']),\n",
        "        'f1': float(llama_results_full['F1']),\n",
        "        'accuracy': float(llama_results_full['Accuracy']),\n",
        "        'precision': float(llama_results_full['Precision']),\n",
        "        'recall': float(llama_results_full['Recall']),\n",
        "        'roc_auc': float(llama_results_full['ROC_AUC']),\n",
        "        'pr_auc': float(llama_results_full['PR_AUC'])\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f\"{llama_results_dir}/full_evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(results_full_summary, f, indent=4)\n",
        "\n",
        "print(f\"\\nFull results saved to {llama_results_dir}/full_evaluation_results.json\")"
      ],
      "metadata": {
        "id": "GDIMaRUGUWDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19: Evaluate baseline Llama performance (before any fine-tuning)\n",
        "print(\"EVALUATING BASELINE LLAMA PERFORMANCE (Before Fine-tuning)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"Loading fresh Llama 3 8B model (no fine-tuning)...\")\n",
        "\n",
        "# Load completely fresh model for baseline evaluation\n",
        "baseline_model, baseline_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(baseline_model)\n",
        "\n",
        "print(\"Fresh baseline model loaded\")\n",
        "\n",
        "def evaluate_baseline_llama(test_df, model, tokenizer):\n",
        "    \"\"\"Evaluate completely untrained Llama model.\"\"\"\n",
        "    print(f\"Evaluating baseline Llama on {len(test_df)} test samples...\")\n",
        "\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "    true_labels = []\n",
        "    failed_predictions = 0\n",
        "    raw_predictions = []\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        try:\n",
        "            # Use same instruction format but with untrained model\n",
        "            prompt = format_sms_instruction(row['message'], label=None)\n",
        "\n",
        "            inputs = baseline_tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=15,\n",
        "                    use_cache=True,\n",
        "                    do_sample=False,\n",
        "                    temperature=None,\n",
        "                    top_p=None,\n",
        "                    pad_token_id=baseline_tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            response = baseline_tokenizer.batch_decode(outputs)[0]\n",
        "            pred, confidence, raw_text = parse_llama_prediction(response)\n",
        "\n",
        "            # Convert confidence to probability distribution\n",
        "            if pred == 1:  # spam\n",
        "                prob_spam = confidence\n",
        "                prob_legit = 1 - confidence\n",
        "            else:  # legitimate\n",
        "                prob_legit = confidence\n",
        "                prob_spam = 1 - confidence\n",
        "\n",
        "            predictions.append(pred)\n",
        "            probabilities.append([prob_legit, prob_spam])\n",
        "            true_labels.append(row['label'])\n",
        "            raw_predictions.append(raw_text)\n",
        "\n",
        "            if (idx + 1) % 25 == 0:\n",
        "                print(f\"Processed {idx + 1}/{len(test_df)} samples...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_predictions += 1\n",
        "            predictions.append(0)  # Default to legitimate\n",
        "            probabilities.append([0.5, 0.5])\n",
        "            true_labels.append(row['label'])\n",
        "            raw_predictions.append(\"failed\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    predictions = np.array(predictions)\n",
        "    probabilities = np.array(probabilities)\n",
        "    true_labels = np.array(true_labels)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
        "    mcc = matthews_corrcoef(true_labels, predictions)\n",
        "    roc_auc = roc_auc_score(true_labels, probabilities[:, 1])\n",
        "    pr_auc = average_precision_score(true_labels, probabilities[:, 1])\n",
        "\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"BASELINE LLAMA 3 8B RESULTS (No Fine-tuning)\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Success rate: {(len(predictions) - failed_predictions) / len(predictions) * 100:.1f}%\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "    print(f\"MCC:       {mcc:.4f}\")\n",
        "    print(f\"ROC AUC:   {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC:    {pr_auc:.4f}\")\n",
        "\n",
        "    print(f\"Confusion Matrix:\")\n",
        "    print(f\"[[{cm[0,0]:3d} {cm[0,1]:3d}]  (Legitimate: True/False)\")\n",
        "    print(f\" [{cm[1,0]:3d} {cm[1,1]:3d}]]  (Spam: False/True)\")\n",
        "\n",
        "    # Show some example predictions\n",
        "    print(f\"\\nSample Baseline Predictions:\")\n",
        "    for i in range(min(3, len(test_df))):\n",
        "        message = test_df.iloc[i]['message']\n",
        "        true_label = \"spam\" if true_labels[i] == 1 else \"legitimate\"\n",
        "        pred_label = \"spam\" if predictions[i] == 1 else \"legitimate\"\n",
        "\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Message: {message[:60]}...\")\n",
        "        print(f\"True: {true_label} | Predicted: {pred_label}\")\n",
        "        print(f\"Raw prediction: '{raw_predictions[i]}'\")\n",
        "\n",
        "    return {\n",
        "        'Model': 'llama_3_8b_baseline',\n",
        "        'MCC': mcc,\n",
        "        'F1': f1,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'ROC_AUC': roc_auc,\n",
        "        'PR_AUC': pr_auc,\n",
        "        'failed_predictions': failed_predictions\n",
        "    }\n",
        "\n",
        "# Run baseline evaluation\n",
        "baseline_llama_results = evaluate_baseline_llama(test_df, baseline_model, baseline_tokenizer)\n",
        "\n",
        "# Clean up baseline model to save memory\n",
        "del baseline_model, baseline_tokenizer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nBaseline evaluation complete, model cleared from memory\")"
      ],
      "metadata": {
        "id": "itXYmUKpVFwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20: Complete performance comparison with baseline\n",
        "print(\"COMPLETE LLAMA PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"FULL PROGRESSION: Baseline → Limited Training → Full Training\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create comprehensive comparison\n",
        "comparison_data = {\n",
        "    'Baseline (No training)': baseline_llama_results,\n",
        "    'Limited (29% data)': llama_results,\n",
        "    'Full (100% data)': llama_results_full\n",
        "}\n",
        "\n",
        "metrics = ['MCC', 'F1', 'Accuracy', 'Precision', 'Recall', 'ROC_AUC', 'PR_AUC']\n",
        "\n",
        "print(f\"{'Stage':<25} {'MCC':<8} {'F1':<8} {'Accuracy':<8} {'Precision':<8} {'Recall':<8}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for stage, results in comparison_data.items():\n",
        "    print(f\"{stage:<25} {results['MCC']:<8.4f} {results['F1']:<8.4f} {results['Accuracy']:<8.4f} {results['Precision']:<8.4f} {results['Recall']:<8.4f}\")\n",
        "\n",
        "# Calculate improvements\n",
        "baseline_mcc = baseline_llama_results['MCC']\n",
        "limited_mcc = llama_results['MCC']\n",
        "full_mcc = llama_results_full['MCC']\n",
        "\n",
        "print(f\"\\nIMPROVEMENT ANALYSIS:\")\n",
        "print(f\"Baseline → Limited:  {limited_mcc - baseline_mcc:+.4f} MCC ({(limited_mcc - baseline_mcc)/abs(baseline_mcc)*100:+.1f}%)\")\n",
        "print(f\"Baseline → Full:     {full_mcc - baseline_mcc:+.4f} MCC ({(full_mcc - baseline_mcc)/abs(baseline_mcc)*100:+.1f}%)\")\n",
        "print(f\"Limited → Full:      {full_mcc - limited_mcc:+.4f} MCC ({(full_mcc - limited_mcc)/limited_mcc*100:+.1f}%)\")\n",
        "\n",
        "print(f\"\\nKEY INSIGHTS:\")\n",
        "if limited_mcc > baseline_mcc:\n",
        "    print(f\"Fine-tuning works: {limited_mcc - baseline_mcc:.4f} MCC improvement with just 29% data\")\n",
        "else:\n",
        "    print(f\"Fine-tuning had minimal impact: {limited_mcc - baseline_mcc:.4f} MCC change\")\n",
        "\n",
        "if full_mcc > limited_mcc:\n",
        "    print(f\"More data helps: Additional {full_mcc - limited_mcc:.4f} MCC with full dataset\")\n",
        "else:\n",
        "    print(f\"Diminishing returns: {limited_mcc - full_mcc:.4f} MCC decline with more data\")\n",
        "\n",
        "# Save complete analysis\n",
        "complete_analysis = {\n",
        "    \"llama_performance_progression\": {\n",
        "        \"baseline_no_training\": {\n",
        "            \"mcc\": float(baseline_llama_results['MCC']),\n",
        "            \"f1\": float(baseline_llama_results['F1']),\n",
        "            \"training_time\": 0,\n",
        "            \"data_samples\": 0\n",
        "        },\n",
        "        \"limited_training_29_percent\": {\n",
        "            \"mcc\": float(llama_results['MCC']),\n",
        "            \"f1\": float(llama_results['F1']),\n",
        "            \"training_time\": 4.74,\n",
        "            \"data_samples\": 200,\n",
        "            \"improvement_vs_baseline\": float(limited_mcc - baseline_mcc)\n",
        "        },\n",
        "        \"full_training_100_percent\": {\n",
        "            \"mcc\": float(llama_results_full['MCC']),\n",
        "            \"f1\": float(llama_results_full['F1']),\n",
        "            \"training_time\": 16.03,\n",
        "            \"data_samples\": 700,\n",
        "            \"improvement_vs_baseline\": float(full_mcc - baseline_mcc),\n",
        "            \"improvement_vs_limited\": float(full_mcc - limited_mcc)\n",
        "        }\n",
        "    },\n",
        "    \"conclusions\": {\n",
        "        \"fine_tuning_effective\": limited_mcc > baseline_mcc,\n",
        "        \"optimal_data_amount\": \"29% (200 samples)\" if limited_mcc >= full_mcc else \"100% (700 samples)\",\n",
        "        \"best_model\": \"limited\" if limited_mcc >= full_mcc else \"full\",\n",
        "        \"efficiency_winner\": \"29% model: better performance per training minute\"\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f\"{llama_results_dir}/complete_progression_analysis.json\", \"w\") as f:\n",
        "    json.dump(complete_analysis, f, indent=4)\n",
        "\n",
        "print(f\"\\nFINAL RECOMMENDATION:\")\n",
        "best_stage = max(comparison_data.keys(), key=lambda k: comparison_data[k]['MCC'])\n",
        "best_mcc = comparison_data[best_stage]['MCC']\n",
        "\n",
        "print(f\"**Best Llama model: {best_stage}** (MCC: {best_mcc:.4f})\")\n",
        "print(f\"Compared to your best overall model (roberta_frozen: 0.8750): {best_mcc - 0.8750:.4f} difference\")\n",
        "\n",
        "print(f\"\\nComplete analysis saved to {llama_results_dir}/complete_progression_analysis.json\")"
      ],
      "metadata": {
        "id": "UcYJUFEEWlvj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
