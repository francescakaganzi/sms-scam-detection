{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNejCG+KWtM5hfVbzN8Bh//"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7uczCOksTt9"
      },
      "outputs": [],
      "source": [
        "# SMS Scam Detection - Llama Model Fine-tuning\n",
        "# =============================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    LlamaTokenizer,\n",
        "    LlamaForSequenceClassification,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
        ")\n",
        "import evaluate\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set plotting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING for better error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Ignore specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n",
        "\n",
        "# Setup Hugging Face token for model access\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Set up project paths\n",
        "project_dir = '/content/drive/MyDrive/sms-scam-detection'\n",
        "os.chdir(project_dir)\n",
        "\n",
        "data_dir = \"data/processed/\"\n",
        "model_dir = \"models/llm_os/\"  # Open Source LLMs\n",
        "results_dir = \"results/\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(results_dir, \"metrics\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(results_dir, \"visualizations\"), exist_ok=True)\n",
        "\n",
        "# Setup label mapping\n",
        "id2label = {0: \"Legitimate\", 1: \"Scam\"}\n",
        "label2id = {\"Legitimate\": 0, \"Scam\": 1}\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load and prepare the dataset.\"\"\"\n",
        "    train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
        "    val_df = pd.read_csv(os.path.join(data_dir, \"val.csv\"))\n",
        "    test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
        "\n",
        "    print(f\"Loaded data: Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "    # Print class distribution\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    for name, df in [(\"Training\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]:\n",
        "        print(f\"{name} Set:\")\n",
        "        print(df['label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "    # Calculate imbalance ratio\n",
        "    train_neg_count = (train_df['label'] == 0).sum()\n",
        "    train_pos_count = (train_df['label'] == 1).sum()\n",
        "    imbalance_ratio = train_neg_count / train_pos_count if train_pos_count > 0 else float('inf')\n",
        "    print(f\"\\nImbalance ratio (legitimate:scam): {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "    # Convert to Hugging Face datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    dataset_dict = DatasetDict({\n",
        "        \"train\": train_dataset,\n",
        "        \"valid\": val_dataset,\n",
        "        \"test\": test_dataset\n",
        "    })\n",
        "\n",
        "    print(\"\\nDataset structure:\")\n",
        "    print(dataset_dict)\n",
        "\n",
        "    return dataset_dict\n",
        "\n",
        "def preprocess_function(examples, tokenizer, max_length=128):\n",
        "    \"\"\"Tokenize and preprocess text examples.\"\"\"\n",
        "    text_column = \"cleaned_text\" if \"cleaned_text\" in examples else \"message\"\n",
        "    return tokenizer(\n",
        "        examples[text_column],\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "def tokenize_data(dataset_dict, tokenizer, max_length=128):\n",
        "    \"\"\"Tokenize all splits in a dataset dictionary.\"\"\"\n",
        "    return dataset_dict.map(\n",
        "        lambda examples: preprocess_function(examples, tokenizer, max_length),\n",
        "        batched=True\n",
        "    )\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics for model predictions.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "    positive_class_probs = probabilities[:, 1]\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predicted_classes)\n",
        "    precision = precision_score(labels, predicted_classes)\n",
        "    recall = recall_score(labels, predicted_classes)\n",
        "    f1 = f1_score(labels, predicted_classes)\n",
        "    mcc = matthews_corrcoef(labels, predicted_classes)\n",
        "    roc_auc = roc_auc_score(labels, positive_class_probs)\n",
        "    pr_auc = average_precision_score(labels, positive_class_probs)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"mcc\": mcc,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"pr_auc\": pr_auc\n",
        "    }\n",
        "\n",
        "def create_data_collator(tokenizer):\n",
        "    \"\"\"Create a data collator for batching examples.\"\"\"\n",
        "    return DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, model_name):\n",
        "    \"\"\"Plot confusion matrix.\"\"\"\n",
        "    results_dir = Path(f\"results/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(results_dir / \"confusion_matrix.png\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curve(fpr, tpr, roc_auc, model_name):\n",
        "    \"\"\"Plot ROC curve.\"\"\"\n",
        "    results_dir = Path(f\"results/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(f\"{model_name} - ROC Curve\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(results_dir / \"roc_curve.png\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_pr_curve(precision, recall, pr_auc, model_name):\n",
        "    \"\"\"Plot precision-recall curve.\"\"\"\n",
        "    results_dir = Path(f\"results/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.step(recall, precision, color='green', lw=2, where='post',\n",
        "             label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
        "    plt.fill_between(recall, precision, alpha=0.2, color='green', step='post')\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(f\"{model_name} - Precision-Recall Curve\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(results_dir / \"pr_curve.png\")\n",
        "    plt.show()\n",
        "\n",
        "def setup_quantization_config():\n",
        "    \"\"\"Setup 4-bit quantization configuration for memory efficiency.\"\"\"\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    return quantization_config\n",
        "\n",
        "def setup_lora_config(model, lora_r=16, lora_alpha=32, lora_dropout=0.1):\n",
        "    \"\"\"Setup LoRA configuration for parameter-efficient fine-tuning.\"\"\"\n",
        "    # Determine target modules based on model architecture\n",
        "    if \"llama\" in model.config._name_or_path.lower():\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "    elif \"mistral\" in model.config._name_or_path.lower():\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "    elif \"falcon\" in model.config._name_or_path.lower():\n",
        "        target_modules = [\"query_key_value\"]\n",
        "    elif \"phi\" in model.config._name_or_path.lower():\n",
        "        target_modules = [\"Wqkv\", \"out_proj\"]\n",
        "    else:\n",
        "        # Default for most decoder architectures\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "    # Define LoRA config\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=lora_dropout,\n",
        "        target_modules=target_modules,\n",
        "        bias=\"none\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    lora_model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print trainable parameters comparison\n",
        "    print(\"Trainable parameters:\")\n",
        "    trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in lora_model.parameters())\n",
        "    print(f\"  - LoRA trainable params: {trainable_params:,} ({trainable_params/total_params:.2%} of total)\")\n",
        "\n",
        "    return lora_model\n",
        "\n",
        "def train_model(model, tokenizer, tokenized_data, training_args, model_name):\n",
        "    \"\"\"Train a model and evaluate it on validation data.\"\"\"\n",
        "    print(f\"\\n=== Training {model_name} ===\")\n",
        "\n",
        "    # Create results directory\n",
        "    results_dir = Path(f\"results/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Set up trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_data[\"train\"],\n",
        "        eval_dataset=tokenized_data[\"valid\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=create_data_collator(tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    train_result = trainer.train()\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    training_time_minutes = round(training_time / 60, 2)\n",
        "\n",
        "    print(f\"\\nTraining completed in {training_time_minutes} minutes\")\n",
        "    print(f\"Training Loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"\\nValidation Results:\")\n",
        "    for key, value in eval_results.items():\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model_save_path = results_dir / \"final_model\"\n",
        "    trainer.save_model(str(model_save_path))\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    # Save training metrics\n",
        "    metrics_file = results_dir / \"training_metrics.json\"\n",
        "    with open(metrics_file, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"training_time_seconds\": training_time,\n",
        "            \"training_time_minutes\": training_time_minutes,\n",
        "            \"final_train_loss\": train_result.training_loss,\n",
        "            \"eval_results\": eval_results\n",
        "        }, f, indent=4)\n",
        "\n",
        "    return trainer\n",
        "\n",
        "def evaluate_model(trainer, tokenized_test_data, model_name):\n",
        "    \"\"\"Evaluate a trained model on test data.\"\"\"\n",
        "    print(f\"\\n=== Evaluating {model_name} on Test Set ===\")\n",
        "\n",
        "    # Make predictions on test set\n",
        "    test_results = trainer.predict(tokenized_test_data)\n",
        "\n",
        "    predictions = test_results.predictions\n",
        "    labels = test_results.label_ids\n",
        "\n",
        "    # Calculate probabilities and predicted classes\n",
        "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "    positive_class_probs = probabilities[:, 1]\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, predicted_classes)\n",
        "    precision = precision_score(labels, predicted_classes)\n",
        "    recall = recall_score(labels, predicted_classes)\n",
        "    f1 = f1_score(labels, predicted_classes)\n",
        "    mcc = matthews_corrcoef(labels, predicted_classes)\n",
        "    roc_auc = roc_auc_score(labels, positive_class_probs)\n",
        "    pr_auc = average_precision_score(labels, positive_class_probs)\n",
        "    cm = confusion_matrix(labels, predicted_classes)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nTest Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "\n",
        "    # Print detailed classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    class_names = list(id2label.values())\n",
        "    report = classification_report(labels, predicted_classes, target_names=class_names)\n",
        "    print(report)\n",
        "\n",
        "    # Save results\n",
        "    results_dir = Path(f\"results/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save metrics as CSV\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Model': [model_name],\n",
        "        'Accuracy': [accuracy],\n",
        "        'Precision': [precision],\n",
        "        'Recall': [recall],\n",
        "        'F1': [f1],\n",
        "        'MCC': [mcc],\n",
        "        'ROC_AUC': [roc_auc],\n",
        "        'PR_AUC': [pr_auc]\n",
        "    })\n",
        "    metrics_df.to_csv(results_dir / \"test_metrics.csv\", index=False)\n",
        "\n",
        "    # Save classification report\n",
        "    with open(results_dir / \"classification_report.txt\", \"w\") as f:\n",
        "        f.write(report)\n",
        "\n",
        "    # Create visualizations\n",
        "    plot_confusion_matrix(cm, class_names, model_name)\n",
        "\n",
        "    # ROC curve\n",
        "    fpr, tpr, _ = roc_curve(labels, positive_class_probs)\n",
        "    plot_roc_curve(fpr, tpr, roc_auc, model_name)\n",
        "\n",
        "    # Precision-Recall curve\n",
        "    precision_points, recall_points, _ = precision_recall_curve(labels, positive_class_probs)\n",
        "    plot_pr_curve(precision_points, recall_points, pr_auc, model_name)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"mcc\": mcc,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"pr_auc\": pr_auc,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"predicted_classes\": predicted_classes,\n",
        "        \"probabilities\": probabilities\n",
        "    }\n",
        "\n",
        "def predict_examples(model, tokenizer, examples, model_name):\n",
        "    \"\"\"Predict on a list of example messages.\"\"\"\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    # Process each example\n",
        "    for text in examples:\n",
        "        # Tokenize the text\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        ).to(device)\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Calculate probabilities and prediction\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "        prob_score = probs[0][predicted_class].item()\n",
        "\n",
        "        results.append({\n",
        "            \"text\": text,\n",
        "            \"predicted_class\": id2label[predicted_class],\n",
        "            \"confidence\": prob_score\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame for easy viewing\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    print(f\"\\n=== Example Predictions with {model_name} ===\")\n",
        "    for i, row in results_df.iterrows():\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Text: {row['text']}\")\n",
        "        print(f\"Prediction: {row['predicted_class']}\")\n",
        "        print(f\"Confidence: {row['confidence']:.4f}\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    # Load data\n",
        "    dataset_dict = load_data()\n",
        "\n",
        "    # Available Llama-based models (adjust based on availability and resources)\n",
        "    model_configs = [\n",
        "        {\n",
        "            \"name\": \"llama2_7b_lora\",\n",
        "            \"model_path\": \"meta-llama/Llama-2-7b-hf\",\n",
        "            \"use_quantization\": True,\n",
        "            \"use_lora\": True\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    # Process each model configuration\n",
        "    for config in model_configs:\n",
        "        model_name = config[\"name\"]\n",
        "        model_path = config[\"model_path\"]\n",
        "        use_quantization = config.get(\"use_quantization\", False)\n",
        "        use_lora = config.get(\"use_lora\", False)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {model_name}\")\n",
        "        print(f\"Model: {model_path}\")\n",
        "        print(f\"Quantization: {use_quantization}\")\n",
        "        print(f\"LoRA: {use_lora}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            # Setup quantization config if needed\n",
        "            quantization_config = setup_quantization_config() if use_quantization else None\n",
        "\n",
        "            # Load tokenizer\n",
        "            print(\"Loading tokenizer...\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\n",
        "                model_path,\n",
        "                use_auth_token=hf_token,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "            # Add padding token if not present\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "            # Load model\n",
        "            print(\"Loading model...\")\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                model_path,\n",
        "                num_labels=2,\n",
        "                id2label=id2label,\n",
        "                label2id=label2id,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\" if use_quantization else None,\n",
        "                torch_dtype=torch.float16 if use_quantization else torch.float32,\n",
        "                use_auth_token=hf_token,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "            # Prepare model for training if using quantization\n",
        "            if use_quantization:\n",
        "                model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "            # Apply LoRA if specified\n",
        "            if use_lora:\n",
        "                print(\"Applying LoRA configuration...\")\n",
        "                model = setup_lora_config(model)\n",
        "\n",
        "            # Move to device if not using quantization\n",
        "            if not use_quantization:\n",
        "                model = model.to(device)\n",
        "\n",
        "            # Tokenize data\n",
        "            print(\"Tokenizing data...\")\n",
        "            tokenized_data = tokenize_data(dataset_dict, tokenizer, max_length=128)\n",
        "\n",
        "            # Define training arguments\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=f\"results/{model_name}/checkpoints\",\n",
        "                num_train_epochs=3,\n",
        "                per_device_train_batch_size=4,  # Small batch size for memory efficiency\n",
        "                per_device_eval_batch_size=4,\n",
        "                gradient_accumulation_steps=4,  # Effective batch size = 4*4 = 16\n",
        "                warmup_steps=100,\n",
        "                weight_decay=0.01,\n",
        "                logging_dir=f\"results/{model_name}/logs\",\n",
        "                logging_steps=50,\n",
        "                eval_strategy=\"epoch\",\n",
        "                save_strategy=\"epoch\",\n",
        "                save_total_limit=1,\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"mcc\",\n",
        "                greater_is_better=True,\n",
        "                push_to_hub=False,\n",
        "                dataloader_pin_memory=False,\n",
        "                remove_unused_columns=False,\n",
        "                fp16=True,  # Use mixed precision for memory efficiency\n",
        "                gradient_checkpointing=True,  # Trade compute for memory\n",
        "            )\n",
        "\n",
        "            # Train the model\n",
        "            trainer = train_model(model, tokenizer, tokenized_data, training_args, model_name)\n",
        "\n",
        "            # Evaluate on test set\n",
        "            test_results = evaluate_model(trainer, tokenized_data[\"test\"], model_name)\n",
        "\n",
        "            # Store results\n",
        "            all_results[model_name] = test_results\n",
        "\n",
        "            # Test on example messages\n",
        "            example_messages = [\n",
        "                \"Congratulations! You've won ugx100000. Click here to claim: www.example.com\",\n",
        "                \"Your account has been suspended. Please verify your identity by sending your PIN.\",\n",
        "                \"Hi, just checking if we're still meeting for lunch tomorrow at 12?\",\n",
        "                \"URGENT: Your payment of ugx55000 has been processed. If this was not you, call immediately.\",\n",
        "                \"Your package will be delivered tomorrow between 10am and 2pm. No signature required.\",\n",
        "                \"You have been selected to win ugx500000! Call now to claim your prize before it expires!\",\n",
        "                \"Meeting postponed to next Wednesday at 3pm. Please confirm if you can attend.\"\n",
        "            ]\n",
        "\n",
        "            # Make predictions on examples\n",
        "            example_predictions = predict_examples(model, tokenizer, example_messages, model_name)\n",
        "\n",
        "            # Save example predictions\n",
        "            example_predictions.to_csv(f\"results/{model_name}/example_predictions.csv\", index=False)\n",
        "\n",
        "            # Clean up GPU memory\n",
        "            del model\n",
        "            del trainer\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {model_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Compile and save results\n",
        "    if all_results:\n",
        "        # Create comparison DataFrame\n",
        "        comparison_results = []\n",
        "        for model_name, results in all_results.items():\n",
        "            comparison_results.append({\n",
        "                \"Model\": model_name,\n",
        "                \"Model_Type\": \"LLM_OS\",  # Open Source LLM\n",
        "                \"Accuracy\": results[\"accuracy\"],\n",
        "                \"Precision\": results[\"precision\"],\n",
        "                \"Recall\": results[\"recall\"],\n",
        "                \"F1\": results[\"f1\"],\n",
        "                \"MCC\": results[\"mcc\"],\n",
        "                \"ROC_AUC\": results[\"roc_auc\"],\n",
        "                \"PR_AUC\": results[\"pr_auc\"]\n",
        "            })\n",
        "\n",
        "        comparison_df = pd.DataFrame(comparison_results)\n",
        "        comparison_df = comparison_df.sort_values(\"MCC\", ascending=False)\n",
        "\n",
        "        # Save Llama results\n",
        "        comparison_df.to_csv(\"results/metrics/llama_results.csv\", index=False)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"LLAMA MODEL RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(comparison_df.to_string(index=False))\n",
        "\n",
        "        # Load and combine with previous results if available\n",
        "        all_models_path = \"results/metrics/final_all_models_comparison.csv\"\n",
        "\n",
        "        try:\n",
        "            existing_df = pd.read_csv(all_models_path)\n",
        "\n",
        "            # Combine results\n",
        "            combined_df = pd.concat([existing_df, comparison_df], ignore_index=True)\n",
        "            combined_df = combined_df.sort_values(\"MCC\", ascending=False)\n",
        "\n",
        "            # Save updated results\n",
        "            combined_df.to_csv(all_models_path, index=False)\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"UPDATED FINAL COMPARISON OF ALL MODELS\")\n",
        "            print(\"=\"*80)\n",
        "            key_columns = [\"Model\", \"Model_Type\", \"MCC\", \"F1\", \"ROC_AUC\", \"PR_AUC\"]\n",
        "            available_columns = [col for col in key_columns if col in combined_df.columns]\n",
        "            print(combined_df[available_columns].head(10).to_string(index=False))\n",
        "\n",
        "            # Create updated visualization\n",
        "            plt.figure(figsize=(16, 10))\n",
        "\n",
        "            # Focus on key metrics\n",
        "            key_metrics = [\"F1\", \"MCC\", \"ROC_AUC\", \"PR_AUC\"]\n",
        "            available_metrics = [col for col in key_metrics if col in combined_df.columns]\n",
        "\n",
        "            melted_df = combined_df.melt(\n",
        "                id_vars=[\"Model\"],\n",
        "                value_vars=available_metrics,\n",
        "                var_name=\"Metric\",\n",
        "                value_name=\"Score\"\n",
        "            )\n",
        "\n",
        "            sns.barplot(data=melted_df, x=\"Model\", y=\"Score\", hue=\"Metric\")\n",
        "            plt.title(\"Final Performance Comparison: All Models Including Llama\")\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.ylim(0, 1)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(\"results/visualizations/final_all_models_with_llama.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            # Print top 5 models\n",
        "            print(f\"\\nüèÜ TOP 5 MODELS BY MCC:\")\n",
        "            top_5 = combined_df.head(5)\n",
        "            for i, (_, row) in enumerate(top_5.iterrows(), 1):\n",
        "                model_type = row.get('Model_Type', 'Unknown')\n",
        "                print(f\"{i}. {row['Model']} ({model_type}): MCC = {row['MCC']:.4f}, F1 = {row['F1']:.4f}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(\"No previous model results found for comparison.\")\n",
        "\n",
        "            # Create new comparison file with just this model\n",
        "            new_df = pd.DataFrame({\n",
        "                'Model': [model_name],\n",
        "                'Model_Type': ['LLM_OS'],  # Open Source LLM\n",
        "                'Accuracy': [results['accuracy']],\n",
        "                'Precision': [results['precision']],\n",
        "                'Recall': [results['recall']],\n",
        "                'F1': [results['f1']],\n",
        "                'MCC': [results['mcc']],\n",
        "                'ROC_AUC': [results['roc_auc']],\n",
        "                'PR_AUC': [results['pr_auc']]\n",
        "            })\n",
        "\n",
        "            # Save results\n",
        "            new_df.to_csv(all_models_path, index=False)\n",
        "\n",
        "            print(\"\\nModel performance:\")\n",
        "            print(new_df[['Model', 'Model_Type', 'MCC', 'F1', 'Accuracy']])\n",
        "\n",
        "    print(f\"\\nLlama model fine-tuning completed successfully!\")\n",
        "    print(f\"Results saved to: results/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}