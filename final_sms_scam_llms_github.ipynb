{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kSVhtpF1t72"
      },
      "outputs": [],
      "source": [
        "# 1: Set up environment and mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up project paths\n",
        "import os\n",
        "\n",
        "# Define project directory\n",
        "project_dir = '/content/drive/MyDrive/final-sms-scam-detection'\n",
        "os.chdir(project_dir)\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Libraries\n",
        "!pip install torch transformers datasets evaluate peft optuna matplotlib seaborn scikit-learn pandas numpy captum lime -q"
      ],
      "metadata": {
        "id": "JUBGqoCh1--s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3: Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from captum.attr import IntegratedGradients\n",
        "from pathlib import Path\n",
        "import evaluate\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    get_peft_model,\n",
        ")\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
        ")\n",
        "import warnings\n",
        "import optuna"
      ],
      "metadata": {
        "id": "R4r574X82KSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set plotting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)"
      ],
      "metadata": {
        "id": "vjvzTj4r2dQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "5T7B38sO2hRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING for better error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Ignore specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")"
      ],
      "metadata": {
        "id": "_cpJaIAv2jyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4: Load data\n",
        "# Define paths\n",
        "data_dir = \"data/processed/\"\n",
        "model_dir = \"models/llm/\"\n",
        "results_dir = \"results/\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(results_dir, \"metrics\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(results_dir, \"visualizations\"), exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
        "val_df = pd.read_csv(os.path.join(data_dir, \"val.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
        "\n",
        "print(f\"Loaded data: Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "xfA2HcRT2o1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your data structure\n",
        "print(\"\\nData structure:\")\n",
        "print(\"Train columns:\", train_df.columns.tolist())\n",
        "print(\"Sample row:\", train_df.iloc[0])\n",
        "\n",
        "# Check class distribution (for balancing)\n",
        "print(\"\\nClass distribution:\")\n",
        "for split_name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
        "    class_counts = df['label'].value_counts()\n",
        "    print(f\"{split_name}: {class_counts.to_dict()}\")\n",
        "    print(f\"  Imbalance ratio: {class_counts.max() / class_counts.min():.2f}\")"
      ],
      "metadata": {
        "id": "hViS5qpik8PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup label mapping for consistency\n",
        "id2label = {0: \"Legitimate\", 1: \"Scam\"}\n",
        "label2id = {\"Legitimate\": 0, \"Scam\": 1}\n",
        "\n",
        "# Convert pandas DataFrames to Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Create a DatasetDict to group the datasets\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"valid\": val_dataset,\n",
        "    \"test\": test_dataset\n",
        "})\n",
        "\n",
        "print(\"\\nDataset structure:\")\n",
        "print(dataset_dict)"
      ],
      "metadata": {
        "id": "c_bnONYd2zuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5: Define helper functions\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_function(examples, tokenizer, max_length=128):\n",
        "    \"\"\"Tokenize and preprocess text examples.\n",
        "\n",
        "    Args:\n",
        "        examples: Dictionary of examples to process.\n",
        "        tokenizer: Tokenizer to use for tokenization.\n",
        "        max_length: Maximum sequence length.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of tokenized examples.\n",
        "    \"\"\"\n",
        "    # FIXED: Use \"message\" column (original text) instead of cleaned_text\n",
        "    text_column = \"message\" if \"message\" in examples else \"cleaned_text\"\n",
        "    return tokenizer(\n",
        "        examples[text_column],\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "# Function to tokenize all datasets\n",
        "def tokenize_data(dataset_dict, tokenizer, max_length=128):\n",
        "    \"\"\"Tokenize all splits in a dataset dictionary.\n",
        "\n",
        "    Args:\n",
        "        dataset_dict: Dictionary of datasets to tokenize.\n",
        "        tokenizer: Tokenizer to use.\n",
        "        max_length: Maximum sequence length.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of tokenized datasets.\n",
        "    \"\"\"\n",
        "    return dataset_dict.map(\n",
        "        lambda examples: preprocess_function(examples, tokenizer, max_length),\n",
        "        batched=True\n",
        "    )\n",
        "\n",
        "# Function to compute evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics for model predictions.\n",
        "\n",
        "    Args:\n",
        "        eval_pred: Tuple of (predictions, labels).\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metrics.\n",
        "    \"\"\"\n",
        "    # Get predictions and labels\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "\n",
        "    # Use probabilities of the positive class for ROC AUC and PR AUC\n",
        "    positive_class_probs = probabilities[:, 1]\n",
        "\n",
        "    # Predict most probable class\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, predicted_classes)\n",
        "    precision = precision_score(labels, predicted_classes)\n",
        "    recall = recall_score(labels, predicted_classes)\n",
        "    f1 = f1_score(labels, predicted_classes)\n",
        "    mcc = matthews_corrcoef(labels, predicted_classes)\n",
        "    roc_auc = roc_auc_score(labels, positive_class_probs)\n",
        "    pr_auc = average_precision_score(labels, positive_class_probs)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"mcc\": mcc,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"pr_auc\": pr_auc\n",
        "    }\n",
        "\n",
        "# Function to create data collator\n",
        "def create_data_collator(tokenizer):\n",
        "    \"\"\"Create a data collator for batching examples.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: Tokenizer to use.\n",
        "\n",
        "    Returns:\n",
        "        Data collator object.\n",
        "    \"\"\"\n",
        "    return DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "T20Ji3xX23L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            # Apply class weights to handle imbalance\n",
        "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        else:\n",
        "            loss = outputs.loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "Je8Fz-_enoEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_weights(dataset, device):\n",
        "    \"\"\"Calculate class weights for imbalanced dataset.\"\"\"\n",
        "    # Extract labels from dataset\n",
        "    labels = [example['label'] for example in dataset]\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(labels),\n",
        "        y=labels\n",
        "    )\n",
        "\n",
        "    # Convert to tensor\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "    print(f\"Class weights: Legitimate={class_weights[0]:.3f}, Scam={class_weights[1]:.3f}\")\n",
        "    return class_weights_tensor"
      ],
      "metadata": {
        "id": "Po8hel-1n7Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_baseline_model(model_name, tokenizer, test_dataset, device):\n",
        "    \"\"\"Evaluate baseline model performance before fine-tuning.\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Evaluating baseline model: {model_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Load pre-trained model without fine-tuning\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    ).to(device)\n",
        "\n",
        "    # Set up minimal training args for evaluation\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./temp_baseline\",\n",
        "        per_device_eval_batch_size=32,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # Create trainer for evaluation\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=create_data_collator(tokenizer),\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    results = trainer.evaluate(test_dataset)\n",
        "\n",
        "    print(f\"Baseline Results for {model_name}:\")\n",
        "    for metric, value in results.items():\n",
        "        if metric.startswith('eval_'):\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    # Clean up\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "kcuspWhjt3BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explainability functions\n",
        "def setup_lime_explainer(model, tokenizer, device, class_names=['Legitimate', 'Scam']):\n",
        "    \"\"\"Set up LIME explainer for model interpretability.\"\"\"\n",
        "\n",
        "    def predict_proba(texts):\n",
        "        \"\"\"Prediction function for LIME.\"\"\"\n",
        "        # Tokenize texts\n",
        "        inputs = tokenizer(\n",
        "            texts,\n",
        "            max_length=128,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        # Get predictions\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "        return probabilities.cpu().numpy()\n",
        "\n",
        "    # Create LIME explainer\n",
        "    explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "    return explainer, predict_proba"
      ],
      "metadata": {
        "id": "W34VsEEDuqjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_prediction(explainer, predict_fn, text, num_features=10):\n",
        "    \"\"\"Generate explanation for a single prediction.\"\"\"\n",
        "    explanation = explainer.explain_instance(\n",
        "        text,\n",
        "        predict_fn,\n",
        "        num_features=num_features,\n",
        "        num_samples=1000\n",
        "    )\n",
        "    return explanation"
      ],
      "metadata": {
        "id": "wrYpPJdxuuUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize training metrics\n",
        "# Fixed visualization function to handle metrics with different lengths\n",
        "def visualize_training_metrics(metrics, model_name):\n",
        "    \"\"\"Visualize training and evaluation metrics with improved handling of different length metrics.\n",
        "\n",
        "    Args:\n",
        "        metrics: Dictionary of training metrics.\n",
        "        model_name: Name of the model for saving files.\n",
        "    \"\"\"\n",
        "    # Create directory for plots\n",
        "    results_dir = Path(f\"results/visualizations/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Extract metrics, handling missing values\n",
        "    train_loss = [x['loss'] for x in metrics if 'loss' in x]\n",
        "    eval_loss = [x['eval_loss'] for x in metrics if 'eval_loss' in x]\n",
        "\n",
        "    # Make sure we have data to plot\n",
        "    if train_loss and eval_loss:\n",
        "        # Use the minimum length to avoid dimension mismatch\n",
        "        min_length = min(len(train_loss), len(eval_loss))\n",
        "        epochs = range(1, min_length + 1)\n",
        "\n",
        "        # Plot training and evaluation loss\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(epochs, train_loss[:min_length], label='Training Loss', marker='o')\n",
        "        plt.plot(epochs, eval_loss[:min_length], label='Validation Loss', marker='o')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss vs Validation Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(results_dir / \"loss_curve.png\")\n",
        "        plt.show()\n",
        "\n",
        "    # Extract evaluation metrics\n",
        "    if 'eval_accuracy' in metrics[0]:\n",
        "        eval_accuracy = [x.get('eval_accuracy', None) for x in metrics if 'eval_loss' in x]\n",
        "        eval_f1 = [x.get('eval_f1', None) for x in metrics if 'eval_loss' in x]\n",
        "        eval_mcc = [x.get('eval_mcc', None) for x in metrics if 'eval_loss' in x]\n",
        "\n",
        "        # Remove None values\n",
        "        eval_accuracy = [x for x in eval_accuracy if x is not None]\n",
        "        eval_f1 = [x for x in eval_f1 if x is not None]\n",
        "        eval_mcc = [x for x in eval_mcc if x is not None]\n",
        "\n",
        "        # Plot each metric separately to avoid dimension issues\n",
        "        if eval_accuracy:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(range(1, len(eval_accuracy) + 1), eval_accuracy, label='Accuracy', marker='o', color='blue')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Score')\n",
        "            plt.title('Validation Accuracy')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(results_dir / \"accuracy_curve.png\")\n",
        "            plt.show()\n",
        "\n",
        "        if eval_f1:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(range(1, len(eval_f1) + 1), eval_f1, label='F1 Score', marker='o', color='green')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Score')\n",
        "            plt.title('Validation F1 Score')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(results_dir / \"f1_curve.png\")\n",
        "            plt.show()\n",
        "\n",
        "        if eval_mcc:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(range(1, len(eval_mcc) + 1), eval_mcc, label='MCC', marker='o', color='purple')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Score')\n",
        "            plt.title('Validation MCC')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(results_dir / \"mcc_curve.png\")\n",
        "            plt.show()\n",
        "\n",
        "# Function to visualize confusion matrix\n",
        "def plot_confusion_matrix(cm, classes, model_name):\n",
        "    \"\"\"Plot confusion matrix.\n",
        "\n",
        "    Args:\n",
        "        cm: Confusion matrix to plot.\n",
        "        classes: List of class names.\n",
        "        model_name: Name of the model for saving files.\n",
        "    \"\"\"\n",
        "    results_dir = Path(f\"results/visualizations/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(results_dir / \"confusion_matrix.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot ROC curve\n",
        "def plot_roc_curve(fpr, tpr, roc_auc, model_name):\n",
        "    \"\"\"Plot ROC curve.\n",
        "\n",
        "    Args:\n",
        "        fpr: False positive rates.\n",
        "        tpr: True positive rates.\n",
        "        roc_auc: ROC AUC score.\n",
        "        model_name: Name of the model for saving files.\n",
        "    \"\"\"\n",
        "    results_dir = Path(f\"results/visualizations/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(results_dir / \"roc_curve.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot PR curve\n",
        "def plot_pr_curve(precision, recall, pr_auc, model_name):\n",
        "    \"\"\"Plot precision-recall curve.\n",
        "\n",
        "    Args:\n",
        "        precision: Precision values.\n",
        "        recall: Recall values.\n",
        "        pr_auc: PR AUC score.\n",
        "        model_name: Name of the model for saving files.\n",
        "    \"\"\"\n",
        "    results_dir = Path(f\"results/visualizations/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.step(recall, precision, color='green', lw=2, where='post',\n",
        "             label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
        "    plt.fill_between(recall, precision, alpha=0.2, color='green', step='post')\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(results_dir / \"pr_curve.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rl20lOWSJ_Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, tokenizer, tokenized_data, training_args, model_name):\n",
        "    \"\"\"Train a model and evaluate it on validation data.\n",
        "    Args:\n",
        "        model: Model to train.\n",
        "        tokenizer: Tokenizer to use.\n",
        "        tokenized_data: Tokenized dataset.\n",
        "        training_args: Training arguments.\n",
        "        model_name: Name of the model for saving.\n",
        "    Returns:\n",
        "        Trained Trainer object.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Training {model_name} ===\")\n",
        "\n",
        "    # Create results directory\n",
        "    results_dir = Path(f\"results/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Calculate class weights for handling imbalance\n",
        "    class_weights = calculate_class_weights(tokenized_data[\"train\"], device)\n",
        "\n",
        "    # Count trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params / total_params:.2%} of total)\")\n",
        "\n",
        "    # Use WeightedTrainer instead of regular Trainer\n",
        "    trainer = WeightedTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_data[\"train\"],\n",
        "        eval_dataset=tokenized_data[\"valid\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=create_data_collator(tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "        class_weights=class_weights\n",
        "    )\n",
        "\n",
        "    # Start timing the training process\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    # End timing and calculate training time\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    training_time_minutes = round(training_time / 60, 2)\n",
        "\n",
        "    # Print training time and metrics\n",
        "    print(f\"\\nTraining completed in {training_time_minutes} minutes\")\n",
        "    print(f\"Training Loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "    # Run evaluation\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"\\nValidation Results:\")\n",
        "    for key, value in eval_results.items():\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model_save_path = results_dir / \"final_model\"\n",
        "    trainer.save_model(str(model_save_path))\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    # Visualize training metrics\n",
        "    visualize_training_metrics(trainer.state.log_history, model_name)\n",
        "\n",
        "    # Save training metrics\n",
        "    metrics_file = results_dir / \"training_metrics.json\"\n",
        "    with open(metrics_file, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"training_time_seconds\": training_time,\n",
        "            \"training_time_minutes\": training_time_minutes,\n",
        "            \"final_train_loss\": train_result.training_loss,\n",
        "            \"eval_results\": eval_results\n",
        "        }, f, indent=4)\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "mee37r9TLM07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(trainer, tokenized_test_data, model_name):\n",
        "    \"\"\"Evaluate a trained model on test data.\n",
        "    Args:\n",
        "        trainer: Trained Trainer object.\n",
        "        tokenized_test_data: Tokenized test dataset.\n",
        "        model_name: Name of the model for saving results.\n",
        "    Returns:\n",
        "        Dictionary of evaluation results.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Evaluating {model_name} on Test Set ===\")\n",
        "\n",
        "    # Run prediction on test set\n",
        "    test_results = trainer.predict(tokenized_test_data)\n",
        "\n",
        "    # Extract predictions and labels\n",
        "    predictions = test_results.predictions\n",
        "    labels = test_results.label_ids\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "\n",
        "    # Use probabilities of the positive class for ROC AUC and PR AUC\n",
        "    positive_class_probs = probabilities[:, 1]\n",
        "\n",
        "    # Predict most probable class\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, predicted_classes)\n",
        "    precision = precision_score(labels, predicted_classes)\n",
        "    recall = recall_score(labels, predicted_classes)\n",
        "    f1 = f1_score(labels, predicted_classes)\n",
        "    mcc = matthews_corrcoef(labels, predicted_classes)\n",
        "    roc_auc = roc_auc_score(labels, positive_class_probs)\n",
        "    pr_auc = average_precision_score(labels, positive_class_probs)\n",
        "    cm = confusion_matrix(labels, predicted_classes)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nTest Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "\n",
        "    # Detailed classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    class_names = list(id2label.values())\n",
        "    report = classification_report(labels, predicted_classes, target_names=class_names)\n",
        "    print(report)\n",
        "\n",
        "    # Save results\n",
        "    results_dir = Path(f\"results/{model_name}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save metrics to CSV for easy comparison\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Model': [model_name],\n",
        "        'Accuracy': [accuracy],\n",
        "        'Precision': [precision],\n",
        "        'Recall': [recall],\n",
        "        'F1': [f1],\n",
        "        'MCC': [mcc],\n",
        "        'ROC_AUC': [roc_auc],\n",
        "        'PR_AUC': [pr_auc]\n",
        "    })\n",
        "    metrics_df.to_csv(results_dir / \"test_metrics.csv\", index=False)\n",
        "\n",
        "    # Save classification report\n",
        "    with open(results_dir / \"classification_report.txt\", \"w\") as f:\n",
        "        f.write(report)\n",
        "\n",
        "    # Visualize confusion matrix\n",
        "    plot_confusion_matrix(cm, class_names, model_name)\n",
        "\n",
        "    # ROC curve\n",
        "    fpr, tpr, _ = roc_curve(labels, positive_class_probs)\n",
        "    plot_roc_curve(fpr, tpr, roc_auc, model_name)\n",
        "\n",
        "    # Precision-Recall curve\n",
        "    precision_points, recall_points, _ = precision_recall_curve(labels, positive_class_probs)\n",
        "    plot_pr_curve(precision_points, recall_points, pr_auc, model_name)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"mcc\": mcc,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"pr_auc\": pr_auc,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"predicted_classes\": predicted_classes,\n",
        "        \"probabilities\": probabilities\n",
        "    }"
      ],
      "metadata": {
        "id": "8d_KyJ8qLXFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_examples(model, tokenizer, examples, model_name):\n",
        "    \"\"\"Predict on a list of example messages.\n",
        "    Args:\n",
        "        model: Trained model.\n",
        "        tokenizer: Tokenizer.\n",
        "        examples: List of example messages.\n",
        "        model_name: Name of the model.\n",
        "    Returns:\n",
        "        DataFrame with predictions.\n",
        "    \"\"\"\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    # Process each example\n",
        "    for text in examples:\n",
        "        # Tokenize the text\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        ).to(device)\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Get probabilities\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "        # Get predicted class\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "        prob_score = probs[0][predicted_class].item()\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            \"text\": text,\n",
        "            \"predicted_class\": id2label[predicted_class],\n",
        "            \"confidence\": prob_score\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n=== Example Predictions with {model_name} ===\")\n",
        "    for i, row in results_df.iterrows():\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Text: {row['text']}\")\n",
        "        print(f\"Prediction: {row['predicted_class']}\")\n",
        "        print(f\"Confidence: {row['confidence']:.4f}\")\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "BeUSdMJc29fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the fine-tuning techniques\n",
        "fine_tuning_techniques = [\n",
        "    \"roberta_frozen\",\n",
        "    \"roberta_full\",\n",
        "    \"roberta_lora\",\n",
        "    \"distilbert_frozen\",\n",
        "    \"distilbert_full\",\n",
        "    \"distilbert_lora\"\n",
        "]\n",
        "\n",
        "# Set up model families\n",
        "model_families = {\n",
        "    \"roberta\": {\n",
        "        \"model_name\": \"roberta-base\",\n",
        "        \"tokenizer\": AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "    },\n",
        "    \"distilbert\": {\n",
        "        \"model_name\": \"distilbert-base-uncased\",\n",
        "        \"tokenizer\": AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    }\n",
        "}\n",
        "\n",
        "# Tokenize datasets for each model family\n",
        "print(\"Tokenizing datasets with original text\")\n",
        "tokenized_datasets = {}\n",
        "for family, config in model_families.items():\n",
        "    print(f\"\\nTokenizing datasets for {family}...\")\n",
        "    tokenized_datasets[family] = tokenize_data(dataset_dict, config[\"tokenizer\"], max_length=128)\n",
        "    print(f\"{family} tokenization complete\")"
      ],
      "metadata": {
        "id": "aGTYBZzyMTI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline evaluation\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"EVALUATING BASELINE MODELS (Before Fine-tuning)\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Store results for later comparison\n",
        "all_results = {}\n",
        "\n",
        "baseline_results = {}\n",
        "for family, config in model_families.items():\n",
        "    baseline_results[family] = evaluate_baseline_model(\n",
        "        config[\"model_name\"],\n",
        "        config[\"tokenizer\"],\n",
        "        tokenized_datasets[family][\"test\"],\n",
        "        device\n",
        "    )\n",
        "\n",
        "# Store baseline results in the format expected by comparison\n",
        "for family, results in baseline_results.items():\n",
        "    # Convert the eval_* format to the expected format\n",
        "    all_results[f\"baseline_{family}\"] = {\n",
        "        'accuracy': results.get('eval_accuracy', 0),\n",
        "        'precision': results.get('eval_precision', 0),\n",
        "        'recall': results.get('eval_recall', 0),\n",
        "        'f1': results.get('eval_f1', 0),\n",
        "        'mcc': results.get('eval_mcc', 0),\n",
        "        'roc_auc': results.get('eval_roc_auc', 0),\n",
        "        'pr_auc': results.get('eval_pr_auc', 0)\n",
        "    }\n",
        "\n",
        "print(\"\\nBaseline results stored:\")\n",
        "for key, value in all_results.items():\n",
        "    print(f\"{key}: MCC={value['mcc']:.4f}, F1={value['f1']:.4f}\")"
      ],
      "metadata": {
        "id": "0DlP_Km5s7rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'='*60}\")\n",
        "print(\"STARTING FINE-TUNING WITH HYPERPARAMETER OPTIMIZATION\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Process each fine-tuning technique\n",
        "for technique in fine_tuning_techniques:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Processing {technique}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Parse technique to get model family and fine-tuning approach\n",
        "    family, approach = technique.split(\"_\")\n",
        "\n",
        "    # Get the appropriate model configuration and tokenized data\n",
        "    model_name = model_families[family][\"model_name\"]\n",
        "    tokenizer = model_families[family][\"tokenizer\"]\n",
        "    datasets = tokenized_datasets[family]\n",
        "\n",
        "    # Define the hyperparameter optimization function\n",
        "    def objective(trial):\n",
        "        # Define hyperparameter search space\n",
        "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True)\n",
        "        batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "        weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1)\n",
        "        num_epochs = trial.suggest_int(\"num_epochs\", 3, 8)\n",
        "\n",
        "        # Initialize model based on the approach\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2,\n",
        "            id2label=id2label,\n",
        "            label2id=label2id\n",
        "        )\n",
        "\n",
        "        if approach == \"frozen\":\n",
        "            # Freeze most layers, only train the last layers and classifier\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "                # For RoBERTa, unfreeze last two layers\n",
        "                if family == \"roberta\" and (\"encoder.layer.10\" in name or \"encoder.layer.11\" in name):\n",
        "                    param.requires_grad = True\n",
        "\n",
        "                # For DistilBERT, unfreeze last layer (has fewer layers)\n",
        "                if family == \"distilbert\" and \"transformer.layer.5\" in name:\n",
        "                    param.requires_grad = True\n",
        "\n",
        "                # Always unfreeze classifier\n",
        "                if \"classifier\" in name:\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        elif approach == \"lora\":\n",
        "            # Apply LoRA configuration\n",
        "            lora_r = trial.suggest_int(\"lora_r\", 4, 16)\n",
        "            lora_alpha = trial.suggest_int(\"lora_alpha\", 8, 32)\n",
        "            lora_dropout = trial.suggest_float(\"lora_dropout\", 0.05, 0.2)\n",
        "\n",
        "            # Define target modules based on model family\n",
        "            if family == \"roberta\":\n",
        "                target_modules = [\"query\", \"key\", \"value\"]\n",
        "            else:  # distilbert\n",
        "                target_modules = [\"q_lin\", \"v_lin\"]\n",
        "\n",
        "            # Define LoRA configuration\n",
        "            peft_config = LoraConfig(\n",
        "                task_type=TaskType.SEQ_CLS,\n",
        "                r=lora_r,\n",
        "                lora_alpha=lora_alpha,\n",
        "                lora_dropout=lora_dropout,\n",
        "                target_modules=target_modules\n",
        "            )\n",
        "\n",
        "            # Apply LoRA to the model\n",
        "            model = get_peft_model(model, peft_config)\n",
        "\n",
        "        # For \"full\" approach, we don't modify the model (all parameters are trainable by default)\n",
        "\n",
        "        # Define training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f\"results/optuna_{technique}_trial_{trial.number}\",\n",
        "            learning_rate=learning_rate,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            num_train_epochs=num_epochs,\n",
        "            weight_decay=weight_decay,\n",
        "            eval_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            save_strategy=\"no\",  # Don't save checkpoints during optimization\n",
        "            load_best_model_at_end=False,\n",
        "            report_to=\"none\",\n",
        "            fp16=torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "        # Use WeightedTrainer for hyperparameter optimization too\n",
        "        class_weights = calculate_class_weights(datasets[\"train\"], device)\n",
        "\n",
        "        trainer = WeightedTrainer(\n",
        "            model=model.to(device),\n",
        "            args=training_args,\n",
        "            train_dataset=datasets[\"train\"],\n",
        "            eval_dataset=datasets[\"valid\"],\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=create_data_collator(tokenizer),\n",
        "            compute_metrics=compute_metrics,\n",
        "            class_weights=class_weights\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        trainer.train()\n",
        "\n",
        "        # Evaluate model\n",
        "        eval_result = trainer.evaluate()\n",
        "\n",
        "        # Extract MCC score\n",
        "        mcc_score = eval_result.get(\"eval_mcc\", 0.0)\n",
        "\n",
        "        # Clean up to save memory\n",
        "        del model\n",
        "        del trainer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return mcc_score\n",
        "\n",
        "    # Run hyperparameter optimization\n",
        "    print(f\"Running hyperparameter optimization for {technique}...\")\n",
        "    study = optuna.create_study(direction=\"maximize\", study_name=f\"{technique}_hpo\")\n",
        "    study.optimize(objective, n_trials=10)  # Adjust number of trials as needed\n",
        "\n",
        "    # Get best hyperparameters\n",
        "    best_params = study.best_params\n",
        "    print(f\"\\nBest hyperparameters for {technique}:\")\n",
        "    for key, value in best_params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    # Save best hyperparameters\n",
        "    os.makedirs(\"results/hyperparameters\", exist_ok=True)\n",
        "    with open(f\"results/hyperparameters/{technique}_best_params.json\", \"w\") as f:\n",
        "        json.dump(best_params, f, indent=4)\n",
        "\n",
        "    # Build model with best hyperparameters\n",
        "    print(f\"\\nTraining final {technique} model with best hyperparameters...\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    # Apply appropriate configuration based on approach\n",
        "    if approach == \"frozen\":\n",
        "        # Freeze most layers, only train the last layers and classifier\n",
        "        for name, param in model.named_parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "            # For RoBERTa, unfreeze last two layers\n",
        "            if family == \"roberta\" and (\"encoder.layer.10\" in name or \"encoder.layer.11\" in name):\n",
        "                param.requires_grad = True\n",
        "\n",
        "            # For DistilBERT, unfreeze last layer\n",
        "            if family == \"distilbert\" and \"transformer.layer.5\" in name:\n",
        "                param.requires_grad = True\n",
        "\n",
        "            # Always unfreeze classifier\n",
        "            if \"classifier\" in name:\n",
        "                param.requires_grad = True\n",
        "\n",
        "    elif approach == \"lora\":\n",
        "        # Define LoRA configuration with best hyperparameters\n",
        "        if family == \"roberta\":\n",
        "            target_modules = [\"query\", \"key\", \"value\"]\n",
        "        else:  # distilbert\n",
        "            target_modules = [\"q_lin\", \"v_lin\"]\n",
        "\n",
        "        peft_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=best_params.get(\"lora_r\", 8),  # Use default if not in best_params\n",
        "            lora_alpha=best_params.get(\"lora_alpha\", 16),\n",
        "            lora_dropout=best_params.get(\"lora_dropout\", 0.1),\n",
        "            target_modules=target_modules\n",
        "        )\n",
        "\n",
        "        # Apply LoRA to the model\n",
        "        model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # For \"full\" approach, we don't modify the model\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"results/{technique}\",\n",
        "        learning_rate=best_params[\"learning_rate\"],\n",
        "        per_device_train_batch_size=best_params[\"batch_size\"],\n",
        "        per_device_eval_batch_size=best_params[\"batch_size\"],\n",
        "        num_train_epochs=best_params[\"num_epochs\"],\n",
        "        weight_decay=best_params.get(\"weight_decay\", 0.01),\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"mcc\",\n",
        "        greater_is_better=True,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=\"none\",\n",
        "        logging_dir=f\"results/{technique}/logs\"\n",
        "    )\n",
        "\n",
        "    # Train the model (using our corrected train_model function)\n",
        "    model = model.to(device)\n",
        "    trainer = train_model(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        datasets,\n",
        "        training_args,\n",
        "        technique\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    results = evaluate_model(\n",
        "        trainer,\n",
        "        datasets[\"test\"],\n",
        "        technique\n",
        "    )\n",
        "\n",
        "    # Store results for later comparison\n",
        "    all_results[technique] = results\n",
        "\n",
        "    # Save trainer and model reference for example message testing\n",
        "    globals()[f\"{technique}_trainer\"] = trainer\n",
        "\n",
        "    # Clear GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ALL FINE-TUNING COMPLETE!\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "id": "Zl4ZXPieMfwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comparison DataFrame from all results\n",
        "comparison_data = []\n",
        "for technique, results in all_results.items():\n",
        "    comparison_data.append({\n",
        "        'Model': technique,\n",
        "        'MCC': results.get('mcc', 0),\n",
        "        'F1': results.get('f1', 0),\n",
        "        'Accuracy': results.get('accuracy', 0),\n",
        "        'Precision': results.get('precision', 0),\n",
        "        'Recall': results.get('recall', 0),\n",
        "        'ROC_AUC': results.get('roc_auc', 0),\n",
        "        'PR_AUC': results.get('pr_auc', 0)\n",
        "    })"
      ],
      "metadata": {
        "id": "TY7lO9KajvIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame for comparison\n",
        "if comparison_data:\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    # Save combined results\n",
        "    comparison_df.to_csv(\"results/all_llm_models_comparison.csv\", index=False)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nAll LLM Models Comparison:\")\n",
        "    print(comparison_df.sort_values(by='MCC', ascending=False))\n",
        "\n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.barplot(data=comparison_df, x='Model', y='MCC')\n",
        "    plt.title('Matthews Correlation Coefficient (MCC) Across LLM Models')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"results/visualizations/llm_models_mcc_comparison.png\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No results available for comparison.\")"
      ],
      "metadata": {
        "id": "i2Wz-a9aj2cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9: Explainability Analysis on Real Test Data\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPLAINABILITY ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Select best performing model for explainability\n",
        "if comparison_data:\n",
        "    # Find the best model (highest MCC, excluding baselines)\n",
        "    finetuned_results = [r for r in comparison_data if not r['Model'].startswith('baseline')]\n",
        "    if finetuned_results:\n",
        "        best_model_name = max(finetuned_results, key=lambda x: x['MCC'])['Model']\n",
        "        print(f\"\\nRunning explainability analysis on best model: {best_model_name}\")\n",
        "\n",
        "        # Get the best model and tokenizer\n",
        "        family = best_model_name.split(\"_\")[0]\n",
        "        best_trainer = globals()[f\"{best_model_name}_trainer\"]\n",
        "        tokenizer = model_families[family][\"tokenizer\"]\n",
        "\n",
        "        # Get some real test examples for explanation\n",
        "        test_dataset = tokenized_datasets[family][\"test\"]\n",
        "\n",
        "        # Select a few interesting examples from test set\n",
        "        print(\"\\nAnalyzing real test examples...\")\n",
        "\n",
        "        # Get predictions on test set to find interesting cases\n",
        "        test_predictions = best_trainer.predict(test_dataset)\n",
        "        probabilities = torch.nn.functional.softmax(torch.tensor(test_predictions.predictions), dim=-1).numpy()\n",
        "        predicted_classes = np.argmax(test_predictions.predictions, axis=1)\n",
        "        true_labels = test_predictions.label_ids\n",
        "\n",
        "        # Find examples for explanation:\n",
        "        # 1. High confidence correct predictions (both classes)\n",
        "        # 2. Misclassified examples\n",
        "        # 3. Low confidence predictions\n",
        "\n",
        "        explanation_indices = []\n",
        "\n",
        "        # High confidence correct spam detection\n",
        "        spam_correct = np.where((predicted_classes == 1) & (true_labels == 1) & (probabilities[:, 1] > 0.9))[0]\n",
        "        if len(spam_correct) > 0:\n",
        "            explanation_indices.append(spam_correct[0])\n",
        "\n",
        "        # High confidence correct legitimate detection\n",
        "        legit_correct = np.where((predicted_classes == 0) & (true_labels == 0) & (probabilities[:, 0] > 0.9))[0]\n",
        "        if len(legit_correct) > 0:\n",
        "            explanation_indices.append(legit_correct[0])\n",
        "\n",
        "        # Misclassified example\n",
        "        misclassified = np.where(predicted_classes != true_labels)[0]\n",
        "        if len(misclassified) > 0:\n",
        "            explanation_indices.append(misclassified[0])\n",
        "\n",
        "        # Low confidence prediction\n",
        "        min_confidence = np.min(probabilities, axis=1)\n",
        "        low_conf = np.where(min_confidence < 0.6)[0]\n",
        "        if len(low_conf) > 0:\n",
        "            explanation_indices.append(low_conf[0])\n",
        "\n",
        "        # Run LIME explanations on selected examples\n",
        "        explainer, predict_fn = setup_lime_explainer(\n",
        "            best_trainer.model,\n",
        "            tokenizer,\n",
        "            device\n",
        "        )\n",
        "\n",
        "        for idx in explanation_indices[:4]:  # Limit to 4 examples\n",
        "            # Get original text\n",
        "            original_text = test_df.iloc[idx]['message']\n",
        "            true_label = id2label[true_labels[idx]]\n",
        "            predicted_label = id2label[predicted_classes[idx]]\n",
        "            confidence = probabilities[idx].max()\n",
        "\n",
        "            print(f\"\\n{'-'*80}\")\n",
        "            print(f\"Example {idx + 1}:\")\n",
        "            print(f\"Text: {original_text}\")\n",
        "            print(f\"True Label: {true_label}\")\n",
        "            print(f\"Predicted: {predicted_label} (confidence: {confidence:.3f})\")\n",
        "\n",
        "            # Generate explanation\n",
        "            explanation = explain_prediction(explainer, predict_fn, original_text, num_features=8)\n",
        "\n",
        "            print(\"Top contributing features:\")\n",
        "            for feature, weight in explanation.as_list()[:5]:\n",
        "                direction = \" SPAM\" if weight > 0 else \" LEGITIMATE\"\n",
        "                print(f\"  '{feature}': {weight:.3f} {direction}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ANALYSIS COMPLETE!\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "id": "evNGI3JVM-6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11: Compare all models\n",
        "print(\"\\n=== Comparing All Models ===\")\n",
        "\n",
        "if 'comparison_df' in locals() and not comparison_df.empty:\n",
        "    # Plot comparison of all metrics for all models\n",
        "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1', 'MCC', 'ROC_AUC', 'PR_AUC']\n",
        "\n",
        "    # Reshape data for plotting\n",
        "    plot_data = comparison_df.melt(\n",
        "        id_vars=['Model'],\n",
        "        value_vars=metrics_to_plot,\n",
        "        var_name='Metric',\n",
        "        value_name='Score'\n",
        "    )\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.barplot(data=plot_data, x='Metric', y='Score', hue='Model')\n",
        "    plt.title('Performance Comparison Across LLM Models')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"results/visualizations/llm_models_all_metrics_comparison.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Compare RoBERTa vs DistilBERT (average across fine-tuning approaches)\n",
        "    model_family_comparison = comparison_df.copy()\n",
        "    model_family_comparison['Model_Family'] = model_family_comparison['Model'].apply(\n",
        "        lambda x: 'RoBERTa' if x.startswith('roberta') else 'DistilBERT'\n",
        "    )\n",
        "\n",
        "    family_avg = model_family_comparison.groupby('Model_Family')[metrics_to_plot].mean().reset_index()\n",
        "\n",
        "    # Plot model family comparison\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.barplot(data=family_avg.melt(\n",
        "        id_vars=['Model_Family'],\n",
        "        value_vars=metrics_to_plot,\n",
        "        var_name='Metric',\n",
        "        value_name='Score'\n",
        "    ), x='Metric', y='Score', hue='Model_Family')\n",
        "    plt.title('Average Performance by Model Family')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"results/visualizations/model_family_comparison.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Compare fine-tuning approaches (average across model families)\n",
        "    comparison_df['Fine_Tuning_Approach'] = comparison_df['Model'].apply(\n",
        "        lambda x: x.split('_')[1]  # Extract approach (frozen, full, lora)\n",
        "    )\n",
        "\n",
        "    approach_avg = comparison_df.groupby('Fine_Tuning_Approach')[metrics_to_plot].mean().reset_index()\n",
        "\n",
        "    # Plot fine-tuning approach comparison\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.barplot(data=approach_avg.melt(\n",
        "        id_vars=['Fine_Tuning_Approach'],\n",
        "        value_vars=metrics_to_plot,\n",
        "        var_name='Metric',\n",
        "        value_name='Score'\n",
        "    ), x='Metric', y='Score', hue='Fine_Tuning_Approach')\n",
        "    plt.title('Average Performance by Fine-Tuning Approach')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"results/visualizations/fine_tuning_approach_comparison.png\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No results available for comparison.\")"
      ],
      "metadata": {
        "id": "rkgafTBr7qfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12: Compare with previous models (ML and DL)\n",
        "print(\"\\n=== Comparing All Models (ML, DL, and LLM) ===\")\n",
        "\n",
        "# Load results from previous models\n",
        "ml_results_path = \"results/metrics/baseline_ml_results_regularized.csv\"\n",
        "dl_results_path = \"results/metrics/all_models_comparison.csv\"\n",
        "\n",
        "all_model_results = []\n",
        "\n",
        "# 1. Add LLM results (current notebook)\n",
        "if 'comparison_df' in locals() and not comparison_df.empty:\n",
        "    llm_results = comparison_df.copy()\n",
        "    llm_results['Model_Type'] = 'LLM'\n",
        "\n",
        "    # Add fine-tuning approach\n",
        "    llm_results['Fine_Tuning_Approach'] = llm_results['Model'].apply(\n",
        "        lambda x: x.split('_')[-1] if 'baseline' not in x else 'baseline'\n",
        "    )\n",
        "\n",
        "    all_model_results.append(llm_results)\n",
        "    print(f\"Added {len(llm_results)} LLM models\")\n",
        "\n",
        "# 2. Load and process ML results\n",
        "if Path(ml_results_path).exists():\n",
        "    ml_df = pd.read_csv(ml_results_path)\n",
        "    print(f\"Found ML results file with columns: {ml_df.columns.tolist()}\")\n",
        "\n",
        "    # Standardize column names\n",
        "    ml_standardized = pd.DataFrame()\n",
        "    ml_standardized['Model'] = ml_df['Model']\n",
        "    ml_standardized['MCC'] = ml_df.get('Test MCC', 0)\n",
        "    ml_standardized['F1'] = ml_df.get('Test F1', 0)\n",
        "    ml_standardized['Accuracy'] = ml_df.get('Test Accuracy', 0)\n",
        "    ml_standardized['Precision'] = ml_df.get('Test Precision', 0)\n",
        "    ml_standardized['Recall'] = ml_df.get('Test Recall', 0)\n",
        "    ml_standardized['ROC_AUC'] = ml_df.get('Test ROC AUC', 0)\n",
        "    ml_standardized['PR_AUC'] = ml_df.get('Test PR AUC', 0)\n",
        "    ml_standardized['Model_Type'] = 'ML'\n",
        "    ml_standardized['Fine_Tuning_Approach'] = 'N/A'\n",
        "\n",
        "    all_model_results.append(ml_standardized)\n",
        "    print(f\"Added {len(ml_standardized)} ML models\")\n",
        "else:\n",
        "    print(\"ML results file not found\")\n",
        "\n",
        "# 3. Load and process DL results\n",
        "if Path(dl_results_path).exists():\n",
        "    dl_combined_df = pd.read_csv(dl_results_path)\n",
        "    print(f\"Found DL results file with columns: {dl_combined_df.columns.tolist()}\")\n",
        "\n",
        "    # Filter to get only DL models (exclude ML models that might be in the file)\n",
        "    dl_model_names = ['TextCNN', 'BiLSTM', 'CNN', 'LSTM', 'Bi-LSTM']  # Add your DL model names\n",
        "    dl_only = dl_combined_df[dl_combined_df['Model'].isin(dl_model_names)]\n",
        "\n",
        "    if len(dl_only) > 0:\n",
        "        # Standardize column names for DL models\n",
        "        dl_standardized = pd.DataFrame()\n",
        "        dl_standardized['Model'] = dl_only['Model']\n",
        "        dl_standardized['MCC'] = dl_only.get('Test MCC', 0)\n",
        "        dl_standardized['F1'] = dl_only.get('Test F1', 0)\n",
        "        dl_standardized['Accuracy'] = dl_only.get('Test Accuracy', 0)\n",
        "        dl_standardized['Precision'] = dl_only.get('Test Precision', 0)\n",
        "        dl_standardized['Recall'] = dl_only.get('Test Recall', 0)\n",
        "        dl_standardized['ROC_AUC'] = dl_only.get('Test ROC AUC', 0)\n",
        "        dl_standardized['PR_AUC'] = dl_only.get('Test PR AUC', 0)\n",
        "        dl_standardized['Model_Type'] = 'DL'\n",
        "        dl_standardized['Fine_Tuning_Approach'] = 'N/A'\n",
        "\n",
        "        all_model_results.append(dl_standardized)\n",
        "        print(f\"Added {len(dl_standardized)} DL models\")\n",
        "    else:\n",
        "        print(\"No DL models found in the combined results file\")\n",
        "else:\n",
        "    print(\"DL results file not found\")\n",
        "\n",
        "# 4. Combine all results\n",
        "if all_model_results:\n",
        "    # Combine all dataframes\n",
        "    final_comparison_df = pd.concat(all_model_results, ignore_index=True)\n",
        "\n",
        "    # Remove any duplicate rows (same model name and type)\n",
        "    final_comparison_df = final_comparison_df.drop_duplicates(subset=['Model', 'Model_Type'], keep='first')\n",
        "\n",
        "    # Sort by MCC (descending)\n",
        "    final_comparison_df = final_comparison_df.sort_values('MCC', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Save the clean combined results\n",
        "    final_comparison_df.to_csv(\"results/final_comparison/all_models_clean_comparison.csv\", index=False)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"CLEAN FINAL COMPARISON - ALL MODELS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    display_columns = ['Model', 'Model_Type', 'MCC', 'F1', 'Accuracy', 'Precision', 'Recall', 'ROC_AUC', 'PR_AUC']\n",
        "    print(final_comparison_df[display_columns].round(4))\n",
        "\n",
        "    # Summary by model type\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"SUMMARY BY MODEL TYPE\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    summary_stats = final_comparison_df.groupby('Model_Type')[['MCC', 'F1', 'ROC_AUC', 'PR_AUC']].agg(['mean', 'max', 'count']).round(4)\n",
        "    print(summary_stats)\n",
        "\n",
        "    # Best model overall\n",
        "    best_model = final_comparison_df.iloc[0]\n",
        "    print(f\"\\nBEST OVERALL MODEL:\")\n",
        "    print(f\"   Model: {best_model['Model']} ({best_model['Model_Type']})\")\n",
        "    print(f\"   MCC: {best_model['MCC']:.4f}\")\n",
        "    print(f\"   F1: {best_model['F1']:.4f}\")\n",
        "    print(f\"   ROC AUC: {best_model['ROC_AUC']:.4f}\")\n",
        "\n",
        "    # Create visualizations\n",
        "    plt.figure(figsize=(16, 10))\n",
        "\n",
        "    # Plot 1: Top 10 models by MCC\n",
        "    plt.subplot(2, 2, 1)\n",
        "    top_10 = final_comparison_df.head(10)\n",
        "    colors = {'ML': 'skyblue', 'DL': 'lightgreen', 'LLM': 'lightcoral'}\n",
        "    bar_colors = [colors.get(model_type, 'gray') for model_type in top_10['Model_Type']]\n",
        "\n",
        "    plt.bar(range(len(top_10)), top_10['MCC'], color=bar_colors)\n",
        "    plt.title('Top 10 Models by MCC')\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('MCC Score')\n",
        "    plt.xticks(range(len(top_10)), top_10['Model'], rotation=45, ha='right')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add legend\n",
        "    import matplotlib.patches as mpatches\n",
        "    patches = [mpatches.Patch(color=color, label=label) for label, color in colors.items()]\n",
        "    plt.legend(handles=patches, loc='upper right')\n",
        "\n",
        "    # Plot 2: Model type comparison (average metrics)\n",
        "    plt.subplot(2, 2, 2)\n",
        "    type_avg = final_comparison_df.groupby('Model_Type')[['MCC', 'F1', 'ROC_AUC']].mean()\n",
        "    type_avg.plot(kind='bar', ax=plt.gca())\n",
        "    plt.title('Average Performance by Model Type')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Plot 3: MCC vs F1 scatter\n",
        "    plt.subplot(2, 2, 3)\n",
        "    for model_type, color in colors.items():\n",
        "        subset = final_comparison_df[final_comparison_df['Model_Type'] == model_type]\n",
        "        plt.scatter(subset['MCC'], subset['F1'], c=color, label=model_type, alpha=0.7, s=60)\n",
        "\n",
        "    plt.xlabel('MCC Score')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title('MCC vs F1 Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Count by model type\n",
        "    plt.subplot(2, 2, 4)\n",
        "    type_counts = final_comparison_df['Model_Type'].value_counts()\n",
        "    plt.pie(type_counts.values, labels=type_counts.index, autopct='%1.0f%%', colors=[colors[t] for t in type_counts.index])\n",
        "    plt.title('Number of Models by Type')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"results/final_comparison/comprehensive_model_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No model results found for comparison\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"COMPARISON COMPLETE!\")\n",
        "print(f\"{'='*80}\")"
      ],
      "metadata": {
        "id": "NzdQD8CrmL7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training curve visualization\n",
        "def plot_training_curves_comparison():\n",
        "    \"\"\"\n",
        "    Plot training curves for all models to visualize overfitting\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    models_data = [\n",
        "        ('roberta_frozen', roberta_frozen_trainer),\n",
        "        ('roberta_full', roberta_full_trainer),\n",
        "        ('roberta_lora', roberta_lora_trainer),\n",
        "        ('distilbert_frozen', distilbert_frozen_trainer),\n",
        "        ('distilbert_full', distilbert_full_trainer),\n",
        "        ('distilbert_lora', distilbert_lora_trainer),\n",
        "    ]\n",
        "\n",
        "    for idx, (model_name, trainer) in enumerate(models_data):\n",
        "        if trainer and idx < len(axes):\n",
        "            logs = trainer.state.log_history\n",
        "\n",
        "            # Extract training and validation losses\n",
        "            train_losses = [log['loss'] for log in logs if 'loss' in log and 'eval_loss' not in log]\n",
        "            val_losses = [log['eval_loss'] for log in logs if 'eval_loss' in log]\n",
        "            val_f1 = [log['eval_f1'] for log in logs if 'eval_f1' in log]\n",
        "\n",
        "            epochs = range(1, len(val_losses) + 1)\n",
        "\n",
        "            # Plot validation loss and F1\n",
        "            ax = axes[idx]\n",
        "            ax2 = ax.twinx()\n",
        "\n",
        "            line1 = ax.plot(epochs, val_losses, 'b-', label='Val Loss', marker='o')\n",
        "            line2 = ax2.plot(epochs, val_f1, 'r-', label='Val F1', marker='s')\n",
        "\n",
        "            ax.set_xlabel('Epoch')\n",
        "            ax.set_ylabel('Validation Loss', color='b')\n",
        "            ax2.set_ylabel('Validation F1', color='r')\n",
        "            ax.set_title(f'{model_name}')\n",
        "\n",
        "            # Add overfitting indicators\n",
        "            if len(val_losses) > 3:\n",
        "                best_epoch = np.argmax(val_f1) + 1\n",
        "                ax2.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.7, label=f'Best F1 (Epoch {best_epoch})')\n",
        "\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # Combine legends\n",
        "            lines1, labels1 = ax.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/visualizations/training_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_training_curves_comparison()"
      ],
      "metadata": {
        "id": "jgvlU8JukGBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computational requirements analysis\n",
        "def analyze_computational_requirements():\n",
        "    \"\"\"\n",
        "    Analyze computational requirements for different fine-tuning approaches\n",
        "    \"\"\"\n",
        "\n",
        "    # Training times (extract from your existing results)\n",
        "    training_times = {\n",
        "        'roberta_frozen': 3.96,  # minutes - from your output\n",
        "        'roberta_full': 2.65,    # minutes\n",
        "        'roberta_lora': 1.93,    # minutes\n",
        "        'distilbert_frozen': 5.01, # minutes\n",
        "        'distilbert_full': 5.51,   # minutes\n",
        "        'distilbert_lora': 1.89,   # minutes\n",
        "    }\n",
        "\n",
        "    # Parameter counts (extract from your model summaries)\n",
        "    parameter_counts = {\n",
        "        'roberta_frozen': {'total': 124647170, 'trainable': 14767874, 'trainable_pct': 11.85},\n",
        "        'roberta_full': {'total': 124647170, 'trainable': 124647170, 'trainable_pct': 100.00},\n",
        "        'roberta_lora': {'total': 124647170, 'trainable': 979202, 'trainable_pct': 0.78},\n",
        "        'distilbert_frozen': {'total': 66955010, 'trainable': 7680002, 'trainable_pct': 11.47},\n",
        "        'distilbert_full': {'total': 66955010, 'trainable': 66955010, 'trainable_pct': 100.00},\n",
        "        'distilbert_lora': {'total': 66955010, 'trainable': 684290, 'trainable_pct': 1.01},\n",
        "    }\n",
        "\n",
        "    # Performance results (from your comparison_df)\n",
        "    performance_results = {\n",
        "        'roberta_frozen': {'accuracy': 0.9533, 'f1': 0.8772, 'mcc': 0.8503},\n",
        "        'roberta_full': {'accuracy': 0.9400, 'f1': 0.8525, 'mcc': 0.8150},\n",
        "        'roberta_lora': {'accuracy': 0.9333, 'f1': 0.8276, 'mcc': 0.7870},\n",
        "        'distilbert_frozen': {'accuracy': 0.9333, 'f1': 0.8333, 'mcc': 0.7917},\n",
        "        'distilbert_full': {'accuracy': 0.9600, 'f1': 0.9032, 'mcc': 0.8788},\n",
        "        'distilbert_lora': {'accuracy': 0.9467, 'f1': 0.8667, 'mcc': 0.8333},\n",
        "    }\n",
        "\n",
        "    # Create comprehensive analysis DataFrame\n",
        "    analysis_data = []\n",
        "    for model in training_times.keys():\n",
        "        analysis_data.append({\n",
        "            'Model': model,\n",
        "            'Training_Time_Min': training_times[model],\n",
        "            'Total_Parameters': parameter_counts[model]['total'],\n",
        "            'Trainable_Parameters': parameter_counts[model]['trainable'],\n",
        "            'Trainable_Percentage': parameter_counts[model]['trainable_pct'],\n",
        "            'Test_Accuracy': performance_results[model]['accuracy'],\n",
        "            'Test_F1': performance_results[model]['f1'],\n",
        "            'Test_MCC': performance_results[model]['mcc'],\n",
        "            'Efficiency_Score': performance_results[model]['mcc'] / (parameter_counts[model]['trainable_pct'] / 100),  # MCC per % of trainable params\n",
        "        })\n",
        "\n",
        "    efficiency_df = pd.DataFrame(analysis_data)\n",
        "\n",
        "    # Add model family and approach columns\n",
        "    efficiency_df['Model_Family'] = efficiency_df['Model'].apply(lambda x: 'RoBERTa' if 'roberta' in x else 'DistilBERT')\n",
        "    efficiency_df['Fine_Tuning_Approach'] = efficiency_df['Model'].apply(lambda x: x.split('_')[1])\n",
        "\n",
        "    # Save results\n",
        "    efficiency_df.to_csv('results/metrics/computational_efficiency_analysis.csv', index=False)\n",
        "\n",
        "    return efficiency_df\n",
        "\n",
        "# Run the analysis\n",
        "efficiency_results = analyze_computational_requirements()\n",
        "print(\"Computational Efficiency Analysis:\")\n",
        "print(efficiency_results.round(4))"
      ],
      "metadata": {
        "id": "JjWUpyR9kK3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Efficiency visualization\n",
        "def plot_efficiency_analysis(efficiency_df):\n",
        "    \"\"\"\n",
        "    Create comprehensive efficiency plots\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Plot 1: Performance vs Trainable Parameters\n",
        "    ax1 = axes[0, 0]\n",
        "    scatter = ax1.scatter(efficiency_df['Trainable_Percentage'], efficiency_df['Test_MCC'],\n",
        "                         c=efficiency_df['Training_Time_Min'], s=100, alpha=0.7, cmap='viridis')\n",
        "\n",
        "    for idx, row in efficiency_df.iterrows():\n",
        "        ax1.annotate(row['Model'], (row['Trainable_Percentage'], row['Test_MCC']),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    ax1.set_xlabel('Trainable Parameters (%)')\n",
        "    ax1.set_ylabel('Test MCC')\n",
        "    ax1.set_title('Performance vs Parameter Efficiency')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    plt.colorbar(scatter, ax=ax1, label='Training Time (min)')\n",
        "\n",
        "    # Plot 2: Efficiency Score by Approach\n",
        "    ax2 = axes[0, 1]\n",
        "    approach_efficiency = efficiency_df.groupby('Fine_Tuning_Approach')['Efficiency_Score'].mean().reset_index()\n",
        "    bars = ax2.bar(approach_efficiency['Fine_Tuning_Approach'], approach_efficiency['Efficiency_Score'])\n",
        "    ax2.set_ylabel('Efficiency Score (MCC per % trainable params)')\n",
        "    ax2.set_title('Efficiency by Fine-tuning Approach')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # Plot 3: Training Time vs Performance\n",
        "    ax3 = axes[1, 0]\n",
        "    colors = {'lora': 'red', 'frozen': 'blue', 'full': 'green'}\n",
        "    for approach in efficiency_df['Fine_Tuning_Approach'].unique():\n",
        "        subset = efficiency_df[efficiency_df['Fine_Tuning_Approach'] == approach]\n",
        "        ax3.scatter(subset['Training_Time_Min'], subset['Test_MCC'],\n",
        "                   label=approach, color=colors.get(approach, 'gray'), s=100, alpha=0.7)\n",
        "\n",
        "    ax3.set_xlabel('Training Time (minutes)')\n",
        "    ax3.set_ylabel('Test MCC')\n",
        "    ax3.set_title('Training Time vs Performance')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Model Family Comparison\n",
        "    ax4 = axes[1, 1]\n",
        "    family_comparison = efficiency_df.groupby(['Model_Family', 'Fine_Tuning_Approach']).agg({\n",
        "        'Test_MCC': 'mean',\n",
        "        'Trainable_Percentage': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    x = np.arange(len(family_comparison['Fine_Tuning_Approach'].unique()))\n",
        "    width = 0.35\n",
        "\n",
        "    roberta_data = family_comparison[family_comparison['Model_Family'] == 'RoBERTa']\n",
        "    distilbert_data = family_comparison[family_comparison['Model_Family'] == 'DistilBERT']\n",
        "\n",
        "    ax4.bar(x - width/2, roberta_data['Test_MCC'], width, label='RoBERTa', alpha=0.7)\n",
        "    ax4.bar(x + width/2, distilbert_data['Test_MCC'], width, label='DistilBERT', alpha=0.7)\n",
        "\n",
        "    ax4.set_xlabel('Fine-tuning Approach')\n",
        "    ax4.set_ylabel('Test MCC')\n",
        "    ax4.set_title('Model Family Performance Comparison')\n",
        "    ax4.set_xticks(x)\n",
        "    ax4.set_xticklabels(roberta_data['Fine_Tuning_Approach'])\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/visualizations/efficiency_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_efficiency_analysis(efficiency_results)"
      ],
      "metadata": {
        "id": "ynXhIazXkO-x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}